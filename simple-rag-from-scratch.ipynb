{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8241706,"sourceType":"datasetVersion","datasetId":4889036}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T06:36:20.391606Z","iopub.execute_input":"2024-05-01T06:36:20.392010Z","iopub.status.idle":"2024-05-01T06:36:21.900602Z","shell.execute_reply.started":"2024-05-01T06:36:20.391979Z","shell.execute_reply":"2024-05-01T06:36:21.899269Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/publications/LEE.pdf\n/kaggle/input/publications/Dawson.pdf\n/kaggle/input/publications/Qiu.pdf\n/kaggle/input/publications/1_Ramrez-Duque_.pdf\n/kaggle/input/publications/Abbas_2020.pdf\n/kaggle/input/publications/22_Ouss_ASD.pdf\n/kaggle/input/publications/Asd_Cry_patterns.pdf\n/kaggle/input/publications/zhao2020.pdf\n/kaggle/input/publications/Abbas_2018.pdf\n/kaggle/input/publications/carpenter2020 (1).pdf\n/kaggle/input/publications/Young_Behavior.pdf\n/kaggle/input/publications/Tariq2018.pdf\n/kaggle/input/publications/Patten_Audio.pdf\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Installing all the dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install -q unstructured\n!pip install -q sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:39:44.791534Z","iopub.execute_input":"2024-05-01T06:39:44.792945Z","iopub.status.idle":"2024-05-01T06:40:26.691662Z","shell.execute_reply.started":"2024-05-01T06:39:44.792887Z","shell.execute_reply":"2024-05-01T06:40:26.689937Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"**Importing all the dependencies**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport json\nfrom unstructured_client import UnstructuredClient\nfrom unstructured_client.models import shared\nfrom unstructured_client.models.errors import SDKError\nfrom unstructured.staging.base import dict_to_elements, elements_to_json\nfrom IPython.display import JSON","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:40:34.311552Z","iopub.execute_input":"2024-05-01T06:40:34.311968Z","iopub.status.idle":"2024-05-01T06:40:35.223242Z","shell.execute_reply.started":"2024-05-01T06:40:34.311932Z","shell.execute_reply":"2024-05-01T06:40:35.221973Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Setting up the user secrets and unstructured client**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"uns_api\")\nsecret_value_1 = user_secrets.get_secret(\"uns_server_url\")\ns = UnstructuredClient(\n    api_key_auth=secret_value_0,\n    server_url=secret_value_1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:40:40.165710Z","iopub.execute_input":"2024-05-01T06:40:40.166659Z","iopub.status.idle":"2024-05-01T06:40:40.500142Z","shell.execute_reply.started":"2024-05-01T06:40:40.166614Z","shell.execute_reply":"2024-05-01T06:40:40.498812Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Getting elements from a single pdf using the Unstructured client**","metadata":{}},{"cell_type":"code","source":"def get_elements_from_pdf(filename):\n    with open(filename, \"rb\") as f:\n        files=shared.Files(\n            content=f.read(), \n            file_name=filename,\n        )\n\n    req = shared.PartitionParameters(\n        files=files,\n        strategy='hi_res',\n        pdf_infer_table_structure=True,\n        languages=[\"eng\"],\n    )\n    try:\n        resp = s.general.partition(req)\n    except SDKError as e:\n        print(e)\n    return resp\nfilename = \"/kaggle/input/publications/1_Ramrez-Duque_.pdf\"\nresponse = get_elements_from_pdf(filename)\nprint(json.dumps(response.elements[:3], indent=2))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:40:45.170468Z","iopub.execute_input":"2024-05-01T06:40:45.170902Z","iopub.status.idle":"2024-05-01T06:43:16.030231Z","shell.execute_reply.started":"2024-05-01T06:40:45.170869Z","shell.execute_reply":"2024-05-01T06:43:16.028639Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[\n  {\n    \"type\": \"Header\",\n    \"element_id\": \"124519724f8942a945cd489488e97294\",\n    \"text\": \"Journal of Intelligent & Robotic Systems (2019) 96:267-281 https://doi.org/10.1007/510846-018-00975-y\",\n    \"metadata\": {\n      \"filetype\": \"application/pdf\",\n      \"languages\": [\n        \"eng\"\n      ],\n      \"page_number\": 1,\n      \"filename\": \"1_Ramrez-Duque_.pdf\"\n    }\n  },\n  {\n    \"type\": \"Title\",\n    \"element_id\": \"0606ba94dc82b8248e73d33d5dce1c37\",\n    \"text\": \"Robot-Assisted Autism Spectrum Disorder Diagnostic Based on Artificial Reasoning\",\n    \"metadata\": {\n      \"filetype\": \"application/pdf\",\n      \"languages\": [\n        \"eng\"\n      ],\n      \"page_number\": 1,\n      \"parent_id\": \"124519724f8942a945cd489488e97294\",\n      \"filename\": \"1_Ramrez-Duque_.pdf\"\n    }\n  },\n  {\n    \"type\": \"Title\",\n    \"element_id\": \"165abf064c3a95bbee57e8aa4016140c\",\n    \"text\": \"Andr\\u00e9s A. Ramirez-Duque\\u2019 @ . Anselmo Frizera-Neto' - Teodiano Freire Bastos\\u2019\",\n    \"metadata\": {\n      \"filetype\": \"application/pdf\",\n      \"languages\": [\n        \"eng\"\n      ],\n      \"page_number\": 1,\n      \"parent_id\": \"124519724f8942a945cd489488e97294\",\n      \"filename\": \"1_Ramrez-Duque_.pdf\"\n    }\n  }\n]\n","output_type":"stream"}]},{"cell_type":"code","source":"JSON(json.dumps(response.elements, indent=2))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:43:16.032675Z","iopub.execute_input":"2024-05-01T06:43:16.033628Z","iopub.status.idle":"2024-05-01T06:43:16.063324Z","shell.execute_reply.started":"2024-05-01T06:43:16.033582Z","shell.execute_reply":"2024-05-01T06:43:16.061675Z"},"scrolled":true,"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.JSON object>","application/json":[{"type":"Header","element_id":"124519724f8942a945cd489488e97294","text":"Journal of Intelligent & Robotic Systems (2019) 96:267-281 https://doi.org/10.1007/510846-018-00975-y","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"0606ba94dc82b8248e73d33d5dce1c37","text":"Robot-Assisted Autism Spectrum Disorder Diagnostic Based on Artificial Reasoning","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"124519724f8942a945cd489488e97294","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"165abf064c3a95bbee57e8aa4016140c","text":"Andrés A. Ramirez-Duque’ @ . Anselmo Frizera-Neto' - Teodiano Freire Bastos’","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"124519724f8942a945cd489488e97294","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"b2181c6eefd507a2a68fcd65e3af62e0","text":"l.) Check for updates","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"dbc91cf0825c7c18192c20fc0182d322","text":"Received: 25 April 2018 / Accepted: 20 December 2018 / Published online: 29 March 2019 © Springer Nature B.V. 2019","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"d21b4a64a2d8656a0fdf7ab2e89a4916","text":"Abstract","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"08a7af2c20854c0fb98bd73e96e71949","text":"Autism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are found in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral observation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral observation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior assessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum Disorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when compared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding to summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance the traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable network of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face analysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three children in risk of suffering from ASD.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"d21b4a64a2d8656a0fdf7ab2e89a4916","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"5cc5ea18de1f40df2f5b162220386243","text":"Keywords Child-Robot interaction - Autism spectrum disorder - Convolutional neural network - Robot reasoning model Statistical shape modeling","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"d21b4a64a2d8656a0fdf7ab2e89a4916","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"e3b0c44298fc1c149afbf4c8996fb924","text":"","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"d21b4a64a2d8656a0fdf7ab2e89a4916","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"c9234967b670c32478644b4236ec8fd2","text":"1 Introduction","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a0521f85b05b7c844eaf04a2dc2dcece","text":"Research in Child-Robot Interaction (CRI) aims to provide the necessary conditions for the interaction between a child and a robotic device taking into account some fundamental features, such as child’s neurophysical and physical condition, and the child’s mental health [I]. That is how Robot-Assisted Therapies (RAT) using CRI theories have been of interest as an intervention for CwASD, elucidating faster and more significant gains from the therapeutic intervention when compared to traditional therapies [2—4].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"c9234967b670c32478644b4236ec8fd2","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"6426a405691271b1520e4d84c649f619","text":"ASD is a neurodevelopmental disorder that affects people from birth, and its symptoms are found in the early","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"parent_id":"c9234967b670c32478644b4236ec8fd2","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Footer","element_id":"98a0fb74cda85c466d98065917126404","text":"P4 Andrés A. Ramirez-Duque","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Footer","element_id":"bd0bd257779f91a83968e0eca139b144","text":"aaramirezd @gmail.com","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"385803c089c7c20974cb39c700f2ac5f","text":"1 Universidade Federal do Espirito Santo., Av. Fernando Ferrari, 514 (29075-910), Vitoria, Brazil","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3d4308f5726dfe004c98470e156401db","text":"developmental period. Individuals suffering from ASD exhibit persistent deficits in social communication, social interaction and repetitive patterns of behavior, interests, or activities [5]. Some of the ASD signs may be observed before the age of 10 months, although a reliable diagnosis can only be performed at 18 months of age, according to [6], or 24 months according to [7].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"bb0d4e69ca2513889b9a6d3edd27fbef","text":"The use of computer vision to analyze the child’s behaviors, and automated video coding to summarize the interventions, can help the clinicians to reduce the delay of ASD diagnosis, providing the CwASD with access to early therapeutic interventions. In addition, CRI-based intervention can transform traditional diagnosis methods through a robotic device to systematically elicit child’s behaviors that exhibit ASD signs [8].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"bc8ee2a1b59f82b68761db5c263ae82f","text":"Some of the first systems developed to assist ASD therapists and make diagnosis based on robotic devices have primarily been open loop and remotely operated sys- tems. However, these approaches are unable to perform autonomous feedback to enhance the interaction [9-11].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":1,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"8b496bf96bbcc9e5ac11c068b6cfb00c","text":"268","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"c2e41ed2557f021216d7307fbc7fc62a","text":"Nevertheless, different systems are able to modify the behavior of the robot according to environmental interac- tions and the child’s response, using a closed-loop and artificial cognition approaches [12—-16]. These systems have been hypothesized to offer technological mechanisms for supporting more flexible and potentially more naturalistic interaction [17]. In fact, literature reports that automatic robot’s social behaviors modulation according to specifics scenarios has a strong effect on child’s social behavior [12]. However, despite the increase of positive evidence, this technology has rarely been applied to specific ASD diagnosis.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"8b496bf96bbcc9e5ac11c068b6cfb00c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f80d0afc72ff2f1a0d61fec89707a2c1","text":"This work aims to present a robot-assisted framework using an artificial reasoning module to assist clinicians with the ASD diagnostic process. The framework is composed of a responsive robotic platform, a flexible and scalable vision sensor network, and an automated face analysis algorithm based on machine learning models. In this research we take advantage of some neural models available as open sources projects to build a completely new pipeline algorithm for global recognition and tracking of child’s face among many faces present in a typical unstructured clinical intervention, in order to estimate the child’s visual focus of attention along the time. The proposed system can be used in different behavioral analysis scenarios typical of an ASD diagnostic process. In order to illustrate the feasibility of the proposed system, in this paper an experimental trial to assess joint- attention behavior is presented employing an in-clinic setup (unstructured environment).","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"8b496bf96bbcc9e5ac11c068b6cfb00c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"ffc4375d4e443d4ee862bcf1af7d20f9","text":"The main contributions of this paper are: (i) the development of a new artificial reasoning module upon a flexible and scalable ROS-based vision system using state-of-the-art machine learning neural models; (ii) the proposal and implementation of a supervised CRI (child- robot interaction) based on an open source social robotic platform to enhance the traditional tools for ASD diagnosis using an in-clinic setup protocol. For the best of our knowledge, there are no open source projects available for face analysis based on a multi-camera approach using ROS with the characteristics described in our research.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"8b496bf96bbcc9e5ac11c068b6cfb00c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"23cc58a1d4b5d23adc438f5f85d26816","text":"2 Related Work","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"8b496bf96bbcc9e5ac11c068b6cfb00c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"b44ee5e856edf7d88aee0b2717ae336e","text":"Recent researches have shown the acceptance and efficiency of technologies used as auxiliary tools for therapy and teaching of individuals with ASD [18-21]. Such technologies may also be useful for people surrounding ASD individuals (therapists, caregivers, family members). For example, the use of artificial vision systems to measure and analyze the child’s behavior can lead to alternative screening and monitoring tools that help the clinicians to get feedback from the effectiveness of the intervention [22].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"23cc58a1d4b5d23adc438f5f85d26816","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"9b381a1306805b7184431d0cc7c910ac","text":"Additionally, social robots have great potential for aid in the diagnosis and therapy of children with ASD [18, 23]. A higher degree of control, prediction and simplicity may be achieved in interactions with robots, impacting directly on frustration and reducing the anxiety of these individuals [24].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a31722e7f4bd3f6d4d57cd625325eaa6","text":"Respect to the use of computer vision techniques, previous studies already analyzed child’s behaviors, such as visual attention, eye gaze, eye contact, smile events, and visual exploration using cameras and eye trackers [25, 26] and RGBd cameras [27, 28]. These studies have shown the potential of vision systems in improving the behavioral coding in ASD therapies. However, these studies did not implement techniques of CRI to enhance the intervention.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"c780fa9e88ab690ba62a1d5f56d118b5","text":"On the other hand, studies about how CwASD respond to a robot mediator compared to human mediator have been reported, such as intervention scenarios with imitation games [29, 30], telling stories [9] and free play tasks [12, 31]. These works used features, such as proxemics, body gestures, visual contact and eye gaze as behavioral descriptors, whereas the behavior analysis was estimated using manual video coding.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"d33b47497c35ad9d3404251bcf5ead62","text":"Researchers of Vanderbilt University published a series of research showing an experimental protocol to assess joint attention (JA) tasks defined as the capacity for coordinated orientation of two people toward an object or event [6]. The protocol consisted of directing the attention of the child towards objects located in the room through adaptive prompts [32]. Bekele et al. inferred the participant’s eye gaze by the head pose, which was calculated in real-time by an IR camera array [17]. In their last works, Zheng et al. and Warren et al. used a commercial eye tracker to estimated the children’s eye gaze around the robot and manual behavioral coding for global evaluation [10, 33]. However, eye tracker devices require pre-calibration and may limit the movement of the individual. The results of these works showed that the robot attracted children’s attention and that CwASD reached all JA task. Nevertheless, developing JA tasks is more difficult with a robot than with humans [10]. Anzalone et al. developed a CRI scenario using the NAO robot to perform JA tasks, in which the authors used an RGBD camera to estimate only body and head movements. The results showed that JA performance of children with ASD was similar to the performance of TD children when interacting with the human mediator, however, with a robot mediator, the children with ASD presented a lower performance than the TD children, i.e, the children with ASD needed more social cues to finalize the task [34]. Chevalier et al. analyzed in their study, some features, such as proprioceptive and visual integration in CwASD, using an RGBD sensor to record the interventions sessions and manual behavior coding to analyzed the participants’ performance [35]. In none of the previous works, a closed-loop subsystem was","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":2,"parent_id":"4ff9e071d38c013c2f37b690bf641be5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3f63aff9eba8a38fb123a0e16aa9cc6d","text":"implemented to provide some level of artificial cognition to enable automated robot behavior.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"934567c047fab7ddfb647038d29e5667","text":"In contrast with the aforementioned researches, other works implemented automated face analysis and artificial cognition through robot-mediator and computer vision, which analyzed child’s engagement [36, 37], emotions recognition capability [13, 15, 38] and child’s intentions [14, 16]. In these works, two different strategies were implemented, where the most common is based on mono- camera approach using an external RGB or RGBd sensor [15, 36, 37] or using on-board RGB cameras mounted on the robotic-platform [13, 16]. Other strategies are based on a highly structured environment composed of an external camera plus an on-board camera [38] or a network of vision sensors attached to a small table [14]. These strategies based on multi-camera methods improve the system’s performance, but remain constrained in relation to desired features, such as flexibility, scalability, and modularity. Thus, despite the potential that these techniques have shown, achieving automated child’s behavior analysis in a naturalistic way into unstructured clinical-setups with robots that interact accordingly remains a challenge in CRI.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"ec9709f8f50cc3c1d0be72c74edc0106","text":"3 System Architecture Overview","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f13e55a84d621752dfa796a7ea92b49e","text":"The ROS system used in this work is a flexible and scalable open framework for writing modular robot- centered systems. Similar to a computing operating system, ROS manages the interface between robot hardware and software modules and provides common device drivers, data structures and tool-based packages, such as visualization and debugging tools. In addition, ROS uses an interface definition language (IDL) to describe the messages sent between process or nodes, this feature facilitates the multi- language (C++, Python and Lisp) development [39].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"ec9709f8f50cc3c1d0be72c74edc0106","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"2648c893a24c0878036c78a63dd7c01f","text":"The overall system developed here was built using a node graph architecture, taking advantages of the principal ROS design criteria. As with ROS, our system consists of a number of nodes to local video processing together a robot’s behavior estimation, distributed around a number of different hosts and connected at runtime in a peer-to- peer topology. The inter-node connection is implemented as a hand-shaking and occurs in XML-RPC protocol along with a web-socket communication for robot’s web- based node (/ONO_node, see Fig. 1). The node structure is flexible, scalable and can be dynamically modified, i.e., each node can be started and left running along an experimental session or resumed and connected to each other at runtime. In addition, from a general perspective, any robotic platform with web-socket communication can be integrated. The developed system is composed of two interconnected modules as shown in Fig. 1: an artificial","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"ec9709f8f50cc3c1d0be72c74edc0106","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"f747870ae666c39b589f577856a0f719","text":"269","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"c63ce927c87ba0c7f4d61ae545cd8514","text":"reasoning module and a CRI-channel module. The module architectures are detailed in the following subsections.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"f747870ae666c39b589f577856a0f719","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"11c7b34fe45dd618090905eac8a1eca5","text":"3.1 Architecture of Reasoning Module","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"f747870ae666c39b589f577856a0f719","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"6621a1804c4d70ff424d1c4a29281367","text":"In this module, a distributed architecture for local video processing is implemented. The data of each RGBD sensor in the multi-camera system are processed for two nodes, in which the first is a driver level node and the second 1 node transforms the is a processing node. The driver streaming data of the RGBD sensor into the ROS messages format. The driver addresses the data through a specialized transport provided by plugings to publishes images in a compressed representations while the receptor node only sees sensor_msgs/Image messages. The data processing node executes the face analysis algorithm. This node uses a image_transport subscriber and a ROS packages called CvBridge to turn the data into a image format supported for the typical computer vision algorithms. Later, the same node publishes the head pose and eye gaze direction by means of a ROS navigation message defined as nav_msgs/Odometry.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"11c7b34fe45dd618090905eac8a1eca5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3989663f647bb623e56c67b44b954f14","text":"An additional node hosted in the most powerful workstation carries out a data fusion of all navigation messages that were generated in the local processing stage. In addition to the fusion, this node computes the visual focus of attention (VFOA) and publishes this as a std_msgs/Header, in which the time stamp and the target name of the VFOA estimation are registered.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"11c7b34fe45dd618090905eac8a1eca5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"94f8a58878c9c429b08e0f78aa377964","text":"3.2 Architecture of CRI-Channel","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"f747870ae666c39b589f577856a0f719","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"0a129621d09fcd4f2ef0aeaf97fe1553","text":"The system proposed here has two bidirectional communi- cation channels, a robot-device, and a web-based applica- tion to interact with both the child and the therapist. The robot device can interact with the CwASD executing differ- ent physical actions, such as facial expression, upper limb poses, and verbal communication. Thus, according to the child’s performance, the reasoning module can modify the robot’s behavior through automatic gaze shifting, chang- ing the facial expression and providing sound rewards. The client-side application was developed to allow the therapist to control and register all step of the intervention proto- col. This interface was also used to supervise and control the robot’s behavior and to offer feedback to the therapist about the child’s performance along the intervention. This App has two channels of communication for interacting with the reasoning module. The first connection uses a web- socket protocol and a RosBridge_suite package to support the interpretation of ROS messages, as well as, JSON-based commands in ROS. The second one uses a ROS module","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94f8a58878c9c429b08e0f78aa377964","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f189d346a2450dd0c4d12bb69c2e4a9e","text":"!Tools for using the Kinect One (Kinect V2) in ROS, https://github. com/code-iai/iai _kinect2.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94f8a58878c9c429b08e0f78aa377964","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94f8a58878c9c429b08e0f78aa377964","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":3,"parent_id":"94f8a58878c9c429b08e0f78aa377964","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"d8d1790737d57ac4fe91a2c0a28087c0","text":"270","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"120b81905447e609be62deec33618c56","text":"Fig.1 Node graph architecture of the proposed ROS-based system. The system is composed of two interconnected modules, an artificial reasoning module and a CRI-channel module. The ONO web server has two way of bidirectional communication: a websocket and a standard ROS Subscriber","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"922ee7bade7231184056f0f4063ff6f2","text":"ROS Manager /ROS_core /Kinect2_bridge g /Fusion_Node /kinect2_n/hd/ image_color_rect ry Sound /ROS_bridge Controller /tf_transform websocket /Detect_Recognize X /nav_msgs/ Head_Odomets ONO Web Server g /NFOA_Node /known_face n /std_msgs/header/ /ONO_Node Servo g vfoa_msgs Controller /CLNF_node 8 Artificial Reasoning Module Child-Robot Interaction Modul T Topic Wire Topic Wireless - Local leo & oI Connection Connection 5, \",","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f8188ba0a0b0a7b478fcf5037a4ffc9e","text":"developed in the server-side application to directly run a ROS node and communicate with standard ROS publishers and subscribers.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"b89358afb1f7731110d3282afe7ab44f","text":"The CRI is implemented through the open source platform for social robotics (OPSOR0),> which is a promising and straightforward system developed for face to face communication composed of a low-cost modular robot called ONO (see Fig. 2) and web-based applications [40]. Some of the most important requirements and characteristics that make ONO interesting for this CRI strategy are explained in the following sections.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"73dc2dc4ae72394d1534f791360e0fc5","text":"The robot is covered in foam and also fabric to have a more inviting and huggable appearance to the children. The robot has an oversized head to make its facial expressions more prominent and to highlight the importance for communica- tion and emotional interaction. As a consequence of its size and pose, children can interact with the robot at eye height when the robot is placed on a table.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"232cc5c984860a865612bfeff5abd952","text":"The robot ONO has not a predefined identity, as the only element previously conceived is the name. Unlike other robots that have well-defined identities, such as Probo [9] or Kaspar [41], in this work the ONO’s identity is built with the participation of the child through a co-creation process. For this reason, a neutral appearance is initially used. In the","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Footer","element_id":"7a5af435bcb1244896d73526fbc57451","text":"20pen Source Platform for Social Robotics (OPSORO) http://www. opsoro.com.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"51f83d0411a6ea0b4d319f5ff16165a8","text":"4 The Robotic Platform ONO","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"e9423f8b5da94e8db9eca116b1180ba1","text":"4.1 Appearance and Identity","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"FigureCaption","element_id":"99a8bb438ecb5001f9544060f47452f8","text":"intervention, the therapist can provide the child with clothes and accessories to define the identity of ONO.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"e9423f8b5da94e8db9eca116b1180ba1","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"5f819df89f71dc03d45107e1aa058c21","text":"4.2 Mechanics Platform","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"8229b5b8e451ac161af0079e59c9ead5","text":"As the initial design of ONO is composed only of the actuated face, in this work it was needed to provide the ONO with some body language. For this purpose, motorized arms were designed and implemented.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"5f819df89f71dc03d45107e1aa058c21","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"ae86dfe11d99ca7420703635a558963f","text":"The new design of ONO has a fully face and two arms actuated, giving a total of 17 Degrees of Freedom (DOF). The ONO is able to perform facial expressions and nonverbal cues, such as waving, shake hands and pointing towards objects, moving its arms (2 DOF x 2), eyes (2 DOF x 2), eyelids (2 DOF x 2), eyebrows (1 DOF x 2), and mouth (3 DOF). The robot has also a sound module that allows explicit positive feedback as well as reinforcement learning through playing words, conversations and other sounds.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"5f819df89f71dc03d45107e1aa058c21","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"918e1207453bf0835345660335ee941b","text":"4.3 Social Expressiveness","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"948368b22e93def0dccfdadd03e5fb40","text":"In order to improve social interaction with a child, the ONO is able to exhibit different facial expressions. The ONO’s expressiveness is based on the Facial Action Coding System (FACS) developed in [42]. Each DOF that composes the ONO’s face is linked with a set of Action Units (AU) defined by the FACT, and each facial expression is determined for specific AU values. The facial expressions are represented as a 2D vector fe = (v, a) in the emotion circumplex model defined by valence and arousal [9]. In this context, the basic facial expressions are specified on a unit circle, where the neutral expression corresponds to the origin of the space feo = (0,0). The relation between the DOF position and the AU values is resolved through a lookup table algorithm using a predefined configuration file [40].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"918e1207453bf0835345660335ee941b","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"918e1207453bf0835345660335ee941b","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":4,"parent_id":"918e1207453bf0835345660335ee941b","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"fa07f5351fe49a7290272ae8ace3b96e","text":"Fig.2 ONO robot, developed through the open source platform for social robotics (OPSORO)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"e3b0c44298fc1c149afbf4c8996fb924","text":"","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"3635a91e3da857f7847f68185a116a52","text":"271","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"316645f2c2e33bfb331960966ad61636","text":"4.4 Adaptability and Reproducibility","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"3635a91e3da857f7847f68185a116a52","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f0caaf8fe4d8f2e0a2aa4d264c46c237","text":"The application of the Do-It-Yourself (DIY) concept is the principal feature of ONO’s design, which facilitates its dissemination and use in research areas other than engineering as health care. These characteristics allow ONO building for any person without specialized engineering knowledge. Additionally, it is possible to replicate ONO without the need for high-end components or manufacturing machines [40]. The electronic system is based on a Raspberry Pi single-board computer combined with a custom OPSORO module with circuitry to control up to 32 servos, drive speakers and touch sensors. Any sensor or actuator compatible with the embedded communication protocols (UART, I2C, SPI) implemented on the Raspberry Pi can be used by this platform.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"316645f2c2e33bfb331960966ad61636","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"d43cb68c3a133a83845d62e21dfb6682","text":"4.5 Control and Autonomy","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"3635a91e3da857f7847f68185a116a52","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"879f3b9180ddbb5052e1ca580542ca43","text":"With the information delivered for the automated reasoning module, it was possible to automate the ONO’s behavior and, then, the robot can infer and interpret the children’s intentions to react most accurately to the action performed by them, thus enabling a more efficient and dynamic interaction with ONO. In this work, the automated ONO’s behavior is partially implemented, i.e., the framework can modify some physical actions of ONO using the feedback information about the child’s behavior. The actions suitable to be modified are gaze shift toward the child in specifics events, changing from neutral to positive facial expression when the child looks toward the target, and providing sound rewards. Also, an Aliveness Behavior Module (ABM) is implemented to improve the CRI, which consist of blinking the robot’s eyes and changing its arms among","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"d43cb68c3a133a83845d62e21dfb6682","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a01bc141b634e98be4c72f534f7c8f8f","text":"some predefined poses. Also, the robot can be manually operated through a remote controller hosted in the client- side application.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"d43cb68c3a133a83845d62e21dfb6682","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"6e863ca5ac725b81a05813361d2ebf4f","text":"5 Reasoning Module: Machine Learning Methods for Child’s Face Analysis","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"3635a91e3da857f7847f68185a116a52","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a20d4b776fd9305044be7f8b1b941fd4","text":"The automated child’s face analysis consists of monitoring nonverbal cues, such as head and body movements, head pose, eye gaze, visual contact and visual focus of attention. In this work, a pipeline algorithm is implemented using machine learning neural models for face analysis. The chosen methods were developed using state-of-art trained neural models, available by DIib> [43] and OpenFace* [44]. Some modification such as, turn the neural model an attribute of the ROS node class and evaluate this in each topic callback, were needed to run the neural models into a common ROS node.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"6e863ca5ac725b81a05813361d2ebf4f","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"704e75e5a2299db3c602dc4c8b521526","text":"The algorithm proposed for child’s face analysis involves face detection, recognition, segmentation and tracking, landmarks detection and tracking, head pose, eye gaze and visual focus of attention (VFOA) estimation. In addition, the architecture proposed here also implement new methods for asynchronous matching and fusion of all local data, visual focus of attention estimation based on Hidden Markov Model (HMM) and direct connection with the CRI-channel to influence the robot’s behaviors. A scheme of the pipeline algorithm is shown in Fig. 3.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"parent_id":"6e863ca5ac725b81a05813361d2ebf4f","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Footer","element_id":"f8927daa906dc935a513aef33ea74af2","text":"3Dlib C++ Library http://dlib.net/.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"54b6ffc64820488c897cca5ba264322d","text":"4A Open Source Facial Behavior Analysis https:/github.com/ TadasBaltrusaitis/OpenFace.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":5,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"1c6c0bb2c7ecdc3be8e134f79b9de451","text":"272","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"6262a06473304e92fa49d6ab6660c070","text":"Fig.3 Pipeline algorithm of the automated child’s face analysis","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"1c6c0bb2c7ecdc3be8e134f79b9de451","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"3b7333e125230d7891ffbc4d0aca2584","text":"Face Detection Head Pose and Face Recognition Eye Gaze Estimation “oomon = Scooooo o R v g CLNF Statistical CNN_MMOD CNN_ResNet29 Shape Model","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"04c5cd7a548709b0f44d8defc51bb34e","text":"5.1 Child’s Face Detection and Recognition","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"0bee7978856d9731f5c1d4196e839c75","text":"The in-clinic setup requires differentiate the child’s face from other faces detected and found in the scene. For this reason, a face recognition process was also implemented in this work. First, the face detection is executed to initialize the face recognition process and, subsequently, initialize the landmarks detection. In this work, both detection and recognition are implemented using deep learning models, which are described in this section.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"04c5cd7a548709b0f44d8defc51bb34e","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"44ec71e381b303796e5ae3609e3aaf4f","text":"In the detection process, a Convolutional Neural Network (CNN) based face detector with a Max-Margin Object Detection (MMOD) as loss layer is used [45]. The CNN consist first of a block composed of three downsampling layers, which apply convolution with a 5x5 filter size and 2x2 stride to reduce the size of the image up to eight times its original size and generate a feature map with 16 dimensions. Later, the result are processed for one more block composed of four convolutional layers to get the final output of the network. The three first layers of the last block have 5x 5 filter size and 1x1 stride, but, the last layer has only 1 channel and a 9x9 filter size. The values in the last channel are large when the network thinks it has found a face at a particular location. All convolutional block above are implemented with two additional layers among convolutional layers, pointwise linear transformation, and Rectified Linear Units (RELU) to apply the non-saturating activation function f(x) = max(0, x). The training dataset used to create the model is composed of 6975 faces and is available at Dlib’s homepage.’","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"04c5cd7a548709b0f44d8defc51bb34e","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"7c3fb7eafd5e42fff2ac1b6b93d28d79","text":"The face recognition algorithm used in this work is inspired on the deep residual model from [46]. The","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"04c5cd7a548709b0f44d8defc51bb34e","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"12480823cf810828b66bfe4303052476","text":"Shttp://dlib.net/files/data/dlib_face_detection_dataset-2016-09-30.tar. gz","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"04c5cd7a548709b0f44d8defc51bb34e","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"e333fdf3a8b197ae5548c02a9a8ab8fe","text":"residual network (ResNet) model developed by He er. al reformulates the convolutional layers to learn a residual = — functions F(x) : H(x) — x with reference to the layer inputs x, instead of learning unreferenced functions. In the practical implementation, the previous formulation means inserting shortcut connections, which turn the network into its counterpart residual version [46]. The CNN model then transforms each face detected to a 128D vector space in which images from the same person will be close to each other, but faces from different people will be far apart. Finally, the faces are classified as child’s face, caregiver’s face and therapist’s face.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"04c5cd7a548709b0f44d8defc51bb34e","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"151bc7127f557ac468b665bca285696a","text":"Both detection and recognition CNN model were implemented and trained from [43] and released in Dlib 19.6.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"04c5cd7a548709b0f44d8defc51bb34e","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"33c2910e1c5f12582e29dc9cb02e5144","text":"5.2 Face Analysis, Landmarks, Head Pose and Eye Gaze","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"1630a23dfae3dab4e8fafc4628b1c75a","text":"This work uses the technique for landmarks detection, head pose and eye gaze estimation developed by Baltrusaitis et al., named Conditional Local Neural Fields (CLNF) [47]. This technique is an extension of the Constrained Local Model (CLM) algorithm using specialized local detectors or patch experts. CNLF model consists of a statistical shape model, which its learned from data examples and is parametrized for m components of linear deformation to control the possible shape variations of the non-rigid objects [48]. Approaches based on CLM [49, 50] and CLNF [47] model the object appearance in a local fashion, i.e, each feature point has its own appearance model to describe the amount of misalignment.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"33c2910e1c5f12582e29dc9cb02e5144","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"72046e6f76dbca13aee8de53461b1374","text":"CLNF-based landmark detection consists of three main parts: the shape model, the local detectors or patch experts, and the fitting algorithm, which are detailed below.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"33c2910e1c5f12582e29dc9cb02e5144","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":6,"parent_id":"4ff9e071d38c013c2f37b690bf641be5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"c2a7d9ca8ec811a9ac6a10ba44a98c46","text":"5.2.1 Shape Model","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"50bbce4d1aa5b8df249efb0837eeb201","text":"The CLNF technique uses a linear model to describe non- rigid deformations called Point Distribution Model (PDM). The PDM is used to estimate the likelihood of the shapes being in a specific class, given a set of feature points [48]. This is important for model fitting and shape recognition.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"c2a7d9ca8ec811a9ac6a10ba44a98c46","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"cd677ee0d0b0b0c4c1c5aaaa7c10cf64","text":"The shape of a face that has n landmark points can be described as:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"c2a7d9ca8ec811a9ac6a10ba44a98c46","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"d16e4edd17bf9599cd7e872029d9b1c4","text":"X =[X1, X2, o X Y1, Yo, < Yn, Z1, Zs, s Znl,","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"48584011972a8111da2824aafbe76d08","text":"and the class that describes a valid instance of a face using PDM can be represented as:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"2e376eb78942d31c6434a003f4a34d1d","text":"X =X+ &g, 2","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"2af079da4cfb10096a345abb23468e7f","text":"where X is the mean shape of the face, ® described the principal deformation modes of the shape, and ¢ represent the non-rigid deformation parameters. Both X and & are learned automatically from labeled data using Principal Component Analysis (PCA). The probability density distribution of the instances into the shape class is expressed as a zero mean Gaussian with Covariance matrix A= (A1 .5 Am]) evaluated at g:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"6de12bde8046f1e96e25cc9e482521fe","text":"p(@=N(g;0;A) = ¢@?Wﬂﬂ”p{ —%wTA”qﬁ 3)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3a145d014c74d360054a8499b7e57164","text":"Once the model is defined, it is necessary to place the 3D PDM in an image space. The following equation is used to transform between 3D space to image space using weak perspective projection [49]:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"8bd675e1332607740164f2cde3722f8b","text":";=5 Rap- (Xi + ®iq) +1, “)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f946f2084ed414ba026f7b2eb600b8b6","text":"where X; = [, 3. % ]T is the mean value of the i'” landmark. The instance of the face in an image is, therefore, controlled using the parameter vector p = [s,w,1,q], where g represents the local non-rigid deformation, s is a scaling term, w is the rotation term that controls the 2 x 3 matrix Ryp, and ¢ is the translation term.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"93257843e393e84f802a7835e3625cca","text":"The global parameters are used to estimate the head pose in reference to the camera space using orthographic camera projection and solving the Perspective-n-Point (PnP) problem respect to the detected landmarks. The PDM used in [44] was trained on two public datasets [51, 52]. This result in a model with 34 non-rigid (Principal modes) and 6 rigid shape parameters.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"1071415da470bf093cbe8f152fae2160","text":"5.2.2 Patch Experts","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"2b47ec6ed03aab52d394765245a4c7c6","text":"The patch experts scheme is the main novelty implemented in the CLNF model. The new Local Neural Field (LNF)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"1071415da470bf093cbe8f152fae2160","filename":"1_Ramrez-Duque_.pdf"}},{"type":"UncategorizedText","element_id":"e97e45f6cfa81a1cf1a93443a2898817","text":"(6]","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"1071415da470bf093cbe8f152fae2160","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"303c8bd55875dda240897db158acf70a","text":"273","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"14cb188a9dbeef84b2ed55cb668483d9","text":"patch expert takes advantage of the non linear relationship between pixel values and the patch response maps. The LNF captures two kinds of spatial characteristics between pixels, such as similarity and sparsity [47].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"303c8bd55875dda240897db158acf70a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a51a142e484e4e83b1958c27d75e6477","text":"LNF patch expert can be interpreted as a three layer perceptron with a sigmoid activation function followed by a weighted sum of the hidden layers. It is also similar to the first layer of a Convolutional Neural Network [44]. The new LNF patch expert is able to learn from multiple illuminations and retain accuracy. This becomes important when creating landmark detectors and trackers that are expected to work in unseen environments and on unseen people.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"303c8bd55875dda240897db158acf70a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"89baf7b3830ad71437114f097d5bf71b","text":"The learning and inference process is developed using a gradient-based optimization method to help in finding locally optimal model parameters faster and more accu- rately.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"303c8bd55875dda240897db158acf70a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a9da04f0e17d931d3773ed86771bbf4c","text":"In the CLNF model implemented in [44], 28 set in total of LNF patch experts were trained for seven views and four scales. The framework uses patch experts specifically trained to recognize the eyelids, iris and the pupil, in order to estimate the eye gaze [44].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"303c8bd55875dda240897db158acf70a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"a0924207beeaa89307ab67c53d0672e0","text":"5.2.3 Fitting Algorithm","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"303c8bd55875dda240897db158acf70a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"b9926a8f0f659b290a9b03940854851a","text":"For each new image or video frame, the fitting algorithm of CLNF-based landmark detection process attempts to find the value of the local and global deformable model parameters p that minimizes the following function [49]:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"parent_id":"a0924207beeaa89307ab67c53d0672e0","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"ff26cb3d702095313e1176edbc5b2c69","text":"Ep) = R(P)Zn:Di(xi; D)., (©)) i=1","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3701b13cdb7c96a05795eee297848ade","text":"where R is a weight to penalize unlikely shapes, which depends on the shape model, and D represents the misalignment of the i* landmark in the image Z, which is function of both the parameters p and the patch experts. Under the probabilistic point of view, the solution of (5) is equivalent to maximize the a posteriori probability (MAP) of the deformable model parameters p:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"c076cd9edf5cbb393ed333e9560f7ee4","text":"plplli=1_,T) p((p))ﬁp(li =11x,D), (6 i=1","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"8f34cfbc52648e4fa6deaa300992b899","text":"where, [; € {1, —1} is a discrete random variable indicating whether the i landmark is aligned or misaligned, p(p) is the prior probability of the deformable parameters p, and p (I; = 1 | x;, Z) is the probability of a landmark being aligned at a particular pixel location x;, which is quantified from the response maps created by patch. Therefore, the last term in (6) represents the joint probability of the patch expert response maps.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"09ad8eedc0a797963ef85fcb44379f51","text":"The MAP problem is solved using a optimization strategy designed specifically for CLNF fitting called non-uniform","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":7,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"718127812c05853f0bec61582a4a3840","text":"274","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"32a802030298dc7eaf1c09d2a8de4456","text":"regularized landmark mean shift (NU-RLMS) [47], which uses two step process. The first step evaluates each of the patch experts around the current landmark using a Gaussian Kernel Density Estimator (KDE). The second step iteratively updates the model parameters to maximize (6).","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"718127812c05853f0bec61582a4a3840","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"d307c38c8e91d89698d6efc646f55f2f","text":"The NU-RLMS uses expectation maximization algo- rithm, where the E-step involves evaluating the posterior probability over the candidates, and the M-step finds the parameter updated through the mean shift vector v. The mean shift vector points in the direction where the feature point should go, but the motion is restricted by the statisti- cal shape model and the R(p). This interpretation leads to the new update function:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"718127812c05853f0bec61582a4a3840","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"6c03ae8069f32ca57ab80a8d338971c5","text":"(7 argmin {17 ap — VI, + 7 lIp+ Al } Ap","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"439bbe805f5c0b6464f20064e1a27b75","text":"where r is a regularization term, J is the Jacobian, which describe how the landmarks location are changing~ based 1 = on the infinitesimal changes of the parameters p, A~ =","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"6a4a0af98f36e14a38958d4b6f666304","text":"diag([0: 0; 0; 0; 0; 0; A7 \"~ ! m 1), and W allows for weighting of mean-shift vectors. Non-linear least squares leads to the following update rule:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"e962514ae068cb21c6c62292800ede38","text":"®) Ap=— (JTWJ + rA*’) (rA*‘p - JTWv) A","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"724b73b20de5f0bc03650517b508c43f","text":"To construct W, the performance of patch experts on training data is used.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"bef7b8fb55d32e9be8861ae9394d3023","text":"5.3 Data Fusion","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4c74f7aa1ebd02e570d80199e636a3a3","text":"The fusion of the local results for the head pose estimation is done applying a consensus over the rotation algorithm [53]. This algorithm consists of calculating the weighted average pose between each camera estimation and its immediate sensors’ estimation neighbors using the axis- angle representation. The local pose is penalized by two weights: the alignment confidence of landmarks detection procedure and the Mahalanobis distances between the head pose and a neutral pose.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"bef7b8fb55d32e9be8861ae9394d3023","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"d008a8383635cf9b99a932e161c72ca0","text":"5.4 Field of View (FoV) and Visual focus of Attention (VFOA)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"9e8de3958c9ab1551732759617c45671","text":"The VFOA estimation model is implemented as a dynamic Bayesian network through a Hidden Markov Model (HMM). The model assumes a specific set of child’s attention attractors or targets F. The estimation process decodes the sequence of child’s head poses H; = (H“\", HP\"\") ¢ R? in terms of VFOA states F; € F at time 7 [54]. The probability distribution of the head poses in reference to a given VFOA target is represented by a Gaussian distribution, whereas the transitions among","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"d008a8383635cf9b99a932e161c72ca0","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3ff8aaa5973afa0e2f061798f7ac548a","text":"these targets are represented by the transition matrix A. The HMM equations can then be written as follows:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"4f4c0a21c866e111c55a458351834bba","text":"P(H; | Fy = f.ul) = N(H| 1 (f), Bu(f) (©)] PFi=flFoi=f)=A, (10)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"7db9df72f6b70944c59e27649300859e","text":"The Gaussian covariances is defined manually to reflect target sizes and head pose estimation variability. Moreover, the Gaussian means corresponding to each specific target Hf is calculated through a gaze model that sets this parameter as a fixed linear combination o the target direction and the head reference direction [55]:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"537272ffb5a6f75fe16a56cdc9274537","text":"W) =axpu(f) + (2 —a) xRy, an","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"8a851ea4ada30275fc7f1c3e9e5181b9","text":"where * denotes the component wise product 1o = (1, 1), a = (¥, aPitchy = (0.7,0.5) are adjustable constants that describe the fraction of the gaze shift that corresponds to the child’s head rotation, u,; € (R2)K is the directions of the given K targets, and R, € R? represents the reference direction, which is the average head pose over a time window WX. The above assumption describes the body orientation behavior of any child who tends to orient himself/herself towards the set of gaze targets to make more comfortable to rotate his/her head towards different targets [55].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Formula","element_id":"23b453b82a80c0e371697c03b34669e0","text":"R,=% Zt: H;. (12) i=t—WR","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"473ac6e8b2b78f5fd3bdaafb8856db00","text":"Finally, for the estimation of the VFOA sequence a classic Viterbi algorithm of HMM is implemented [54].","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"bdef573e52c8eedae80011811f007ae1","text":"6 Case Study","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"219dd103bf0afdcdb5c31cb8a38cc3d8","text":"For the case study, the vision system is composed of three Kinect V2 sensors. Each sensor is connected to a workstation equipped with a processor of Intel Core i5 family and a GeForce GTX GPU board (two workstation with GTX960 board, and one workstation with GTX580 board). All workstation are connected through a local area network synchronized using the NTP protocol.® The sensors were intrinsically and extrinsically calibrated through a conventional calibration process using a standard black- white chessboard.”","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"bdef573e52c8eedae80011811f007ae1","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"8da83cab572b8ea96ccf19b1bab70505","text":"6.1 In-clinic Setup","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"2143d96dc572f120a17fce1e77ddaba4","text":"A multidisciplinary team of psychologists, doctors and engineers developed a case study using a psychology room equipped with a unidirectional mirror to perform behavioral","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"8da83cab572b8ea96ccf19b1bab70505","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"dc9c6b55d572a6a0fba649f7b59331dc","text":"6Network Time Protocol Homepage, http://www.ntp.org.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"8da83cab572b8ea96ccf19b1bab70505","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"dfa7ebdd674e1ff703fd4c5d8cd98c58","text":"TTools for using the Kinect One (Kinect V2) in ROS, https:/github. com/code-iai/iai _kinect2.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"8da83cab572b8ea96ccf19b1bab70505","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":8,"parent_id":"4ff9e071d38c013c2f37b690bf641be5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"e327f63719d2c8d7b8e1503510b66541","text":"Fig.4 Representation of the interventions room of in-clinic setup","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"265d9bd2c3c186208956b3eaf068a9cc","text":"| * One-way || mirror .0\\7 Therapist Engineers Child e L2k ¢ m n 8] Parent/ Caregiver © ° Robot & (] p—","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"1562dfacc4ef2bfc831c9b5dc7dd2024","text":"observation appropriately. The room was prepared with a table and three chairs: one for the child, another for the caregiver and a third one for the therapist. The robot was placed on the table, and the following toys, a helicopter, a truck and a train, were attached to room’s walls. The RGBD sensors were located close to the walls, and no additional camera was placed on the robot or the table, so as not to attract the child’s attention. A representation of the interventions room of in-clinic setup is shown in Fig. 4.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"c6171ed108124da800a2d4670121407a","text":"In this work, a technology-based system was used as a tool in various stages of the ASD diagnostic process. The framework can be implemented to extract different","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"c1c554989a34f76d43020c7d45d0acf0","text":"6.2 Intervention Protocol","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"d3ef25b47a95892d5cd724937a903ac8","text":"behavioral features to be assessed, e.g., eye contact, stereotyped movements of the head, concentration and excessive interest in objects or events. However, for the scope of this research, a specific clinical setup intervention to assess Joint Attention (JA) behaviors is presented. The intervention aims to evaluate the capacity of JA; which can be divided into three classes: initiation of joint attention (IJA), responding to joint attention bids (RJA), and initiation of request behavior (IRB) [6]. The therapist guides the intervention all the time and leverages the robot device as an alternative channel of communication with the child, for the above, both the specialist and the robot remained in the room during the intervention. The children were accompanied throughout the session by a caregiver who was oriented not to help the child in the execution of the","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"parent_id":"c1c554989a34f76d43020c7d45d0acf0","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"3a1dfb05d7257530e6349233688c3e12","text":"275","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"ed42dd4124eb17181c5db9e1222fdadb","text":"Fig.5 The child’s nonverbal cues elicited by the CRI, to look towards the therapist, towards the robot, point and self occlusion","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"parent_id":"3a1dfb05d7257530e6349233688c3e12","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"74e34732952b1eb11989d855fdfbbec0","text":"Look At Point Self-Occlusion LI - \\-7 & | \\E7 ‘ =3 | o, s","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"FigureCaption","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":9,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"c76b405781134be1dab7fe45adfb8c32","text":"276","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"7c4466bb8a1362f959c1a1221ba0e60b","text":"Fig.6 Performance of the child’s face analysis pipeline for the case study. Face detection and recognition, landmarks detection, head pose and eye gaze estimation were executed","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"c76b405781134be1dab7fe45adfb8c32","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"5911887bc0dfb46972da482bed9c054b","text":"Face Detection and Recognition Landmarks Detection Head Pose Unknown Unknown B9 Unknown .“'L,","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"db8c3b26e2cd9d95cb269c593bdc2b54","text":"an d Eye Gaze","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"e951371b3268d7dee477123e27a32fe3","text":"tasks. The exercise developed aimed to direct the attention of the child towards objects located in the room through stimuli, such as, look at, point and speak. The stimuli were generated first only by the therapist and later just by the robot.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"db8c3b26e2cd9d95cb269c593bdc2b54","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"c06bde9600062185ac41ae167f675ed9","text":"6.3 Subjects","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"d90a8232e9763bc44741003a1228e21c","text":"Three children without confirmed ASD diagnosis, but with evidence of risk factors, and three typically developing (TD) children as the control group participated in the experiments. All volunteers participated with their parent’s consent, which were five boys (3 ASD, 2 TD) and one TD girl, between 36 months to 48 months. Each volunteer participated in one single session. The goal was to analyze the based-line of the child’s behavior and establish differences in the behavioral reaction between TD and ASD","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"c06bde9600062185ac41ae167f675ed9","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"63b422a56c8dcacc5dc08cfcdb600ce3","text":"children for stimuli generated through CRI and leverage the novelty effect raised by the robot mediator.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"c06bde9600062185ac41ae167f675ed9","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"b758c27a1393265499d2f0ebb5f1ce50","text":"7 Results and Discussion","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"d01631b56f6d4f3763634b2e2552736a","text":"The child’s nonverbal cues elicited by the CRI can be observed in Fig. 5. Some examples of children’s behavior tagged to perform the behavioral coding are shown in the six pictures. The tagged behaviors were: to look towards an object, towards the robot, and towards the therapist, to point and, to respond to a prompt of both mediators and self occlusion. Typical occlusion problem, as occlusion by hair, hands and the robot were detected.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"b758c27a1393265499d2f0ebb5f1ce50","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"3a8c97d1a33502a4a0f4169cf2163ce7","text":"The performance of video processing in the proof of concept session is reported in Fig. 6. In the case study sessions, the child’s face detection and recognition, the","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"b758c27a1393265499d2f0ebb5f1ce50","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"b987c0ac5ecf8a20dcb2156b4cbbbcfd","text":"Fig.7 Evolution over time of the child’s head/neck rotation (yaw rotation) for a TD group","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"parent_id":"b758c27a1393265499d2f0ebb5f1ce50","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"79cb4c273ae8985f72235d1f1ee9f081","text":"—Yaw Angle —Moving Average Trend = S o =3 o % 10 20 30 40 50 60 70 Orientation (Degrees) Time (s) 50 o -5 b -10 I |—Yaw Angle —Moving Average Trend 10 20 30 40 50 60 70 80 90 100 Orientation (Degrees) Time (s) —100 - c \\—Vaw Angle —Moving Average Trend “\\g . e S Orientation (Degree: 10 20 30 40 50 60 70 80 Time (s)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"FigureCaption","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":10,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"57f91f7537f4a1a864bba83353e006a4","text":"— Yaw Angle — Moving Average Trend 100 50 ey y i w | Orientation (Degrees) DAL 50 B 10 20 30 70 Time (s) b 100 — ; 50 — i M N E I(‘ \" W VAR ? » \\ A ) W Orientation (Degrees) i~ ’ Wi 50 % % ? 10 25 30 T N TN N 100 s W AR AV i O Orientation (Degrees) RN 50 7 VFOA Targets: [N Therapist | Robotono [ Helicopter [N Tuck [N i","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"FigureCaption","element_id":"855641ddbc28d878276475b93519e2d0","text":"Fig.8 Evolution over time of the child’s head/neck rotation (yaw rotation) for a TD volunteer and VFOA estimation results","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"27d719c754aacd492a6dc8a1b7661935","text":"277","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"763374283fedce815ab67759b141cff1","text":"landmarks detection, head pose and eye gaze estimation for different viewpoints are shown in Fig. 3. The recognition process was able to detect all faces in the session successfully in most cases.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"parent_id":"27d719c754aacd492a6dc8a1b7661935","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"95ad5640e2c2bc2d0cf30c4e75d70bb2","text":"The child’s head pose was captured throughout the session and analyzed automatically to estimate the evolution over time of child’s head and the VFOA. Along the session, the child’s neck right/left rotation movement was predominant (Yaw axis), while the neck flexion/extension (Pitch axis) and neck R/L lateral flexion movements (Roll axis) remained approximately constant. The Yaw rotation of the TD children group is reported in Fig. 7. The vertical light blue stripe indicates the intervention period with therapist- mediator, and the vertical light green stripe represents the period with robot-mediator. The continuous blue line represents the raw data recorded, and the continuous red line describes the average data trend. From the observation of the three plot, the TD children started the intervention looking towards the robot, evidently, the robot was a naturalistic attention attractor. Subsequently, when the therapist begins the protocol explaining the tasks, the children attention shifts towards the therapist. The children remained this behavior until that the therapist introduced the robot- mediator. In this transition, the children’s behaviors, such as,","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"parent_id":"27d719c754aacd492a6dc8a1b7661935","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"def1887c4258ff90c4fe5158a216de8b","text":"RJA and IJA toward the therapist were observed. Once the therapist changed the mediation with the robot, the children turned his/her attention to the robot and the objects in the room.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"parent_id":"27d719c754aacd492a6dc8a1b7661935","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"bbc60c54e49691d27fc4c55216f469b9","text":"A more detailed analysis of one of the TD volunteers is shown in Fig. 8. The plot (A) shows the overall intervention session; the plot (B) and plot (C) are a zoom of the period with therapist and robot mediator, respectively. The colors convention in the three plots of Fig. 8 describes the results generated by the automated estimation of VFOA. From these scenarios, some essential aspects already emerge. In the therapist-mediator interval, the child responded to JA task using only one repetition for all prompt level. The child’s behavior of RJA was according to the protocol, ie., the child looked towards the therapist to wait for instructions, rapidly the child searched in the target, and next looked again toward the therapist (Color sequence: light blue - yellow - light blue - orange - light blue - red). This behavior was the same for all prompts. In contrast, with the robot-mediator, the child did not look toward the robot among indications at consecutive targets (Color sequence: light green - yellow - orange - red - orange - yellow). The above happened because, in the protocol, both mediators executed the instructions in the same order, and","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"parent_id":"27d719c754aacd492a6dc8a1b7661935","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"parent_id":"27d719c754aacd492a6dc8a1b7661935","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":11,"parent_id":"4ff9e071d38c013c2f37b690bf641be5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"ee62de25ccc2b55d3a0495244b246fb9","text":"278","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Image","element_id":"fea08ffbf8ceed895df4b691d4122bc0","text":":—Yaw Angle —Moving Average Trend 80 60 40 20 -20 -40 - 50 100 150 200 250 Orientation (Degrees) Time (s) 50 i Y -50 -100 [—Yaw Angle —Moving Average Trend \\ 10 20 30 40 50 60 70 Orientation (Degrees) Time (s) |—Yaw Angle —Moving Average Trend 100 50 -50 1 10 20 30 40 50 60 70 80 90 100 110 Orientation (Degrees) Time (s)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"FigureCaption","element_id":"2d26d4fe7b1169af365f825b5b19559a","text":"Fig.9 Evolution over time of the child’s head/neck rotation (yaw rotation) for a ASD group","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"b225243534712b7b1827d2a461a08012","text":"the child memorized the commands and the object’s position until the robot mediator interval. This fact did not affect the intervention’s aim, as the robot mediator succeeded to elicit the child’s behaviors of RJA and IJA. In addition, as highlighted in the plot (A) in Fig. 8, when the session finalized and the robot mediator said goodbye, again, RJIA and 1JA behaviors were perceived. The pictures (a-d) show these events: first the child said goodbye towards the robot, then, he looked the therapist to confirm that the session ended and looked again towards the robot, finally the child took the robot’s hand.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"05f19e70db259d661931cf7d1d4193d4","text":"From the analysis of the three TD volunteers, the same reported behaviors were perceived. However, the analysis of the children in the ASD group showed different behavior patterns concerning comfort, visual contact and novelty stimulus effect during the sessions. The evolution over time of the child’s head/neck rotation (yaw rotation) for an ASD group is shown in Fig. 9. On the one hand, the three children in the ASD group maintained more visual contact with the robot compared to the therapist and exhibited more interest in the robot platform compared to the TD children. However, the performance of the children in the activities of JA did not improve significantly when the robot executed the prompt. On the other hand, the clinicians manifested that in all cases the first visual contact toward them occurred in the instant that the robot entered the scene and started","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"49523fafe90b1cb2a1e5c5b2fecaa655","text":"interacting, i.e., the ONO mediation elicited behaviors of 1JA towards the therapist. In addition, the CwASD exhibited less discomfort regarding the session, from the first moment when the robot initiated mediation in the room and, in some cases, when showed appearance of verbal and non-verbal pro-social behaviors. These facts did not arise with the TD children, because the first visual contact with the therapist occurred when they entered the room. Additionally, TD children showed the ability to divide the attention between the robot and the therapist from the beginning to the end of the intervention, exhibiting comfort in every moment. The behavior modulation of CwASD is observed in Fig. 9. Before the period with robot-mediator the children exhibited discomfort (unstable movements of their head), and after of this period, the head movement tended to be more stable.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"bdbf1f9c1a19503af6512e49193bc6b8","text":"The novelty of a robot-mediator at diagnostic session can be analyzed as an additional stimulus of the CRI Accordingly, in this case study the children of the ASD group showed more behavior modification (attention and comfort) produced by the robot interaction at the beginning of the CRI, remaining until the end of the session. On the other hand, the children of the TD group responded to the novelty effect of the robot mediator from the time the child entered the room and saw the robot, until the beginning of the therapist presentation. For the above, despite the novelty","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":12,"parent_id":"4ff9e071d38c013c2f37b690bf641be5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"837064b5d1f3b457a60f06025be9b3ea","text":"of the stimuli effect, these did not seem to affect the social interaction between the TD children and the therapist, and in contrast, these stimuli seemed to enhance the CwASD social interaction with the therapist along the intervention.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"99a607ae4a5f286fee2efe82b660178c","text":"These results are impressive, since they show the potential of CRI intervention to systematically elicit differences between the pattern of behavior on TD and ASD children. We identified RJA and IJA toward the therapist at the beginning of the intervention, at the transition between therapist to robot mediator, and at the end for all TD children. In contrast, we only identified IJA towards the therapist in the transition between mediators, for ASD children. This fact shows a clear difference of behavior pattern between CwASD and TD children, which can be analyzed using a JA task protocol. In fact, these pattern differences can be used as evidence to improve the ASD diagnosis.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"07d61f1daa46528a8d20bdb4c1584e06","text":"8 Conclusions","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"6b337b36359739c5c17f509f05965d5f","text":"This work presented a Robot-Assisted tool to assist and enhance the traditional practice of ASD diagnosis. The designed framework combines a vision system with the automated analysis of nonverbal cues in addition to a robotic platform; both developed upon open source projects. This research contributes to the state-of-the-art with an innovative flexible and scalable architecture capable to automatically register events of joint attention and patterns of visual contact before and after of a robot-based mediation as well as the pattern of behavior related to comfort or discomfort along the ASD intervention.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"07d61f1daa46528a8d20bdb4c1584e06","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"72a098cabe3e0d3cfd301f7c12581a4e","text":"In addition, an artificial vision pipeline based on a multi- camera approach was proposed. The vision system performs face detection, recognition and tracking, landmark detection and tracking, head pose, gaze and estimation of visual focus of attention was proposed, with its performance considered suitable for use into conventional ASD intervention. At least one camera captured the child’s face in each sample frame. Furthermore, the feedback information about the child’s performance was successfully used to modulate the supervised behavior of ONO, improving the performance of the CRI and the visual attention of the children. Regarding the VFOA estimation, the algorithm was able to estimate the target into the FoV in different situations recurrently. Also, the robot was able to react according to the estimation. However, the algorithm only failed when occlusion by the child’s hands is generated. On the other hand, the occlusion by the therapist and the robot was compensated using the multi-camera approach. The child’s face recognition system showed to be imperative to analyze the child’s behavior in the clinical setup implemented in this work, which required the caregiver’s attention in the room.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"07d61f1daa46528a8d20bdb4c1584e06","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"efd96aedf377e20afd95285a7c751a86","text":"279","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a061788cc25e3735e185ee8eb5485206","text":"Despite the limited number of children of this study, preliminary results of this case study showed the feasibility of identifying and quantify differences in the patterns of behavior of TD children and CwASD elicited by the CRI intervention. Through the proof of concept, it is evidenced here the system ability to improve the traditional tools used in ASD diagnosis. As future works, it is recommended a study to replicate the protocol proposed in this paper with ten CwASD and ten TD children. Another suggestion is to quantify other kinds of behaviors in addition to that assessed in this paper, such as verbal utterance patterns, physical and emotional engagement, object or event preferences and gather more evidence to improve the assistance to therapists in ASD diagnosis processes.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"efd96aedf377e20afd95285a7c751a86","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a17e691563132a28c4be94475934e9be","text":"Acknowledgements This work was supported by the Google Latin America Research Awards (LARA) program. The first author scholar- ship was supported in part by the Coordenagdo de Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) - Finance Code 001.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"efd96aedf377e20afd95285a7c751a86","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"0e4902971c31c3ab605c0f3b6dbf6ca5","text":"Disclosure statement No potential conflict of interest was reported by the authors.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"efd96aedf377e20afd95285a7c751a86","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"69824d3b0e70ca6aaa0da1613b65fd91","text":"References","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"efd96aedf377e20afd95285a7c751a86","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"0f686510ea78d05d02c0fa7ff9cc4b6a","text":"Belpaeme, T., Baxter, PE., de Greeff, J., Kennedy, J., Read, R., Looije, R., Neerincx, M., Baroni, L., Zelati, M.C.: Child-Robot interaction: perspectives and challenges. In: Sth International Conference, ICSR 2013, pp. 452-459. Springer International Publishing, Bristol (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"0037bc6ef9391c06422c87940c4f9dcd","text":"Diehl, J.J., Schmitt, L.M., Villano, M., Crowell, C.R.: The clinical use of robots for individuals with autism spectrum disorders: A critical review. Res. Autism Spectr. Disord. 6(1), 249-262 (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"25d1bd3fc8f56adb38304e1726736f6d","text":"Scassellati, B., Admoni, H., Maja, M.: Robots for use in autism research. Annu. Rev. Biomed. Eng. 14(1), 275-294 (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"dc25d72f7903fb377c2b98bde655de3e","text":"Pennisi, P., Tonacci, A., Tartarisco, G., Billeci, L., Ruta, L., Gangemi, S., Pioggia, G.: Autism and social robotics: A systematic review (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"e2079d39c21f3e25b5ee36819c2b965a","text":"American Psychiatric Association: DSM-5 diagnostic classifica- tion. In: Diagnostic and Statistical Manual of Mental Disorders. American Psychiatric Association, 5 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"81db06c1a547198109ea3d6f15203937","text":"Eggebrecht, A.T., Elison, J.T., Feczko, E., Todorov, A., Wolff, .J., Kandala, S., Adams, C.M., Snyder, A.Z., Lewis, J.D., Estes, A M., Zwaigenbaum, L., Botteron, K.N., McKinstry, R.C., Constantino, J.N., Evans, A., Hazlett, H.C., Dager, S., Paterson, S.J., Schultz, R.T,, Styner, M.A., Gerig, G., Das, S., Kostopoulos, P., Schlaggar, B.L., Petersen, S.E., Piven, J., Pruett, J.R.: Joint attention and brain functional connectivity in infants and toddlers. Cerebral Cortex 27(3), 1709-1720 (2017)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"05c1d8f8cd911f658816cb0aed01eb80","text":"Steiner, A.M., Goldsmith, T.R., Snow, A.V., Chawarska, K.: Disorders in infants and toddlers. J. Autism Dev. Disord. 42(6), 1183-1196 (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"bc29e9bdc2c17c61758bfec48bec8914","text":"Belpaeme, T., Baxter, PE., Read, R., Wood, R., Cuayahuitl, H., Kiefer, B., Racioppa, S., Kruijff-Korbayova, 1., Athanasopoulos, G., Enescu, V., Looije, R., Neerincx, M., Demiris, Y., Ros- Espinoza, R., Beck, A., Canamero, L., Hiolle, A., Lewis, M., Baroni, I., Nalin, M., Cosi, P, Paci, G., Tesser, F., Sommavilla, G., Humbert, R.: Multimodal child-robot interaction: building social bonds. Journal of Human-Robot Interaction 1(2), 33-53 (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":13,"parent_id":"69824d3b0e70ca6aaa0da1613b65fd91","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"7f0a22117f8fe0172cf9209ff622b64a","text":"280","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"04063d89c1a97f3c857822f7b60666c8","text":"9. Vanderborght, B., Simut, R., Saldien, J., Pop, C., Rusu, A.S., Pintea, S., Lefeber, D., David, D.O.: Using the social robot probo as a social story telling agent for children with ASD. Interact. Stud. 13(3), 348-372 (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"c07477e5caecaed1c0bfdaa4550715cd","text":"10. Warren, Z.E., Zheng, Z., Swanson, A.R., Bekele, E., Zhang, L., Crittendon, J.A., Weitlauf, A.F, Sarkar, N.: Can robotic interaction improve joint attention skills? J. Autism Dev. Disord. 45(11), 3726-3734 (2015)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"09f8aa6d149ab570755b9422be9da3ae","text":"11 Wood, L.J., Dautenhahn, K., Lehmann, H., Robins, B., Rainer, A., Syrdal, D.S.: Robot-mediated interviews: Do robots pos- sess advantages over human interviewers when talking to chil- dren with special needs? Lecture Notes in Computer Sci- ence (including subseries Lecture Notes in Artificial Intelli- gence and Lecture Notes in Bioinformatics) 8239 LNAI, 54-63 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"e95cb27fa268cfc9cf9a44cec37b9ca5","text":"12. Feil-Seifer, D., Mataric, M.J.: b3IA A control architecture for autonomous robot-assisted behavior intervention for children with Autism Spectrum Disorders. In: ROMAN 2008 The 17th IEEE International Symposium on Robot and Human Interactive Communication, pp. 328-333 (2008)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"78b3ff74ec92aae1de41b8644d29072c","text":"13 Leo, M., Del Coco, M., Carcagni, P., Distante, C., Bernava, M., Pioggia, G., Palestra, G.: Automatic emotion recognition in Robot-Children interaction for ASD treatment. In: Proceedings of the IEEE International Conference on Computer Vision, 2015- Febru(c), pp. 537-545 (2015)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"9afd4b1b454f7d609591fc7879fe6558","text":"14. Esteban, P.G., Baxter, PE., Belpaeme, T., Billing, E., Cai, H., Cao, H.-L., Coeckelbergh, M., Costescu, C., David, D., De Beir, A., Fang, Y., Ju, Z., Kennedy, J., Liu, H., Mazel, A., Pandey, A., Richardson, K., Senft, E., Thill, S., Van De Perre, G., Vanderborght, B., Vernon, D., Hui, Y., Ziemke, T.: How to build a supervised autonomous system for Robot-Enhanced therapy for children with autism spectrum disorder. Paladyn Journal of Behavioral Robotics 8(1), 18-38 (2017)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"dee7ae012e465ee6bc446f846f16d14f","text":"15 Pour, A.G., Taheri, A., Alemi, M., Ali, M.: Human-Robot facial expression reciprocal interaction platform: case studies on children with autism. Int. J. Soc. Robot. 10(2), 179-198 (2018)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"e25bb11f10101260cf9e8e95f891afe2","text":"16. Feng, Y., Jia, Q., Wei, W.: A control architecture of Robot- Assisted intervention for children with autism spectrum disorders. J. Robot. 2018, 12 (2018)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"2b8b37e2a50a65bd1e68ee86d07f948d","text":"17 Bekele, E., Crittendon, J.A., Swanson, A., Sarkar, N., Warren, Z.E.: Pilot clinical application of an adaptive robotic system for young children with autism. Autism: The International Journal of Research and Practice 18(5), 598-608 (2014)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"480c11668a61f9e683b403faee3d1ae8","text":"18 Huijnen, C.A.G.J., Lexis, M.A.S., Jansens, R., de Witte, L.P.: Mapping robots to therapy and educational objectives for children with autism spectrum disorder. J. Autism Dev. Disord. 46(6), 2100-2114 (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"2f229d1cc801d3d3c2b56dca066739d7","text":"19 Aresti-Bartolome, N., Begonya, G.-Z.: Technologies as support tools for persons with autistic spectrum disorder: s systematic review. Int. J. Environ. Res. Public Health 11(8), 7767-7802 (2014)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"7b3a1b1e9954965eaf31df7e57a96a16","text":"20. Boucenna, S., Narzisi, A., Tilmont, E., Muratori, F,, Pioggia, G., Cohen, D., Mohamed, C.: Interactive technologies for autistic children: a review. Cogn. Comput. 6(4), 722-740 (2014)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"793513e3981b8aa148437fc380e38d5c","text":"21 Grynszpan, O., Patrice, L., Weiss, T., Perez-Diaz, F., Gal, E.: Innovative technology-based interventions for autism spectrum disorders: a meta-analysis. Autism 18(4), 346-361 (2014)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"151efdca97d8842a0f0785ce4131a826","text":"22 Rehg, J.M., Rozga, A., Abowd, G.D., Goodwin, M.S.: Behavioral imaging and autism. IEEE Pervasive Comput. 13(2), 84-87, 4 (2014)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"58ee681a24092bac8d71c4f1a7535102","text":"23 Cabibihan, J.J., Javed, H., Ang, M., Aljunied, S.M.: Why robots? a survey on the roles and benefits of social robots in the therapy of children with autism. Int. J. Soc. Robot. 5(4), 593-618 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"7f0a22117f8fe0172cf9209ff622b64a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"5ddaaa1d09ee7ae584c46de58e9c6641","text":"24, Sartorato, F., Przybylowski, L., Sarko, D.K.: Improving therapeu- tic outcomes in autism spectrum disorders: enhancing social com- munication and sensory processing through the use of interactive robots. J. Psychiatr. Res. 90, 1-11 (2017)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"da67cb418347f22674fe5e8a72f60a91","text":"25 Chong, E., Chanda, K., Ye, Z., Southerland, A., Ruiz, N., Jones, R.M., Rozga, A., Rehg, J.M.: Detecting gaze towards eyes in natural social interactions and its use in child assessment. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1(3), 43:1— 43:20 (2017)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"aa1587ec51aa47571444bfaf3ae56923","text":"26, Ness, S.L., Manyakov, N.V., Bangerter, A., Lewin, D., Jagannatha, S., Boice, M., Skalkin, A., Dawson, G., Janvier, Y.M., Goodwin, M.S., Hendren, R., Leventhal, B., Shic, F., Cioccia, W., Gahan, P: JAKE® Multimodal data capture system: Insights from an observational study of autism spectrum disorder. Frontiers in Neuroscience 11(SEP) (2017)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"031e6ae63eea31dee3498ed1da51e801","text":"27 Rehg, M., Abowd, G.D., Rozga, A., Romero, M., Clements, M.A., Sclaroff, S., Essa, L, Ousley, O.Y., Li, Y., Kim, C., Rao, H., Kim, J.C., Lo Presti, L., Zhang, J., Lantsman, D., Bidwell, J., Ye, Z.: Decoding children’s social behavior. In: 2013 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3414-3421 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"64a46808bddcf308dbdbeb52da9d08cb","text":"28, Adamo, F., Palestra, G., Crifaci, G., Pennisi, P., Pioggia, G., Ruta, L., Leo, M., Distante, C., Cazzato, D.: Non-intrusive and calibration free visual exploration analysis in children with autism spectrum disorder. In: Computational Vision and Medical Image Processing V - Proceedings of Sth Eccomas Thematic Conference on Computational Vision and Medical Image Processing, VipIMAGE 2015, pp .201-208 (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"3628b8fb50415cf5828578c76bb88c18","text":"29, Michaud, F., Salter, T., Duquette, A., Mercier, H., Lauria, M., Larouche, H., Larose, F.: Assistive technologies and Child-Robot interaction. American Association for Artificial Intelligence ii(3), 8-9 (2007)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"fda8a543a0a7638decdf68ce6eb578cd","text":"30, Duquette, A., Michaud, F., Mercier, H.: Exploring the use of a mobile robot as an imitation agent with children with low- functioning autism. Auton. Robot. 24(2), 147-157 (2008)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"b1e6548ac36e50ed1112e7d4eb25f853","text":"31 Simut, R.E., Vanderfaeillie, J., Peca, A., Van de Perre, G., Bram, V.: Children with autism spectrum disorders make a fruit salad with probo, the social robot: an interaction study. J. Autism Dev. Disord. 46(1), 113-126 (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"87523bbcaf6c9c170ba9efe845393976","text":"32, Bekele, E., Lahiri, U., Swanson, A.R., Crittendon, J.A., Warren, Z.E., Nilanjan, S.: A step towards developing adaptive robot- mediated intervention architecture (ARIA) for children with autism. IEEE Trans. Neural Syst. Rehabil. Eng. 21(2), 289-299 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"3babcb8d561d54248ca4cbbd33d6cd26","text":"33 Zheng, Z., Zhang, L., Bekele, E., Swanson, A., Crittendon, J.A., Warren, Z.E., Sarkar, N.: Impact of robot-mediated interaction system on joint attention skills for children with autism. In: IEEE International Conference on Rehabilitation Robotics (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"8f6515cf1c55b453386872e76ec31efa","text":"34 Anzalone, S.M., Tilmont, E., Boucenna, S., Xavier, J., Jouen, A.L., Bodeau, N., Maharatna, K., Chetouani, M., Cohen, D.: How children with autism spectrum disorder behave and explore the 4-dimensional (spatial 3D + time) environment during a joint attention induction task with a robot. Res. Autism Spectr. Disord. 8(7), 814-826 (2014)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"8f62e317eb1a50dc6a36dc3dccd281f4","text":"35 Chevalier, P, Martin, J.C., Isableu, B., Bazile, C., Iacob, D.O., Adriana, T.: Joint attention using human-robot interaction: impact of sensory preferences of children with autism. In: 25th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2016, pp. 849-854 (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"22b8faebbe16be2679da85c5d11087cf","text":"36, Lemaignan, S., Garcia, F,, Jacq, A., Dillenbourg, P.: From real- time attention assessment to “with-me-ness” in human-robot interaction. In: ACM/IEEE International Conference on Human- Robot Interaction, 2016-April, pp. 157-164 (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"a271f385412c13a2d1fa95a31b0ec2d5","text":"37 Del Coco, M., Leo, M., Carcagni, P., Fama, F., Spadaro, L., Ruta, L., Pioggia, G., Distante, C.: Study of mechanisms of","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":14,"parent_id":"4ff9e071d38c013c2f37b690bf641be5","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"94eb67e7bcde85978dacb4096c81dd5c","text":"JIntell Robot Syst (2019) 96:267-281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"f1fa8d19b3e13a2618a89511446e09df","text":"social interaction stimulation in autism spectrum disorder by assisted humanoid robot. IEEE Transactions on Cognitive and Developmental Systems 8920(c), 1-1 (2017)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"4abbf880bbf50fdb683a44eb1dd50fdb","text":"38 Palestra, G., Varni, G., Chetouani, M., Esposito, F.: A multimodal and multilevel system for robotics treatment of autism in children. In: Proceedings of the International Workshop on Social Learning and Multimodal Interaction for Designing Artificial Agents - DAA ’16, pp. 1-6. ACM Press, New York (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"02109a55052be72ac56dde9f8f648375","text":"39 Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T., Leibs, J., Berger, E., Wheeler, R., Ng, A.: ROS : an open-source robot operating system. In: ICRA workshop on open source software, number 3.2, pp. 5 (2009)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"291015a98d2430888515a415c29c9821","text":"40. Vandevelde, C., Saldien, J., Ciocci, C., Vanderborght, B.: The use of social robot ono in robot assisted therapy. In: International Conference on Social Robotics, Proceedings, m (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"f57b8e7e759fdf11941b6c71f575a52e","text":"41 Dautenhahn, K.: A paradigm shift in artificial intelligence: why social intelligence matters in the design and development of robots with human-like intelligence. 50 Years of Artificial Intelligence, pp. 288-302 (2007)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"d091d618f3025f4f230dfb6e55450652","text":"42 Ekman, P., Friesen, W.: Facial Action Coding System. Consulting Psychologists Press (1978)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"05a52cb54df7439c65b16f99c8cd41d9","text":"43 King, D.E.: Dlib-ml: a machine learning toolkit. J. Mach. Learn. Res. 10, 1755-1758 (2009)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"e18ec98b17854db558b7c658d1824e3e","text":"Baltrusaitis, T., Robinson, P., Morency, L.-P.: OpenFace: an open source facial behavior analysis toolkit. IEEE Winter Conference on Applications of Computer Vision (2016)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"694e8134299c06822b18451e10f7daab","text":"45 King, D.E.: Max-Margin Object Detection. 1 (2015)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"952da517848151f6ddbc57495dba1399","text":"46 He, K., Zhang, X., Ren, S., Jian, S.: Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778. IEEE, 6","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"1ad3984dda1c3970102b4e914892f023","text":"47 Baltrusaitis, T., Robinson, P, Morency, L.P.: Constrained local neural fields for robust facial landmark detection in the wild. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 354-361 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"43876c8fae2cdf443c3cb77de1a5cff3","text":"48 Cristinacce, D., Cootes, T.F.: Feature detection and tracking with constrained local models. In: Proceedings of the British Machine Vision Conference 2006, pp. 1-95 (2006)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"45d020fe24c64701b786e3e907e7ad14","text":"49 Saragih, J.M., Lucey, S., Cohn, J.F.: Deformable model fitting by regularized landmark mean-shift. Int. J. Comput. Vis. 91(2), 200-215 (2011)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"5f58280dd87d1580236667c101cc200e","text":"50. Baltrusaitis, T., Robinson, P., Morency, L.P.: 3D constrained local model for rigid and non-rigid facial tracking. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2610-2617 (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"ca82043f6350f175a137ecf81bb9f641","text":"51 Belhumeur, P.N., Jacobs, D.W., Kriegman, D.J., Neeraj, K. Localizing parts of faces using a consensus of exemplars. IEEE Trans. Pattern Anal. Mach. Intell. 35(12), 2930-2940 (2013)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"980a5f667be69fe7ad4f4fa5e50e7a9a","text":"52 Le, V., Brandt, J., Lin, Z., Bourdev, L., Huang, T.S.: Interactive Facial Feature Localization, pp. 679-692. Springer, Berlin (2012)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"1da89c18b3eb90e97953820e3a1bf47a","text":"53 Jorstad, A., Dementhon, D., Jeng Wang, 1., Burlina, P.: Distributed consensus on camera pose. IEEE Trans. Image Process. 19(9), 2396-2407 (2010)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"9756fb6af200d204d7febc1faf5c5b7b","text":"54 Ba, S.0., Odobez, J.-M.: Multi-Person visual focus of attention from head pose and meeting contextual cues. IEEE Trans. Pattern Anal. Mach. Intell. 33(August), 1-16 (2008)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"94eb67e7bcde85978dacb4096c81dd5c","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Header","element_id":"71a1c003a2b855d85582c8f6c7648c49","text":"281","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"0394638595bb688c6c7da38dd60a7d50","text":"55. Sheikhi, S., Jean-Marc, O.: Combining dynamic head pose- gaze mapping with the robot conversational state for attention recognition in human-robot interactions. Pattern Recogn. Lett. 66, 81-90 (2015)","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"028bfc1a12d6dc98d52d0198a738de4c","text":"Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"75d2f61d33c369e419a48ea44b91fa6c","text":"Andrés A. Ramirez-Duque received his bachelor’s degree in Mecha- tronics Engineering from the Universidad Nacional de Colombia, Bogotd, Colombia, in 2009, and his Industrial Automation Master degree from the Universidad Nacional de Colombia, Bogota, Colom- bia, in 2011. He is currently working toward a Ph.D. degree in the Assistive Technology Center, Federal University of Espirito Santo, Vitéria, Brazil. He won a Google Latin America Research Award 2017. His current research interests include Child-Robot interaction, cloud parallel computing, high performance computing, smart environments and serious games applied to Children with development impairments.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"b96458dc8a499409a661bd37c0b2457f","text":"Anselmo Frizera-Neto received his bachelor’s degree in Electrical Engineering (2006) from the Federal University of Espirito Santo (UFES) in Brazil and his doctorate in Electronics (2010) at the University of Alcald, Spain. From 2006 to 2010 he was a researcher of the Bioengineering Group of the Consejo Superior de Investigaciones Cientificas (Spain) where he carried out research related to his doctoral thesis. He is currently a permanent professor and adjunct coordinator of the Graduate Program in Electrical Engineering at UFES. He has authored or co-authored more than 250 papers in scientific journals, books and conferences in the fields of electrical and biomedical engineering. He has conducted or co-directed master’s and doctoral theses in research institutions from Brazil, Argentina, Italy and Portugal. His research is aimed at rehabilitation robotics, the development of advanced strategies of human-robot interaction and the conception of sensors and measurement technologies with applications in different fields of electrical and biomedical engineering. Along with Andrés Ramirez-Duque, he won a Google Latin America Research Award 2017.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"caa0afa740b7c84651f229f3f57347de","text":"Teodiano Freire Bastos received his B.Sc. degree in Electrical Engineering from Universidade Federal do Espirito Santo (Vitoria, Brazil) in 1987, his Specialist degree in Automation degree from Instituto de Automadtica Industrial (Madrid, Spain) in 1989, and his Ph.D. degree in Physical Science (Electricity and Electronics) from Universidad Complutense de Madrid (Spain) in 1994. He made two postdocs, one at the University of Alcald (Spain, 2005) and another at RMIT University (Australia, 2012). He is currently a full professor at Universidade Federal do Espirito Santo (Vitoria, Brazil), teaching and doing research at the Postgraduate Program of Electrical Enginneering, Postgraduate Program of Biotechnology and RENORBIO Ph.D. Program. His current research interests are signal processing, rehabilitation robotics and assistive technology for people with disabilities","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"4ff9e071d38c013c2f37b690bf641be5","text":"@ Springer","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"497f36f97d02b069ff14e35c1886f2aa","text":"Content courtesy of Springer Nature, terms of use apply. Rights reserved.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":15,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"be6e04ca1e6b9ec19a8e6183603c8215","text":"Terms and Conditions","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"249f73d4626620e105c19677c02b1a36","text":"Springer Nature journal content, brought to you courtesy of Springer Nature Customer Service Center GmbH (“Springer Nature”). Springer Nature supports a reasonable amount of sharing of research papers by authors, subscribers and authorised users (“Users”), for small- scale personal, non-commercial use provided that all copyright, trade and service marks and other proprietary notices are maintained. By accessing, sharing, receiving or otherwise using the Springer Nature journal content you agree to these terms of use (“Terms”). For these purposes, Springer Nature considers academic use (by researchers and students) to be non-commercial. These Terms are supplementary and will apply in addition to any applicable website terms and conditions, a relevant site licence or a personal subscription. These Terms will prevail over any conflict or ambiguity with regards to the relevant terms, a site licence or a personal subscription (to the extent of the conflict or ambiguity only). For Creative Commons-licensed articles, the terms of the Creative Commons license used will apply. We collect and use personal data to provide access to the Springer Nature journal content. We may also use these personal data internally within ResearchGate and Springer Nature and as agreed share it, in an anonymised way, for purposes of tracking, analysis and reporting. We will not otherwise disclose your personal data outside the ResearchGate or the Springer Nature group of companies unless we have your permission as detailed in the Privacy Policy. While Users may use the Springer Nature journal content for small scale, personal non-commercial use, it is important to note that Users may not:","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"be6e04ca1e6b9ec19a8e6183603c8215","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"eceac3a1de8fe0ab4433d4efd58dcfa3","text":"1. use such content for the purpose of providing other users with access on a regular or large scale basis or as a means to circumvent access control;","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"be6e04ca1e6b9ec19a8e6183603c8215","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"9499fd77dbbfd7732b66456ae08676d8","text":"2. use such content where to do so would be considered a criminal or statutory offence in any jurisdiction, or gives rise to civil liability, or is","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"be6e04ca1e6b9ec19a8e6183603c8215","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"05a52cde301f8e726869b3bab1770a4a","text":"otherwise unlawful;","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"19775cb7bc42fdc8441de19b73f11385","text":"3. falsely or misleadingly imply or suggest endorsement, approval , sponsorship, or association unless explicitly agreed to by Springer Nature in","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"05a52cde301f8e726869b3bab1770a4a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"a82cb8eafade3c536bc193a34aef7a96","text":"writing; use bots or other automated methods to access the content or redirect messages","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"05a52cde301f8e726869b3bab1770a4a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"587ba37907dbfe46ecf0e422c0d0f05e","text":"share the content in order to create substitute for Springer Nature products or services or a systematic database of Springer Nature journal","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"05a52cde301f8e726869b3bab1770a4a","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"9d206df5f62b15ba62faabff91b581a9","text":"content.","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"NarrativeText","element_id":"9a69715e6d23d895cae4813fdd35861f","text":"In line with the restriction against commercial use, Springer Nature does not permit the creation of a product or service that creates revenue, royalties, rent or income from our content or its inclusion as part of a paid for service or for other commercial gain. Springer Nature journal content cannot be used for inter-library loans and librarians may not upload Springer Nature journal content on a large scale into their, or any other, institutional repository. These terms of use are reviewed regularly and may be amended at any time. Springer Nature is not obligated to publish any information or content on this website and may remove it or features or functionality at our sole discretion, at any time with or without notice. Springer Nature may revoke this licence to you at any time and remove access to any copies of the Springer Nature journal content which have been saved. To the fullest extent permitted by law, Springer Nature makes no warranties, representations or guarantees to Users, either express or implied with respect to the Springer nature journal content and all parties disclaim and waive any implied warranties or warranties imposed by law, including merchantability or fitness for any particular purpose. Please note that these rights do not automatically extend to content, data or other material published by Springer Nature that may be licensed from third parties. If you would like to use or distribute our Springer Nature journal content to a wider audience or on a regular basis or in any other manner not expressly permitted by these Terms, please contact Springer Nature at","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"9d206df5f62b15ba62faabff91b581a9","filename":"1_Ramrez-Duque_.pdf"}},{"type":"Title","element_id":"947e1d8671dad9b687e01c755317e6ae","text":"onlineservice@springernature.com","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"71a1c003a2b855d85582c8f6c7648c49","filename":"1_Ramrez-Duque_.pdf"}},{"type":"ListItem","element_id":"0d62c266a7d8e5638f3bdfe550b7e1de","text":"override any security feature or exclusionary protocol; or","metadata":{"filetype":"application/pdf","languages":["eng"],"page_number":16,"parent_id":"947e1d8671dad9b687e01c755317e6ae","filename":"1_Ramrez-Duque_.pdf"}}]},"metadata":{"application/json":{"expanded":false,"root":"root"}}}]},{"cell_type":"code","source":"for element in response.elements:\n    if element['type'] == \"Title\" and element['text'] == \"References\":\n        print('element id: ',element['element_id'])\n        print('parent id: ',element['metadata']['parent_id'])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:43:36.296044Z","iopub.execute_input":"2024-05-01T06:43:36.297076Z","iopub.status.idle":"2024-05-01T06:43:36.305068Z","shell.execute_reply.started":"2024-05-01T06:43:36.297031Z","shell.execute_reply":"2024-05-01T06:43:36.304026Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"element id:  69824d3b0e70ca6aaa0da1613b65fd91\nparent id:  efd96aedf377e20afd95285a7c751a86\n","output_type":"stream"}]},{"cell_type":"code","source":"#Extracting text out of this pdf without the references, headers and footers\ndef cleaned_text_without_references(response):\n    cleaned_elements=[]\n    for element in response.elements:\n        if element['text'] == 'REFERENCES' or element['text']=='References':\n            break\n        elif element['type'] != \"Header\" and element['type'] != \"Footer\":\n            cleaned_elements.append({'element_id': element['element_id'],\n                                    'page_number': element['metadata']['page_number'],\n                                    'text': element['text']})\n    return cleaned_elements\ncleaned_text = cleaned_text_without_references(response)\ncleaned_text[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:43:40.336061Z","iopub.execute_input":"2024-05-01T06:43:40.336544Z","iopub.status.idle":"2024-05-01T06:43:40.350479Z","shell.execute_reply.started":"2024-05-01T06:43:40.336511Z","shell.execute_reply":"2024-05-01T06:43:40.348900Z"},"scrolled":true,"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[{'element_id': '0606ba94dc82b8248e73d33d5dce1c37',\n  'page_number': 1,\n  'text': 'Robot-Assisted Autism Spectrum Disorder Diagnostic Based on Artificial Reasoning'},\n {'element_id': '165abf064c3a95bbee57e8aa4016140c',\n  'page_number': 1,\n  'text': \"Andrés A. Ramirez-Duque’ @ . Anselmo Frizera-Neto' - Teodiano Freire Bastos’\"},\n {'element_id': 'b2181c6eefd507a2a68fcd65e3af62e0',\n  'page_number': 1,\n  'text': 'l.) Check for updates'},\n {'element_id': 'dbc91cf0825c7c18192c20fc0182d322',\n  'page_number': 1,\n  'text': 'Received: 25 April 2018 / Accepted: 20 December 2018 / Published online: 29 March 2019 © Springer Nature B.V. 2019'},\n {'element_id': 'd21b4a64a2d8656a0fdf7ab2e89a4916',\n  'page_number': 1,\n  'text': 'Abstract'}]"},"metadata":{}}]},{"cell_type":"code","source":"#grouping all the text on same page\ndef group_by_page(cleaned_text):\n    grouped_texts=[]\n    #finding the max page number\n    max_page_number = 0\n    for item in cleaned_text:\n        page_number = item['page_number']\n        if page_number > max_page_number:\n            max_page_number = page_number\n    #grouping all the text on same page\n    for i in range(max_page_number):\n        item_text = ''\n        item_page_number = i+1\n        for item in cleaned_text:\n            if item['page_number'] == i+1:\n                item_text = item_text + item['text']\n        grouped_texts.append({'page_number': item_page_number, \n                              'text': item_text})\n    return grouped_texts\n\ngrouped_text = group_by_page(cleaned_text)\ngrouped_text[:2]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:56:11.136954Z","iopub.execute_input":"2024-05-01T06:56:11.137566Z","iopub.status.idle":"2024-05-01T06:56:11.152085Z","shell.execute_reply.started":"2024-05-01T06:56:11.137527Z","shell.execute_reply":"2024-05-01T06:56:11.150723Z"},"scrolled":true,"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'page_number': 1,\n  'text': \"Robot-Assisted Autism Spectrum Disorder Diagnostic Based on Artificial ReasoningAndrés A. Ramirez-Duque’ @ . Anselmo Frizera-Neto' - Teodiano Freire Bastos’l.) Check for updatesReceived: 25 April 2018 / Accepted: 20 December 2018 / Published online: 29 March 2019 © Springer Nature B.V. 2019AbstractAutism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are found in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral observation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral observation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior assessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum Disorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when compared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding to summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance the traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable network of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face analysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three children in risk of suffering from ASD.Keywords Child-Robot interaction - Autism spectrum disorder - Convolutional neural network - Robot reasoning model Statistical shape modeling1 IntroductionResearch in Child-Robot Interaction (CRI) aims to provide the necessary conditions for the interaction between a child and a robotic device taking into account some fundamental features, such as child’s neurophysical and physical condition, and the child’s mental health [I]. That is how Robot-Assisted Therapies (RAT) using CRI theories have been of interest as an intervention for CwASD, elucidating faster and more significant gains from the therapeutic intervention when compared to traditional therapies [2—4].ASD is a neurodevelopmental disorder that affects people from birth, and its symptoms are found in the early1 Universidade Federal do Espirito Santo., Av. Fernando Ferrari, 514 (29075-910), Vitoria, Brazildevelopmental period. Individuals suffering from ASD exhibit persistent deficits in social communication, social interaction and repetitive patterns of behavior, interests, or activities [5]. Some of the ASD signs may be observed before the age of 10 months, although a reliable diagnosis can only be performed at 18 months of age, according to [6], or 24 months according to [7].The use of computer vision to analyze the child’s behaviors, and automated video coding to summarize the interventions, can help the clinicians to reduce the delay of ASD diagnosis, providing the CwASD with access to early therapeutic interventions. In addition, CRI-based intervention can transform traditional diagnosis methods through a robotic device to systematically elicit child’s behaviors that exhibit ASD signs [8].Some of the first systems developed to assist ASD therapists and make diagnosis based on robotic devices have primarily been open loop and remotely operated sys- tems. However, these approaches are unable to perform autonomous feedback to enhance the interaction [9-11].@ SpringerContent courtesy of Springer Nature, terms of use apply. Rights reserved.\"},\n {'page_number': 2,\n  'text': 'Nevertheless, different systems are able to modify the behavior of the robot according to environmental interac- tions and the child’s response, using a closed-loop and artificial cognition approaches [12—-16]. These systems have been hypothesized to offer technological mechanisms for supporting more flexible and potentially more naturalistic interaction [17]. In fact, literature reports that automatic robot’s social behaviors modulation according to specifics scenarios has a strong effect on child’s social behavior [12]. However, despite the increase of positive evidence, this technology has rarely been applied to specific ASD diagnosis.This work aims to present a robot-assisted framework using an artificial reasoning module to assist clinicians with the ASD diagnostic process. The framework is composed of a responsive robotic platform, a flexible and scalable vision sensor network, and an automated face analysis algorithm based on machine learning models. In this research we take advantage of some neural models available as open sources projects to build a completely new pipeline algorithm for global recognition and tracking of child’s face among many faces present in a typical unstructured clinical intervention, in order to estimate the child’s visual focus of attention along the time. The proposed system can be used in different behavioral analysis scenarios typical of an ASD diagnostic process. In order to illustrate the feasibility of the proposed system, in this paper an experimental trial to assess joint- attention behavior is presented employing an in-clinic setup (unstructured environment).The main contributions of this paper are: (i) the development of a new artificial reasoning module upon a flexible and scalable ROS-based vision system using state-of-the-art machine learning neural models; (ii) the proposal and implementation of a supervised CRI (child- robot interaction) based on an open source social robotic platform to enhance the traditional tools for ASD diagnosis using an in-clinic setup protocol. For the best of our knowledge, there are no open source projects available for face analysis based on a multi-camera approach using ROS with the characteristics described in our research.2 Related WorkRecent researches have shown the acceptance and efficiency of technologies used as auxiliary tools for therapy and teaching of individuals with ASD [18-21]. Such technologies may also be useful for people surrounding ASD individuals (therapists, caregivers, family members). For example, the use of artificial vision systems to measure and analyze the child’s behavior can lead to alternative screening and monitoring tools that help the clinicians to get feedback from the effectiveness of the intervention [22].Additionally, social robots have great potential for aid in the diagnosis and therapy of children with ASD [18, 23]. A higher degree of control, prediction and simplicity may be achieved in interactions with robots, impacting directly on frustration and reducing the anxiety of these individuals [24].Respect to the use of computer vision techniques, previous studies already analyzed child’s behaviors, such as visual attention, eye gaze, eye contact, smile events, and visual exploration using cameras and eye trackers [25, 26] and RGBd cameras [27, 28]. These studies have shown the potential of vision systems in improving the behavioral coding in ASD therapies. However, these studies did not implement techniques of CRI to enhance the intervention.On the other hand, studies about how CwASD respond to a robot mediator compared to human mediator have been reported, such as intervention scenarios with imitation games [29, 30], telling stories [9] and free play tasks [12, 31]. These works used features, such as proxemics, body gestures, visual contact and eye gaze as behavioral descriptors, whereas the behavior analysis was estimated using manual video coding.Researchers of Vanderbilt University published a series of research showing an experimental protocol to assess joint attention (JA) tasks defined as the capacity for coordinated orientation of two people toward an object or event [6]. The protocol consisted of directing the attention of the child towards objects located in the room through adaptive prompts [32]. Bekele et al. inferred the participant’s eye gaze by the head pose, which was calculated in real-time by an IR camera array [17]. In their last works, Zheng et al. and Warren et al. used a commercial eye tracker to estimated the children’s eye gaze around the robot and manual behavioral coding for global evaluation [10, 33]. However, eye tracker devices require pre-calibration and may limit the movement of the individual. The results of these works showed that the robot attracted children’s attention and that CwASD reached all JA task. Nevertheless, developing JA tasks is more difficult with a robot than with humans [10]. Anzalone et al. developed a CRI scenario using the NAO robot to perform JA tasks, in which the authors used an RGBD camera to estimate only body and head movements. The results showed that JA performance of children with ASD was similar to the performance of TD children when interacting with the human mediator, however, with a robot mediator, the children with ASD presented a lower performance than the TD children, i.e, the children with ASD needed more social cues to finalize the task [34]. Chevalier et al. analyzed in their study, some features, such as proprioceptive and visual integration in CwASD, using an RGBD sensor to record the interventions sessions and manual behavior coding to analyzed the participants’ performance [35]. In none of the previous works, a closed-loop subsystem was@ SpringerContent courtesy of Springer Nature, terms of use apply. Rights reserved.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"**Extracting text out of each pdf without the references, headers and footers**","metadata":{}},{"cell_type":"code","source":"pdf_and_text=[]\nfor filename in os.listdir(\"/kaggle/input/publications\"):\n  #check if the file is a pdf\n  if filename.endswith('.pdf'):\n    # Construct the full path to the PDF file\n    pdf_path = os.path.join(\"/kaggle/input/publications\", filename)\n    pdf_response = get_elements_from_pdf(pdf_path)\n    pdf_cleaned_text = cleaned_text_without_references(pdf_response)\n    pdf_grouped_text = group_by_page(pdf_cleaned_text)\n    pdf_and_text.append({'filename': filename.replace(\"/kaggle/input/publications/\", \"\"), \n                         'text': pdf_grouped_text})\npdf_and_text[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:57:31.109289Z","iopub.execute_input":"2024-05-01T06:57:31.109815Z","iopub.status.idle":"2024-05-01T07:20:03.983687Z","shell.execute_reply.started":"2024-05-01T06:57:31.109780Z","shell.execute_reply":"2024-05-01T07:20:03.982629Z"},"scrolled":true,"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'filename': 'LEE.pdf',\n 'text': [{'page_number': 1,\n   'text': '. sensorsbyLetterLetter Deep-Learning-Based Detection of Infants with Autism Spectrum Disorder Using Auto-Encoder Feature RepresentationJung Hyuk Lee 1, Geon Woo Lee 1, Guiyoung Bong 2, Hee Jeong Yoo 2,3 and Hong Kook Kim 1,*1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, Korea; ljh0412@gist.ac.kr (J.H.L.); geonwoo0801@gist.ac.kr (G.W.L.) 2 Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam-si,Gyeonggi-do 13620, Korea; 20409@snubh.org (G.B.); hjyoo@snu.ac.kr (H.J.Y.)3 Department of Psychiatry, College of Medicine, Seoul National University, Seoul 03980, Korea * Correspondence: hongkook@gist.ac.krReceived: 29 October 2020; Accepted: 24 November 2020; Published: 26 November 2020check for v updatesAbstract: Autism spectrum disorder (ASD) is a developmental disorder with a life-span disability. While diagnostic instruments have been developed and qualiﬁed based on the accuracy of the discrimination of children with ASD from typical development (TD) children, the stability of such procedures can be disrupted by limitations pertaining to time expenses and the subjectivity of clinicians. Consequently, automated diagnostic methods have been developed for acquiring objective measures of autism, and in various ﬁelds of research, vocal characteristics have not only been reported as distinctive characteristics by clinicians, but have also shown promising performance in several studies utilizing deep learning models based on the automated discrimination of children with ASD from children with TD. However, diﬃculties still exist in terms of the characteristics of the data, the complexity of the analysis, and the lack of arranged data caused by the low accessibility for diagnosis and the need to secure anonymity. In order to address these issues, we introduce a pre-trained feature extraction auto-encoder model and a joint optimization scheme, which can achieve robustness for widely distributed and unreﬁned data using a deep-learning-based method for the detection of autism that utilizes various models. By adopting this auto-encoder-based feature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic parameter set (eGeMAPS) speech feature data set, we acquire improved performance in the detection of ASD in infants compared to the raw data set.Keywords: auto-encoder; bidirectional long short-term memory (BLSTM); joint optimization; acoustic feature extraction; autism spectrum disorder1. IntroductionAutism spectrum disorder (ASD) is a developmental disorder with a high probability of causing diﬃculties in social interactions with other people [1]. According to the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), ASD involves several characteristics such as being conﬁned to speciﬁc interests or behaviors, delayed linguistic development, and poor functionality in terms of communicating or functioning in social situations [2]. As there is wide variation in terms of the types and severities of ASD based on its characteristics, the disorder is referred to as a spectrum [1]. Not only does ASD have the characteristics of a developmental disorder with a life-span disability, but its prevalence is also increasing—from 1 in 150 children in 2000 to 1 in 54 children in 2016 [3]. As diverse evidence has been obtained from previous research showing that the chance of improvement in theSensors 2020, 20, 6762; doi:10.3390/s20236762www.mdpi.com/journal/sensors'},\n  {'page_number': 2,\n   'text': 'social abilities of people with ASD increases when an earlier clinical intervention is performed [4], the early detection of ASD characteristics has become a key point of current ASD research.Various instruments for discriminating ASD have been developed, and the commonly accepted gold standard schemes are behavioral assessments, which are time-consuming procedures and require multidisciplinary teams (MDTs). However, most behavioral assessments suﬀer in terms of the stability of their ASD diagnosis as a result of the issues of accessibility or subjectivity and interpretive bias between professions [5]. Therefore, several attempts to develop objective and precise diagnostic methods have been made in multiple ﬁelds, such as genetic determination [6], principle analysis of brain images [7], and physiological approaches [8].One prominent area of behavioral observations is that of infants’ vocal characteristics. Children with ASD are known to have abnormalities in their prosody resulting from deﬁcits in their ability to recognize the inherent mental conditions of others [9], and their atypical vocalizations are known to be monotonous or exaggerated, which can be revealed using various acoustic characteristics, followed by engineering approaches for the discrimination of ASD or typical development (TD) in children based on the vocal and acoustic features. For example, in [10], the researchers estimated deﬁcits in the vocalization of children with ASD at an average age of 18 months, such as “ﬂat” intonation, atypical pitch, or control of volume based on the variability of pitch and the long-term average spectrum (LTAS) using fast Fourier transform, where signiﬁcant diﬀerences were observed in the spectral components at low-band frequencies, as well as spectral peaks and larger pitch ranges and standard deviations. The development of linguistic abilities is also considered to be a distinguishable feature of delayed development in children with ASD. Earlier vocal patterns at age 6–18 months were proven to be diﬀerentiable in a study [11] that aimed to conﬁrm the hypothetical vocal patterns and social quality of vocal behavior in order to diﬀerentiate between ASD and TD cohorts in groups of children aged 0–6, 6–12, and 12–18 months in terms of categorized speech patterns consisting of vocalization, long reduplicated babbling, two-syllable babbling, and ﬁrst words. Evidence of abnormalities in children with ASD were shown, in these cases, as a signiﬁcant decrease in vocalization and ﬁrst word rate, while the diﬀerence in babbling ability between children with ASD and TD was negligible.Given the development and improvement of machine learning algorithms, as the achievement in the performance of state-of-the-art classiﬁcation and discrimination tasks [12], recent attempts to develop automated classiﬁcation methods based on machine learning techniques have been based on the distinctiveness of vocal characteristics, and have been shown to be promising alternatives to the conventional methods in many publications [13]. For examples of machine learning classiﬁcation, the researchers of [14] employed various acoustic–prosodic features, including fundamental frequency, formant frequencies, harmonics, and root mean square signal energy. In their research, support vector machines (SVMs) and probabilistic neural networks (PNNs) were adopted as classiﬁers, which showed eﬀectual accuracy in discriminating children with ASD from children with TD. Meanwhile, the authors of [15] employed more recent deep learning techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) with spectral features from short-time Fourier transform (STFT) and constant Q transform (CQT), to classify children diagnosed using the autism diagnostic observation schedule (ADOS), also showing promising results in multiple outcomes from SVMs, RNNs, and a combination of CNN and RNN classiﬁers.A generalized acoustic feature set, an extended version of the Geneva minimalistic acoustic parameter set (eGeMAPS) [16], and the bidirectional long short-term memory (BLSTM) model were adopted to diﬀerentiate between children with ASD and children with TD in [17], showing that 75% of the subjects’ utterances were correctly classiﬁed with the simple application of a deep learning model and feature sets. While the quality of previous research based on various acoustic features has proven the eﬀectiveness of acoustic features and classiﬁcation algorithms for the detection of abnormalities in children’s voices in ASD group compared to those of TD group, the complexity and relationship being inherent between the features will remain uncertain until a large amount of data can be accumulated. Furthermore, a limitation still remains in terms of the problems regarding data collection, since there are'},\n  {'page_number': 3,\n   'text': 'diﬃculties pertaining to the need to secure the anonymity of infant subjects, as well as the unintended ignorance of parents at earlier stages of their infant’s development. The data of infants are, accordingly, dispersed by gender, age, and number of vocalizations, or consist of comparably small volumes of audio engineering data in general. These problems were typically overlooked by previous research with controlled and small amounts of data.In order to provide suggestions for a method to overcome the abovementioned restrictions, we focus on examining the feasibility of neural networks as a feature extractor, employing an auto-encoder (AE), which can modify acoustic features into lowered and separable feature dimensions [18]. We construct a simple six-layered stacked AE that contains an input layer, three fully connected (FC) layers, an output layer, and one auxiliary output layer, which has categorical targets for ASD and TD for the optimization of the latent feature space of the AE. We train the AE and deep learning models and compare the results for each model based on SVMs and vanilla BLSTM, while adopting the same model parameters from the method suggested in [17].The remainder of this paper is organized as follows. Section 2 describes the speciﬁcations of the participants’ data, data processing, feature extraction, statistical analysis, and experimental setup. Section 3 presents the performance evaluations for each algorithm of the SVMs and vanilla BLSTM. Lastly, Section 4 concludes the paper.2. Proposed MethodThis study was based on the audio data from video recordings of ASD diagnoses, which were collected from 2016 to 2018 at Seoul National University Bundang Hospital (SNUBH). We received approval from the Institutional Review Board (IRB) at SNUBH to use fully anonymized data for retrospective analysis (IRB no: B-1909/567-110) from existing research (IRB no: B-1607/353-005). We collected the audio data of 39 infants who were assessed using seven multiple instruments, consisting of (1) ADOS, second edition (ADOS-2), (2) the autism diagnostic interview, revised (ADI-R), (3) the behavior development screening for toddlers interview (BeDevel-I), (4) the behavior development screening for toddlers play (BeDevel-P), (5) the Korean version of the childhood autism rating scale (K-CARS) reﬁned from CARS-2, (6) the social communication questionnaire (SCQ), and (7) the social responsiveness scale (SRS) [19–22]. The ﬁnal diagnosis was based on the best clinical estimate diagnosis according to the DSM-5 ASD criteria by a licensed child psychiatrist using all of the available participant information. The participants’ ages ranged between 6 and 24 months, where the average age was 19.20 months with a standard deviation (SD) of 2.52 months. Note here that the age means the age at the time when each infant visited the hospital to undergo an initial diagnosis examination. There were four males and six females diagnosed with ASD, whose average age was 14.72 months with a SD of 2.45. The remaining participants consisted of TD children (19 males and 10 females). Table 1 displays the collected data distribution, while Table 2 shows detailed information of collected data from the infants.Table 1. Distribution of age and gender (male/female).Ages (Month) 6–12 months 12–18 months 18–24 months Age (average ± SD) No. of Subjects Diagnosed as ASD 0 1 M/3 F 3 M/3 F 19.20 ± 2.52 No. of Subjects Diagnosed as TD 5 M/1 F 14 M/9 F 0 14.72 ± 2.45 No. of Infant Subjects 5 M/1 F 15 M/12 F 3 M/3 F 15.92 ± 3.172.1. Data Collection and Acoustic Feature Extraction'},\n  {'page_number': 4,\n   'text': 'Table 2. Detailed information on the age, gender, and initial and deﬁnite diagnosis dates of each infant in Table 1.Infant ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 Age (Months) on Initial Diagnosis Date 18 18 10 13 22 16 17 14 18 14 17 12 9 18 18 24 19 19 18 12 16 20 15 17 16 12 17 17 14 16 12 15 16 13 15 13 21 14 14 Gender Male Male Male Male Female Male Female Female Male Male Female Female Male Female Male Female Male Male Female Male Female Male Male Female Male Male Female Male Male Male Male Male Male Female Female Male Male Male Male Initial Diagnosis Date (Year/Month/Day) 2018/07/28 2017/07/27 2018/08/10 2017/06/10 2018/01/31 2018/03/17 2018/06/30 2018/01/06 2018/07/17 2017/11/04 2017/06/29 2018/01/20 2017/02/18 2017/03/04 2018/05/19 2018/08/08 2018/02/24 2017/04/18 2017/03/04 2016/12/31 2018/03/16 2017/10/14 2018/05/09 2017/02/04 2018/03/17 2018/03/29 2017/01/25 2018/02/08 2018/01/13 2016/11/30 2017/03/22 2017/03/11 2017/12/05 2017/12/13 2017/03/25 2018/08/25 2017/06/24 2017/02/22 2018/01/27 Deﬁnite Final Diagnosis Date (Year/Month/Day) 2018/08/28 2017/08/27 2018/09/10 2017/07/10 2018/02/28 2018/04/17 2018/07/30 2018/02/06 2018/08/17 2017/12/04 2017/07/29 2018/02/20 2017/03/18 2017/04/04 2018/06/19 2018/09/08 2018/03/24 2017/05/18 2017/04/04 2017/01/31 2018/04/16 2017/11/14 2018/06/09 2017/03/04 2018/04/17 2018/04/29 2017/02/25 2018/03/08 2018/02/13 2016/12/30 2017/04/22 2017/04/11 2018/01/05 2018/01/13 2018/04/25 2018/09/25 2017/07/24 2017/03/22 2018/02/27 ASD/TD TD TD TD TD ASD TD TD TD TD TD ASD TD TD ASD TD ASD ASD ASD TD TD TD ASD ASD TD TD TD TD ASD TD TD TD TD TD TD TD TD ASD TD TDAs each infant’s audio data were recorded during the clinical procedure to elicit behaviors from infants, with the attendance of one doctor or clinician and one or both parents with the child in the clinical area, the audio components consisted of various speeches from the child, the clinician, and the parent(s), as well as noises from toys or dragging chairs. Note here that the recordings were done in one of two typical clinical rooms in SNUBH, where the room dimensions were 365 cm × 400 cm × 270 cm and 350 cm × 350 cm × 270 cm, and the hospital noise level was around 40 dB. In order to analyze the vocal characteristics of the infants, each audio clip was processed and split into audio segments containing the infant’s voice, not disturbed by music or clattering noises from toys or overlapped by the voices of the clinician or parent(s). Each segment was classiﬁed into one of ﬁve categories, labeled from 0 to 4, for measuring the data distribution. Each label was intended to show diﬀerentiable characteristics relative to the children’s linguistic development: (1) 0 for one syllable, which is a short,'},\n  {'page_number': 5,\n   'text': \"momentary single vocalization such as “ah” or “ba”; (2) 1 for two syllables, commonly denoted as canonical babbling, as a reduplication of clear babbling of two identical or variant syllables such as “baba” or “baga”; (3) 2 for babbling, not containing syllables; (4) 3 for ﬁrst word, such as “mother” or “father”; and (5) 4 for atypical voice, including screaming or crying. The distribution of each type of vocalization in seconds is shown in Table 3. The number of vocalizations per category is presented along with a rational value considering the diﬀerence between the ASD and TD groups. While the data were unbalanced and very small, the distribution of ASD and TD vocalizations show the same tendency as reported in [10], where the ASD group showed a signiﬁcantly lower ratio of ﬁrst words and an increased ratio of atypical vocalizations, revealing developmental delay in linguistic ability.Table 3. Amount (ratio) of each type of vocalization in seconds.For acquiring qualiﬁed and eﬀective feature sets for the vocal data, eGeMAPS was employed for voice feature extraction. GeMAPS is a popular feature set providing minimalistic speech features generally utilized for automatic voice analysis rather than as a large brute force parameter set. As an extended version, eGeMAPS contains 88 acoustic features that were fully utilized in this experiment. Each recorded set of audio data stored as a 48 kHz stereo ﬁle was down-sampled and down-mixed into a 16 kHz mono-audio ﬁle, taking into consideration its usability and resolution in mel-frequency cepstral coeﬃcients (MFCCs). To extract the speech features for ASD classiﬁcation, each infant’s utterances were segmented into 25 ms frames with a 10 ms overlap between frames. Then, 88 diﬀerent features of the eGeMAPS were extracted for each frame with open source speech and music interpretation using the large-space extraction (OpenSMILE) toolkit [23], and these features were normalized by mean and standard deviation. The normalization scaling was acquired and ﬁxed by normalizing the factors of the training data set. The features were grouped for each ﬁve frames considering the time-relevant characteristics of the speech data.2.2. Pre-Trained AE for Acoustic FeaturesTo further process and refine the acoustic data, a feature-extracting AE was introduced. An AEis a hierarchical structure that is trained as a regression model for reproducing the input parameters. The AE takes inputs and converts them into latent representations, and then reconstructs the input parameters from the latent values [24]. If we consider an input of AE, x € R, then the latent representation z € RY and the reconstruction of the input y € R? are obtained by applying a nonlinear activation function f to the weight sum of z using a weighting matrix W € R4’ and a bias vector b € Rd’, such asWTx + b z = f (1)WTz + b y = fy= f(WTz + b') 2 where T is a matrix transpose operator. When the latent dimension d” < d, the output from the latent layer is considered to be a compressed, meaningful value extracted from the input, which is also noted as a bottleneck feature [25].The normalized eGeMAPS features were applied to train the feature-extracting AE, applying the same data as the input and the target. The AE model contained a latent layer with a lowered, compacted feature dimension compared to the input layer to achieve the useful bottleneck feature.Vocal Label ASD TD 0 1 2 3 4 Total 80.134 (0.104) 314.405 (0.409) 33.241 (0.043) 8.311 (0.011) 333.400 (0.433) 769.491 267.897 (0.250) 443.498 (0.414) 34.766 (0.032) 57.286 (0.054) 266.794 (0.249) 1070.241\"},\n  {'page_number': 6,\n   'text': '6 of 11The model was symmetrically structured, centering around the latent layer, and the model could be divided into two components: the encoder, consisting of layers from the input to the latent layers, and a decoder, consisting of layers from the bottleneck to the output layers.The AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the dimensions of 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, respectively. The hidden dimension was selected experimentally and the bottleneck feature dimension was used for comparison with previous research [17], where 54 features were selected considering the statistical dissimilarity of the distributions between the ASD and TD features based on the Mann–Whitney U test [26]. We additionally introduced an auxiliary output as the binary categorical target for ASD and TD, which is known as the semi-supervised method, to train the AE model eﬀectively [27]. The auxiliary output is depicted as Aux in Figure 1. The reconstructed features and auxiliary classiﬁcation can be written aszi = f (Wi−1,izi−1 + bi−1,i)  (3)  yrec = W3,4z3 + b3,4 yaux = ∂(W2,az2 + b2,a)  (4)  (5)  where yrec refers to the reconstructed eGeMAPS features, yaux is the auxiliary classiﬁcation result, f is the activation function, and ∂ is the softmax activation. yrec = W3,4z3 + b3,4 yaux = ∂(W2,az2 + b2,a)where theFigure 1. Structure of a semi-supervised auto-encoder (AE) model. eGeMAPS, extended version of the Geneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical development.The losses of the reconstruction error for main AE target are measured using the mean absolute error, while the auxiliary ASD/TD target loss is the binary cross-entropy, and they are added and simultaneously optimized with rational hyper-parameters. The overall loss equation is() Lyecon = %i\\'yim - in\" i=1(7)(8)where Lrecon, Laux, and Ltotal denote the reconstruction error, auxiliary loss using a binary cross-entropy loss function, and total loss, respectively.where z1 = f (W0,1x + b0,1), andLaux = −y1gt log(y1aux ) − (1 − t)y1gt log(y1aux ) Ltotal = Lrecon + αLaux'},\n  {'page_number': 7,\n   'text': '7 of 11 Forthe proportion normalizationwe fetchedtraining2.3. EstablishingAs thethe machineEach modelclassificationsupervised a positive We composed with 88BLSTMprepared the SVMsfive input activation, adaptive was controlledsaving the of speech audioformerly, vocalizationmodel wasan SVMmodel, andFigure 2. Structure of a joint optimization model of an auto-encoder (AE) and bidirectional long short-term memory (BLSTM).The performance of each method was evaluated through ﬁve-fold cross validation, where 95 average ASD utterances and 130 average TD utterances were proportionally distributed over ﬁve3. Performance Evaluation'},\n  {'page_number': 8,\n   'text': '8 of 11cases of vocalizations for the generalized estimation of unconcentrated utterance data. The averaged performances of the ﬁve validation splits of each model are described in Table 4. The labeled names of the BLSTM were used as the features for training the BLSTM model, where eGeMAPS-88 denotes 88 features of eGeMAPS, eGeMAPS-54 denotes 54 features selected by the Mann–Whitney U test, and AE-encoded denotes the joint optimized model. In the classiﬁcation stage, one utterance was processed in the frame-wise method and the softmax output was converted to class indices 0 and 1, and if the average of class indices of the frames was over 0.5, then the utterance was considered an ASD child’s utterance. The performances were scored with conventional measures, as well as unweighted average recall (UAR) and weighted average recall (WAR), chosen in the INTERSPEECH 2009 Emotion challenge, which considered imbalanced classes [34]. In the experiment, the SVM model showed very low precision, which was extremely biased toward the TD class. The BLSTM classiﬁer with 88 features of eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD children, while the AE model showed only marginal improvement in correctly classifying children with ASD compared to eGeMAPS-88. The 54 selected features showed degraded quality compared to eGeMAPS-88, obtaining more biased results toward children with TD.Table 4. Classiﬁcation results from the support vector machine (SVM), BLSTM with 88 or 54 eGeMAPS features, 54 selected eGeMAPS features, and BLSTM with AE-encoded features.Models SVM BLSTM (eGeMAPS-54) BLSTM (eGeMAPS-88) BLSTM (AE-Encoded) Predicted To ASD TD ASD TD ASD TD ASD TD ASD TD 62 413 18 632 170 305 103 547 196 279 99 551 215 260 98 552 Accuracy Precision Recall F1 score UAR 0.6178 0.1305 0.7750 0.2234 0.5514 0.6373 0.3579 0.6227 0.4545 0.5997 0.6640 0.4126 0.6644 0.5091 0.6302 0.6818 0.4526 0.6869 0.5457 0.6509UAR, unweighted average recall.4. DiscussionThe vanilla BLSTM model presented in [17] conducted discrimination on well-classiﬁed subjects with 10-month-old children and sorted 54 features from eGeMAPS that had a distinctive distribution between ASD and TD selected by the Mann–Whitney U test using the three-fold cross-validation method. However, because the diﬀerence in the data distribution failed to achieve the same eGeMAPS feature selection between the test and classiﬁcation results with the speciﬁed feature set presented herein, the application of an identical model structure and the adoption of the same feature domain will allow both approaches to be indirectly comparable.These results can be interpreted by the data distributions, and we performed t-stochastic neighbor embedding (t-SNE) analysis [35] on the training data set, which can nonlinearly squeeze the data dimension based on a machine learning algorithm. Figure 3 shows each data distribution as a two-dimensional scatter plot. In the ﬁgure, the eGeMAPS features from eGeMAPS-88 and eGeMAPS-54 showed almost identical distribution, except for the amount of ASD outliers, which implies that the ASD and TD features in the eGeMAPS features show similar distributions in this experiment. As shown in [16], eGeMAPS includes temporal features that are relevant to vocalizations and utterances; thus, these features might cause confusion regarding the discrimination between ASD and TD. The AE-encoded features, however, showed a redistributed feature map with a more characteristic distribution compared to the eGeMAPS features. This is because the AE-encoded features were compressed into a bottleneck feature, which was derived by weighting the matrix, paying attention to the signiﬁcant parameters'},\n  {'page_number': 9,\n   'text': 'Sensors 2020, 20, 6762  9 of 11  while reducing the inﬂuence from the ambiguous parameters. While the joint optimization model achieved only marginally improved results compared to eGeMAPS-88, the distribution of the feature map would be more noticeable in improved feature extraction models, as well as more diﬀerentiable in complex models, although BLSTM with eight cells was employed for a comparison with conventional research in this experiment. Figure 3. Two-dimensional scatter plot for (a) eGeMAPS-88, (b) eGeMAPS-54, and (c) the AE processed by t-stochastic neighbor embedding (t-SNE).While the overall performance scores were comparably low for general classiﬁcation problems on account of the subjectivity and complexity of problems, and the limitation in terms of the shortage of data, the results of the jointly optimized model imply the possibility of deep-learning-based feature extraction for the improvement of automated ASD/TD diagnosis under restricted circumstances.5. ConclusionsIn this paper, we conducted experiments for discovering the possibility of auto-encoder-based feature extraction and a joint optimization method for the automated detection of atypicality in voices of children with ASD during early developmental stages. Under the condition of an insuﬃcient and dispersed data set, the classiﬁcation results were relatively poor in comparison to the general classiﬁcation tasks based on deep learning. Although our investigation used a limited number of subjects and an unbalanced data set, the suggested auto-encoder-based feature extraction and joint optimization method revealed the possibility of feature dimension and a slight improvement in model-based diagnosis under such uncertain circumstances.In future work, we will focus on increasing the reliability of the proposed method by addition of a number of infants’ speech data, reﬁnement of the acoustic features, an auto-encoder for feature extraction, and better, deeper, and up-to-date model structures. This research can also be extended to children with the age of 3 or 4 who can speak several sentences. In this case, we will investigate the linguistic features, as well as acoustic features, such as we have done in this paper. In addition to ASD detection, this research can be applied to the detection of infants with development delays.Author Contributions: All authors discussed the contents of the manuscript. H.K.K. contributed to the research idea and the framework of this study; G.B. and H.J.Y. provided the database and helped with the discussion; J.H.L. performed the experiments; G.W.L. contributed to the data collection and pre-processing. All authors have read and agreed to the published version of the manuscript.Funding: This work was supported by the Institute of Information & communications Technology Planning & evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00330, Development of AI Technology for Early Screening of Infant/Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior and Response).Conﬂicts of Interest: The authors declare no conﬂict of interest.'},\n  {'page_number': 10, 'text': '10 of 11'}]}"},"metadata":{}}]},{"cell_type":"code","source":"# The dictionary is large and takes a long time to compute so we will save it locally\nimport pickle\n\n# Specify the folder path\nfolder_path = '/kaggle/working/'\n\n# Save the list of dictionaries to a file in the specified folder\nfile_path = os.path.join(folder_path, 'pdf_and_text.pkl')\nwith open(file_path, 'wb') as f:\n    pickle.dump(pdf_and_text, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:21:28.181796Z","iopub.execute_input":"2024-05-01T07:21:28.182303Z","iopub.status.idle":"2024-05-01T07:21:28.193056Z","shell.execute_reply.started":"2024-05-01T07:21:28.182270Z","shell.execute_reply":"2024-05-01T07:21:28.191675Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Loading the saved list of dictionaries**","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/pdf_and_text.pkl', 'rb') as f:\n    loaded_pdf_and_text = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:21:32.731026Z","iopub.execute_input":"2024-05-01T07:21:32.731503Z","iopub.status.idle":"2024-05-01T07:21:32.739308Z","shell.execute_reply.started":"2024-05-01T07:21:32.731470Z","shell.execute_reply":"2024-05-01T07:21:32.737626Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### **Further text processing (splitting pages into sentences)**","metadata":{}},{"cell_type":"code","source":"from spacy.lang.en import English\n\nnlp = English()\n\n# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \nnlp.add_pipe(\"sentencizer\")\n\n# Create a document instance as an example \ndoc = nlp(\"Hello, my name is Amit. This is my notebook related to RAG.\") \nassert len(list(doc.sents)) == 2\n\n# Access the sentences of the document \nlist(doc.sents)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:21:47.765893Z","iopub.execute_input":"2024-05-01T07:21:47.766370Z","iopub.status.idle":"2024-05-01T07:21:53.768703Z","shell.execute_reply.started":"2024-05-01T07:21:47.766334Z","shell.execute_reply":"2024-05-01T07:21:53.767499Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[Hello, my name is Amit., This is my notebook related to RAG.]"},"metadata":{}}]},{"cell_type":"code","source":"for pdf in loaded_pdf_and_text:\n    for page in pdf['text']:\n        page[\"sentences\"] = list(nlp(page[\"text\"]).sents)\n        # Make sure all sentences are strings\n        page[\"sentences\"] = [str(sentence) for sentence in page[\"sentences\"]]\n        # Count the sentences\n        page[\"page_sentence_count_spacy\"] = len(page[\"sentences\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:22:00.177724Z","iopub.execute_input":"2024-05-01T07:22:00.178935Z","iopub.status.idle":"2024-05-01T07:22:01.971638Z","shell.execute_reply.started":"2024-05-01T07:22:00.178894Z","shell.execute_reply":"2024-05-01T07:22:01.970131Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"loaded_pdf_and_text[5]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-01T07:22:17.487357Z","iopub.execute_input":"2024-05-01T07:22:17.487837Z","iopub.status.idle":"2024-05-01T07:22:17.501581Z","shell.execute_reply.started":"2024-05-01T07:22:17.487804Z","shell.execute_reply":"2024-05-01T07:22:17.500509Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'filename': '22_Ouss_ASD.pdf',\n 'text': [{'page_number': 1,\n   'text': 'A R T I C L EO p e n A c c e s sBehavior and interaction imaging at 9 months of age predict autism/intellectual disability in high-risk infants with West syndrome Lisa Ouss1, Giuseppe Palestra 2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4, Hugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1, Bernard Golse1, Rima Nabbout5, Isabelle Desguerre5, Mariana Guergova-Kuras4 and David Cohen 2,3; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1Abstract Automated behavior analysis are promising tools to overcome current assessment limitations in psychiatry. At 9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during a standardized mother–infant interaction. We computed infant hand movements (HM), speech turn taking of both partners (vocalization, pause, silences, overlap) and motherese. Then, we assessed whether multimodal social signals and interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectual disability (ID)) of infants with WS at 4 years. At follow-up, 10 infants developed ASD/ID (WS+). The best machine learning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS+ vs. WS−. The 10 best features to distinguish WS+ and WS− included a combination of infant vocalizations and HM features combined with synchrony vocalization features. These data indicate that behavioral and interaction imaging was able to predict ASD/ ID in high-risk children with WS.IntroductionBehavior and interaction imaging is a promising domain of affective computing to explore psychiatric conditions1–3. researchers have Regarding attempted to identify reliable indicators of neurodevelop- mental disorders (NDD) in high-risk populations (e.g., sib- lings of children with autism) during the ﬁrst year of life to recommend early interventions4,5. However, social signals and any alterations of them are very difﬁcult to identify at such a young age6. In addition, exploring the quality and dynamics of early interactions is a complex endeavor. It usually requires (i) the perception and integration of mul- timodal social signals and (ii) an understanding of how twoCorrespondence: Lisa Ouss (lisa.ouss@aphp.fr) or David Cohen (david.cohen@aphp.fr) 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France Full list of author information is available at the end of the articleinteractive partners synchronize and proceed in turn taking7,8.Affective computing offers the possibility to simulta- neously analyze the interaction of several partners while considering the multimodal nature and dynamics of social signals and behaviors9. To date, few seminal studies have attempted to social mother–infant interactions with or without a speciﬁc condition, and these studies have focused on speech turns (e.g., Jaffe et al.10), motherese11, head movements12, hand movements13, movement facial expressions3.Here, we focused on West syndrome (WS), a rare epi- leptic encephalopathy with early onset (before age 1 year) and a high risk of NDD outcomes, including one-third of WS children showing later autism spectrum disorder (ASD) and/or intellectual disability (ID). We recruited 32 infants with WS and 19 typically developing (TD) controls to participate in a standardized early mother–infant© The Author(s) 2020',\n   'sentences': ['A R T I C L EO p e n A c c e s sBehavior and interaction imaging at 9 months of age predict autism/intellectual disability in high-risk infants with West syndrome Lisa Ouss1, Giuseppe Palestra 2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4, Hugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1, Bernard Golse1, Rima Nabbout5, Isabelle Desguerre5, Mariana Guergova-Kuras4 and David Cohen 2,3; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1Abstract Automated behavior analysis are promising tools to overcome current assessment limitations in psychiatry.',\n    'At 9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during a standardized mother–infant interaction.',\n    'We computed infant hand movements (HM), speech turn taking of both partners (vocalization, pause, silences, overlap) and motherese.',\n    'Then, we assessed whether multimodal social signals and interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectual disability (ID)) of infants with WS at 4 years.',\n    'At follow-up, 10 infants developed ASD/ID (WS+).',\n    'The best machine learning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS+ vs. WS−. The 10 best features to distinguish WS+ and WS− included a combination of infant vocalizations and HM features combined with synchrony vocalization features.',\n    'These data indicate that behavioral and interaction imaging was able to predict ASD/ ID in high-risk children with WS.IntroductionBehavior and interaction imaging is a promising domain of affective computing to explore psychiatric conditions1–3.',\n    'researchers have Regarding attempted to identify reliable indicators of neurodevelop- mental disorders (NDD) in high-risk populations (e.g., sib- lings of children with autism) during the ﬁrst year of life to recommend early interventions4,5.',\n    'However, social signals and any alterations of them are very difﬁcult to identify at such a young age6.',\n    'In addition, exploring the quality and dynamics of early interactions is a complex endeavor.',\n    'It usually requires (i) the perception and integration of mul- timodal social signals and (ii) an understanding of how twoCorrespondence: Lisa Ouss (lisa.ouss@aphp.fr) or David Cohen (david.cohen@aphp.fr) 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France Full list of author information is available at the end of the articleinteractive partners synchronize and proceed in turn taking7,8.Affective computing offers the possibility to simulta- neously analyze the interaction of several partners while considering the multimodal nature and dynamics of social signals and behaviors9.',\n    'To date, few seminal studies have attempted to social mother–infant interactions with or without a speciﬁc condition, and these studies have focused on speech turns (e.g., Jaffe et al.10), motherese11, head movements12, hand movements13, movement facial expressions3.Here, we focused on West syndrome (WS), a rare epi- leptic encephalopathy with early onset (before age 1 year) and a high risk of NDD outcomes, including one-third of WS children showing later autism spectrum disorder (ASD) and/or intellectual disability (ID).',\n    'We recruited 32 infants with WS and 19 typically developing (TD) controls to participate in a standardized early mother–infant© The Author(s) 2020'],\n   'page_sentence_count_spacy': 13},\n  {'page_number': 2,\n   'text': 'interaction protocol and followed infants with WS to assess outcomes at 4 years of age. We aim to explore whether multimodal social signals and interpersonal infant–mother interactions at 9 months synchrony of could predict outcomes.Materials and methods Design, participants, and clinical measuresWe performed a prospective follow-up study of infants with WS14. The Institutional Review Board (Comité de Protection des Personnes from the Groupe-Hospitalier Necker Enfants Malades) approved the study, and both parents gave written informed consent after they received verbal and written information on the study. They were asked to participate to a follow-up study to assess out- come of WS taking into account development, early interaction, genetics and response to pharmacological treatment14. The study was conducted from November 2004 to March 2010 in the Neuro-pediatrics Department Center for Rare Epilepsia of Necker Enfants-Malades Hospital, Paris. Of the 41 patients screened during the study period, we enrolled all but two cases (N = 39) with WS. Seven patients dropped out before the age of 3 leading to a sample of 32 patients with detailed follow-up data. Typical developing infants (N = 19) were recruited from Maternal and Infant Prevention institutions, in pediatric consultations, or by proxy.To assess neurodevelopmental outcomes, we focused on ID and ASD. ID was assessed through the Brunet-Lézine Developmental Examination, performed for all children at the age of 3 years. The Brunet-Lézine Developmental Examination estimates a developmental quotient (DQ) based upon normative data available for 3-year-old French toddlers15. The diagnosis of autism was based upon several measurements and an expert assessment that was blind to other variables: (i) At 3 years of age, all parents completed the Autism Diagnostic Interview- Revised (ADI-R) to assess autism signs by dimensions and developmental delay16. (ii) At 2 and 3 years of age, all patients were assessed with the Children’s Autism Rating Scale (CARS)17. (iii) An expert clinician (LR) who was blind to child history assessed autism and ID from 20-min videotapes of child/mother play at 2 years of age. Finally, diagnoses of ASD and/or ID at age 4 were based upon a consensus approach using direct assessment of the child by a clinician with expertise in autism (LO) as well as by clinical information from the CARS, ADI-R, and DQ.Video recordingsInfant–mother interactions were assessed between 9 and 12 months of age during a play session (Fig. 1). Two synchronized cameras (face and proﬁle; Fig. S1A) recor- ded the movements in two dimensions while the infant was sitting in a baby chair. Audio interactions were alsoAround 9 months —_—————— At4years WS with Lab recording of infant-mother 10) - = ID/ASD interaction Audio features extraction vs. WS Infant: typical vocalization, without fifant atypical vocalization, pause WS with Motheﬁ ID/ASD (N ID/ASD 32) Mother: vocalization, = motherese, pause (N Synchrony: overlap, silence, infant synchrony ratio 22) = = West syndrome chidlren Machine 3 sequences of interaction learning WS without classification ID/ASD (N Free interaction 19) = = 19) Interaction = = — with the giraffe West syndrome controls (N: Typical developing Mother singing controls (N: vs. TD Typical developing Fig. 1 Pipeline of our machine learning approach to classify WS vs. TD.Infant  Mother Fig. 1 Pipeline of our machine learning approach to classify WS vs. TD.',\n   'sentences': ['interaction protocol and followed infants with WS to assess outcomes at 4 years of age.',\n    'We aim to explore whether multimodal social signals and interpersonal infant–mother interactions at 9 months synchrony of could predict outcomes.',\n    'Materials and methods Design, participants, and clinical measuresWe performed a prospective follow-up study of infants with WS14.',\n    'The Institutional Review Board (Comité de Protection des Personnes from the Groupe-Hospitalier Necker Enfants Malades) approved the study, and both parents gave written informed consent after they received verbal and written information on the study.',\n    'They were asked to participate to a follow-up study to assess out- come of WS taking into account development, early interaction, genetics and response to pharmacological treatment14.',\n    'The study was conducted from November 2004 to March 2010 in the Neuro-pediatrics Department Center for Rare Epilepsia of Necker Enfants-Malades Hospital, Paris.',\n    'Of the 41 patients screened during the study period, we enrolled all but two cases (N = 39) with WS.',\n    'Seven patients dropped out before the age of 3 leading to a sample of 32 patients with detailed follow-up data.',\n    'Typical developing infants (N = 19) were recruited from Maternal and Infant Prevention institutions, in pediatric consultations, or by proxy.',\n    'To assess neurodevelopmental outcomes, we focused on ID and ASD.',\n    'ID was assessed through the Brunet-Lézine Developmental Examination, performed for all children at the age of 3 years.',\n    'The Brunet-Lézine Developmental Examination estimates a developmental quotient (DQ) based upon normative data available for 3-year-old French toddlers15.',\n    'The diagnosis of autism was based upon several measurements and an expert assessment that was blind to other variables: (i) At 3 years of age, all parents completed the Autism Diagnostic Interview- Revised (ADI-R) to assess autism signs by dimensions and developmental delay16. (',\n    'ii) At 2 and 3 years of age, all patients were assessed with the Children’s Autism Rating Scale (CARS)17. (',\n    'iii) An expert clinician (LR) who was blind to child history assessed autism and ID from 20-min videotapes of child/mother play at 2 years of age.',\n    'Finally, diagnoses of ASD and/or ID at age 4 were based upon a consensus approach using direct assessment of the child by a clinician with expertise in autism (LO) as well as by clinical information from the CARS, ADI-R, and DQ.Video recordingsInfant–mother interactions were assessed between 9 and 12 months of age during a play session (Fig.',\n    '1).',\n    'Two synchronized cameras (face and proﬁle; Fig.',\n    'S1A) recor- ded the movements in two dimensions while the infant was sitting in a baby chair.',\n    'Audio interactions were alsoAround 9 months —_—————— At4years WS with Lab recording of infant-mother 10) - = ID/ASD interaction Audio features extraction vs. WS Infant: typical vocalization, without fifant atypical vocalization, pause WS with Motheﬁ ID/ASD (N ID/ASD 32) Mother: vocalization, = motherese, pause (N Synchrony: overlap, silence, infant synchrony ratio 22) = = West syndrome chidlren Machine 3 sequences of interaction learning WS without classification ID/ASD (N Free interaction 19) = = 19) Interaction = = — with the giraffe West syndrome controls (N: Typical developing Mother singing controls (N: vs. TD Typical developing Fig.',\n    '1 Pipeline of our machine learning approach to classify WS vs. TD.Infant  Mother Fig.',\n    '1 Pipeline of our machine learning approach to classify WS vs. TD.'],\n   'page_sentence_count_spacy': 22},\n  {'page_number': 3,\n   'text': 'recorded. The standardized situation encompassed three sequences of 3 min: (sequence 1) free play after instruct- ing the mother to interact “as usual” without any toy; (sequence 2) free play using the help of a toy (Sophie the giraffe); (sequence 3) mother singing to her baby. Due to the position of the baby chair on the ﬂoor and the mother’s seated position, the mother was positioned slightly higher in all of the recordings. The mother’s indicated position was on the left of the child as shown on the picture, but exceptions were sometimes observed during the recordings. For infant hand movement (HM) features, 1 min was extracted from each 3-min video and all recordings, according to two criteria: the child’s hands should be visible for at least part of the sequence (e.g., the mother is not leaning on the child), and the minute represented the greatest amount of interaction between the mother and the child. For audio and speech turn- taking computing, we only used the 3-min audio recording of sequence 1.Vision computing (Fig. S1B, vision computing panel)To process infant hand movements (HM), we used the methods developed in Ouss et al.13. Here, we summarize the successive steps to calculate HM features. In step 1 (hand trajectory extraction and data processing), the two- dimensional coordinates of the hand were extracted from each of the video recordings by tracking a wristband on the right hand (yellow in Fig. S1A, video-audio recording panel). The tracking framework comprised three steps: prediction, observation, and estimation as proposed in ref. 18. As the hand motion was highly nonlinear, we developed an approach using a bootstrap-based particle ﬁlter with a ﬁrst-order model to address abrupt changes in direction and speed19,20. To address hand occlusion, we implemented an approach combining tracking with detection by adding a boolean variable to the state vector associated with each particle18.Each extracted trajectory consisted of 1500 pairs of x and y coordinates (25 frames per second, generating 1500 pairs of coordinates in the 60 s; see Fig. S1 left panel, vision computing). The frames where the hand was not visible were clearly indicated in each trajectory as missing coordinates for these time points. To account for differ- ences in the camera zoom parameters, the trajectories obtained were normalized using a ﬁxed reference system present in the settings of each video recording. The nor- malization was performed on all trajectories, and 95% of the normalization factors ranged between 0.8 and 1.22 with a few outlier trajectories that required greater cor- rection. Forty-one percent of the trajectories required <5% correction. Although the recordings between the two cameras were synchronized and in principle allowed 3D reconstruction of the trajectory, the accumulation of missing data prevented such reconstruction. However, 2Dmotion capture with appropriately deﬁned movement descriptors can be powerful for detecting clinically rele- vant changes21, thereby justifying the independent ana- lysis of the 2D-trajectory videos (see Fig. S1B, vision computing, 2d panel on the left).In step 2, the descriptors of the HM were calculated from the planar trajectories (Fig. S1B, table shown in the vision computing panel). Descriptors covered those already reported in the literature as important in char- acterizing infants’ HM21. (1) To describe the space explored by the hand, we calculated the maximum dis- tance observed on the two axes (xRange, yRange) and the standard deviation of the X and Y coordinates observed during the 60 s (xSd, ySd). We also calculated the max- imum distance between any two points of the trajectory using (http://algs4.cs. java princeton.edu/code/) (Fig. S1B, vision computing panel, red line in the third panel from the left). (2) To evaluate HM dynamics, we calculated the velocity and accelera- tion. (3) Also related to HM dynamics, we calculated HM pauses deﬁned as part of the trajectory in which the velocity was lower than a speciﬁc threshold for a mini- mum duration of 4 s. (4) Finally, the curvature of the trajectories was calculated using a standard deﬁnition of the curvature (κ) of plane curves in Cartesian coordinates as γ(t) = (x(t), y(t)). The curvature calculated at each point of the trajectory is presented in the right panel of Fig. S1B (video computing), where the ﬁrst 1.2 s of the trajectory are plotted and the associated calculated curvatures at each point (and respective time, indicated on the axis) are presented as columns.Audio computing (Fig. S1C, audio computing)We extracted two types of audio social signals from the audio channel of the mother–infant interaction: speech turn taking (STT) and motherese. For STT extraction, we followed the methods developed by Weisman et al.22 and Bourvis et al.23 (Fig. S1, audio computing). First, we used ELAN to segment the infants’ and mothers’ speech turns and annotate the dialog acts. Mothers’ audio interactions were categorized as mother vocalization (meaningful laugh, singing, animal sounds) or other vocalizations, noise (clap hands, snap ﬁngers or snap the tongue, mouth infants’ audio production was noise, etc.). Similarly, deﬁned as infant vocalization (babbling vocalizations, laugh, and cry) or atypical vocalization (other noise such as “rale”). The infant’s and mother’s utterances were labeled by two annotators (blind to group status). Cohen’s kappa between the two annotators was calculated for each dyad, each task and each item of the grid. For all items, the kappa values were between 0.82 and 1.From the annotation, we extracted all the speech turns of the infant and the mother. A speech turn is a con- tinuous stream of speech with <150 ms of silence. We',\n   'sentences': ['recorded.',\n    'The standardized situation encompassed three sequences of 3 min: (sequence 1) free play after instruct- ing the mother to interact “as usual” without any toy; (sequence 2) free play using the help of a toy (Sophie the giraffe); (sequence 3) mother singing to her baby.',\n    'Due to the position of the baby chair on the ﬂoor and the mother’s seated position, the mother was positioned slightly higher in all of the recordings.',\n    'The mother’s indicated position was on the left of the child as shown on the picture, but exceptions were sometimes observed during the recordings.',\n    'For infant hand movement (HM) features, 1 min was extracted from each 3-min video and all recordings, according to two criteria: the child’s hands should be visible for at least part of the sequence (e.g., the mother is not leaning on the child), and the minute represented the greatest amount of interaction between the mother and the child.',\n    'For audio and speech turn- taking computing, we only used the 3-min audio recording of sequence 1.Vision computing (Fig.',\n    'S1B, vision computing panel)To process infant hand movements (HM), we used the methods developed in Ouss et al.13.',\n    'Here, we summarize the successive steps to calculate HM features.',\n    'In step 1 (hand trajectory extraction and data processing), the two- dimensional coordinates of the hand were extracted from each of the video recordings by tracking a wristband on the right hand (yellow in Fig.',\n    'S1A, video-audio recording panel).',\n    'The tracking framework comprised three steps: prediction, observation, and estimation as proposed in ref.',\n    '18.',\n    'As the hand motion was highly nonlinear, we developed an approach using a bootstrap-based particle ﬁlter with a ﬁrst-order model to address abrupt changes in direction and speed19,20.',\n    'To address hand occlusion, we implemented an approach combining tracking with detection by adding a boolean variable to the state vector associated with each particle18.Each extracted trajectory consisted of 1500 pairs of x and y coordinates (25 frames per second, generating 1500 pairs of coordinates in the 60 s; see Fig.',\n    'S1 left panel, vision computing).',\n    'The frames where the hand was not visible were clearly indicated in each trajectory as missing coordinates for these time points.',\n    'To account for differ- ences in the camera zoom parameters, the trajectories obtained were normalized using a ﬁxed reference system present in the settings of each video recording.',\n    'The nor- malization was performed on all trajectories, and 95% of the normalization factors ranged between 0.8 and 1.22 with a few outlier trajectories that required greater cor- rection.',\n    'Forty-one percent of the trajectories required <5% correction.',\n    'Although the recordings between the two cameras were synchronized and in principle allowed 3D reconstruction of the trajectory, the accumulation of missing data prevented such reconstruction.',\n    'However, 2Dmotion capture with appropriately deﬁned movement descriptors can be powerful for detecting clinically rele- vant changes21, thereby justifying the independent ana- lysis of the 2D-trajectory videos (see Fig.',\n    'S1B, vision computing, 2d panel on the left).In step 2, the descriptors of the HM were calculated from the planar trajectories (Fig.',\n    'S1B, table shown in the vision computing panel).',\n    'Descriptors covered those already reported in the literature as important in char- acterizing infants’ HM21. (',\n    '1) To describe the space explored by the hand, we calculated the maximum dis- tance observed on the two axes (xRange, yRange) and the standard deviation of the X and Y coordinates observed during the 60 s (xSd, ySd).',\n    'We also calculated the max- imum distance between any two points of the trajectory using (http://algs4.cs.',\n    'java princeton.edu/code/) (Fig.',\n    'S1B, vision computing panel, red line in the third panel from the left). (',\n    '2) To evaluate HM dynamics, we calculated the velocity and accelera- tion. (',\n    '3) Also related to HM dynamics, we calculated HM pauses deﬁned as part of the trajectory in which the velocity was lower than a speciﬁc threshold for a mini- mum duration of 4 s. (4) Finally, the curvature of the trajectories was calculated using a standard deﬁnition of the curvature (κ) of plane curves in Cartesian coordinates as γ(t) = (x(t), y(t)).',\n    'The curvature calculated at each point of the trajectory is presented in the right panel of Fig.',\n    'S1B (video computing), where the ﬁrst 1.2 s of the trajectory are plotted and the associated calculated curvatures at each point (and respective time, indicated on the axis) are presented as columns.',\n    'Audio computing (Fig.',\n    'S1C, audio computing)We extracted two types of audio social signals from the audio channel of the mother–infant interaction: speech turn taking (STT) and motherese.',\n    'For STT extraction, we followed the methods developed by Weisman et al.22 and Bourvis et al.23 (Fig.',\n    'S1, audio computing).',\n    'First, we used ELAN to segment the infants’ and mothers’ speech turns and annotate the dialog acts.',\n    'Mothers’ audio interactions were categorized as mother vocalization (meaningful laugh, singing, animal sounds) or other vocalizations, noise (clap hands, snap ﬁngers or snap the tongue, mouth infants’ audio production was noise, etc.).',\n    'Similarly, deﬁned as infant vocalization (babbling vocalizations, laugh, and cry) or atypical vocalization (other noise such as “rale”).',\n    'The infant’s and mother’s utterances were labeled by two annotators (blind to group status).',\n    'Cohen’s kappa between the two annotators was calculated for each dyad, each task and each item of the grid.',\n    'For all items, the kappa values were between 0.82 and 1.From the annotation, we extracted all the speech turns of the infant and the mother.',\n    'A speech turn is a con- tinuous stream of speech with <150 ms of silence.',\n    'We'],\n   'page_sentence_count_spacy': 44},\n  {'page_number': 4,\n   'text': 'obtained a list of triples: speaker label (infant or mother), start time, and duration of speech turn. From these triples, we also deduced the start time and duration of the time segments when the mother or the infant were not speaking (pauses). Therefore, we extracted Mother Vocalizations; Mother Other Noise; Infant Vocalizations; Infant Infant Atypical Vocalizations; Mother Pauses; Pauses. We also extracted three dyadic features: (1) Silence deﬁned as sequences of time during which neither participant was speaking for more than 150 ms; (2) Overlap Ratio deﬁned as the duration of vocalization overlaps between mothers and infants divided by the duration of the total interaction. This ratio measures the proportion of interactional time in which both partici- pants were simultaneously vocalizing; (3) Infant Syn- chrony Ratio deﬁned as the number of infants’ responses to their mother’s vocalization within a time limit of 3 s divided by the number of mother vocalizations during the time paradigm. The 3-s window was based on the avail- able literature on synchrony7,24.From the mother vocalizations, we also computed affective speech analysis, as previous work has shown that motherese may shape parent-infant interactions25. The segments of mother vocalizations were analyzed using a computerized classiﬁer for categorization as “motherese” or “non-motherese/other speech” initially developed to analyze home movies11. The system exploits the fusion of two classiﬁers, namely, segmental and suprasegmental26. Consequently, the utterances are characterized by both frequency cepstrum coefﬁcients) and segmental suprasegmental/prosodics (e.g., statistics with regard to fundamental frequency, energy, and duration) features. The detector used the GMM (Gaussian mixture model) classiﬁer for both segmental and suprasegmental features (M, number of Gaussians for the GMM Classiﬁer: M = 12 and 15, respectively, and λ = weighting coefﬁcient used in the equation fusion: λ = 0.4). For the purpose of the current study, we explored the performance of our motherese classiﬁer in French mothers. We analyzed 200 sequences from French mothers (100 motherese vs. 100 other speech) that were blindly validated by two psycholinguists. We calculated the Intraclass correlation (ICC) between the two raters (the expert and the algo- rithm) and found a good and very signiﬁcant ICC (ICC = 0.79 (95% CI: 0.59–0.90), p < 0.001). This level of predic- tion made it suitable for further analysis of the entire data set.Based on this automatic detection of motherese, we created two subclasses for mother vocalizations: mother- ese vs. non-motherese. Two variables were derived: Motherese Ratio (duration of motherese vocalization/ duration of interaction) and Non-motherese Ratio (dura- tion of non-motherese vocalization/duration of interac- tion). We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio, which reﬂect the ratio of time during which the infant vocalizes in response to his/her mother motherese and other speech (non-motherese).Prediction of the outcome using machine learningThe pipeline of our approach is shown in Fig. 1. First, a data quality analysis was performed to ensure the validity of the data. As expected, all data were available for audio analysis. However, a substantial proportion of the data were discarded due to video recording or vision com- puting issues. We ﬁnally kept 18 video recordings for the WS and 17 videos for the TD groups. Second, given the number of features (21 infant HM for each camera and each sequence; 16 STT) compared with the data set (32 WS and 19 TD), we reduced our data using principal component analysis (PCA). Third, we tested several algorithms to classify WS vs. TD based on the whole data set available for both vision and audio computing features (leave one out) (Table S1). The best algorithm was deci- sion stump27. All results presented here are based on the classiﬁcation with a decision stump algorithm. We also analyzed WS with ID/ASD (WS+) vs. WS without ID/ ASD (WS−). For each classiﬁcation, we also extracted a confusion matrix and explored which individual features to a given classiﬁcation using contributed the most Pearson correlations.ResultsTable S2 summarizes the demographic and clinical characteristics of children with WS. At follow-up, 10 infants out of 32 children with WS developed ASD/ID (WS+). Eight children had ASD and ID, whereas 2 had only ID. As expected, all variables related to ASD and ID were signiﬁcantly different in WS+ compared with WS−. Figure 2a summarizes the best classiﬁcation models using the decision stump algorithm (leave one out). As shown, multimodal classiﬁcation outperformed unimodal classiﬁcation to distinguish WS and TD. Therefore, we only used the multimodal approach to classify WS+ vs. WS−. The best model reached 76.47% accuracy classify- ing WS vs. TD and 81.25% accuracy classifying WS+ vs. WS− based on multimodal features extracted during early interactions. Interestingly, the confusion matrices (Fig. 2b) show that when classifying WS vs. TD, all errors came from TD being misclassiﬁed as WS (N = 12); when clas- sifying WS+ vs. WS−, most errors came from WS+ being misclassiﬁed as WS− (N = 5).Table 1 lists the best features for each multimodal classiﬁcation based on the Pearson correlation values. The best features to distinguish WS and TD included four infant HM features, 1 mother audio feature. In contrast, the best features to distinguish WS+ and WS− included a (N = 2), combination of',\n   'sentences': ['obtained a list of triples: speaker label (infant or mother), start time, and duration of speech turn.',\n    'From these triples, we also deduced the start time and duration of the time segments when the mother or the infant were not speaking (pauses).',\n    'Therefore, we extracted Mother Vocalizations; Mother Other Noise; Infant Vocalizations; Infant Infant Atypical Vocalizations; Mother Pauses; Pauses.',\n    'We also extracted three dyadic features: (1) Silence deﬁned as sequences of time during which neither participant was speaking for more than 150 ms; (2) Overlap Ratio deﬁned as the duration of vocalization overlaps between mothers and infants divided by the duration of the total interaction.',\n    'This ratio measures the proportion of interactional time in which both partici- pants were simultaneously vocalizing; (3) Infant Syn- chrony Ratio deﬁned as the number of infants’ responses to their mother’s vocalization within a time limit of 3 s divided by the number of mother vocalizations during the time paradigm.',\n    'The 3-s window was based on the avail- able literature on synchrony7,24.From the mother vocalizations, we also computed affective speech analysis, as previous work has shown that motherese may shape parent-infant interactions25.',\n    'The segments of mother vocalizations were analyzed using a computerized classiﬁer for categorization as “motherese” or “non-motherese/other speech” initially developed to analyze home movies11.',\n    'The system exploits the fusion of two classiﬁers, namely, segmental and suprasegmental26.',\n    'Consequently, the utterances are characterized by both frequency cepstrum coefﬁcients) and segmental suprasegmental/prosodics (e.g., statistics with regard to fundamental frequency, energy, and duration) features.',\n    'The detector used the GMM (Gaussian mixture model) classiﬁer for both segmental and suprasegmental features (M, number of Gaussians for the GMM Classiﬁer: M = 12 and 15, respectively, and λ = weighting coefﬁcient used in the equation fusion: λ = 0.4).',\n    'For the purpose of the current study, we explored the performance of our motherese classiﬁer in French mothers.',\n    'We analyzed 200 sequences from French mothers (100 motherese vs. 100 other speech) that were blindly validated by two psycholinguists.',\n    'We calculated the Intraclass correlation (ICC) between the two raters (the expert and the algo- rithm) and found a good and very signiﬁcant ICC (ICC = 0.79 (95% CI: 0.59–0.90), p < 0.001).',\n    'This level of predic- tion made it suitable for further analysis of the entire data set.',\n    'Based on this automatic detection of motherese, we created two subclasses for mother vocalizations: mother- ese vs. non-motherese.',\n    'Two variables were derived: Motherese Ratio (duration of motherese vocalization/ duration of interaction) and Non-motherese Ratio (dura- tion of non-motherese vocalization/duration of interac- tion).',\n    'We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio, which reﬂect the ratio of time during which the infant vocalizes in response to his/her mother motherese and other speech (non-motherese).Prediction of the outcome using machine learningThe pipeline of our approach is shown in Fig.',\n    '1.',\n    'First, a data quality analysis was performed to ensure the validity of the data.',\n    'As expected, all data were available for audio analysis.',\n    'However, a substantial proportion of the data were discarded due to video recording or vision com- puting issues.',\n    'We ﬁnally kept 18 video recordings for the WS and 17 videos for the TD groups.',\n    'Second, given the number of features (21 infant HM for each camera and each sequence; 16 STT) compared with the data set (32 WS and 19 TD), we reduced our data using principal component analysis (PCA).',\n    'Third, we tested several algorithms to classify WS vs. TD based on the whole data set available for both vision and audio computing features (leave one out) (Table S1).',\n    'The best algorithm was deci- sion stump27.',\n    'All results presented here are based on the classiﬁcation with a decision stump algorithm.',\n    'We also analyzed WS with ID/ASD (WS+) vs. WS without ID/ ASD (WS−).',\n    'For each classiﬁcation, we also extracted a confusion matrix and explored which individual features to a given classiﬁcation using contributed the most Pearson correlations.',\n    'ResultsTable S2 summarizes the demographic and clinical characteristics of children with WS.',\n    'At follow-up, 10 infants out of 32 children with WS developed ASD/ID (WS+).',\n    'Eight children had ASD and ID, whereas 2 had only ID.',\n    'As expected, all variables related to ASD and ID were signiﬁcantly different in WS+ compared with WS−. Figure 2a summarizes the best classiﬁcation models using the decision stump algorithm (leave one out).',\n    'As shown, multimodal classiﬁcation outperformed unimodal classiﬁcation to distinguish WS and TD.',\n    'Therefore, we only used the multimodal approach to classify WS+ vs. WS−. The best model reached 76.47% accuracy classify- ing WS vs. TD and 81.25% accuracy classifying WS+ vs. WS− based on multimodal features extracted during early interactions.',\n    'Interestingly, the confusion matrices (Fig.',\n    '2b) show that when classifying WS vs. TD, all errors came from TD being misclassiﬁed as WS (N = 12); when clas- sifying WS+ vs. WS−, most errors came from WS+ being misclassiﬁed as WS− (N = 5).Table 1 lists the best features for each multimodal classiﬁcation based on the Pearson correlation values.',\n    'The best features to distinguish WS and TD included four infant HM features, 1 mother audio feature.',\n    'In contrast, the best features to distinguish WS+ and WS− included a (N = 2), combination of'],\n   'page_sentence_count_spacy': 38},\n  {'page_number': 5,\n   'text': '2| o Confusion Machine learning classification Decision stump (leave one out) matrices I West vs. TD (n=51) 90 Clas: ied West T 80 as D West 32 70 el 12 60 50 West with ID/ASD vs. 40 I West with out ID/ASD 30 (N=32) West with West with 20 ID/ASD out ID/ASD West with 5 5 10 ID/ASD West with 1 21 Mutimodal Video Audio out ID/ASDFig. 2 Machine learning classiﬁcation of WS vs. TD and WS+ vs. WS− based on uni- and multimodal features extracted during early infant–mother interaction.Table 1 Best features for classiﬁcation (based on signiﬁcant Pearson’s correlation between feature and class).Feature characteristics Pearson r West vs. Typical developing Ratio of all maternal audio intervention during free interaction Audio, mother 0.35 Total number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 Total number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 Vertical amplitude of the giraffe (front view camera) Video, infant −0.30 Movement acceleration max (side view camera) during free interaction Video, infant 0.29 West with ASD/ID vs. West without ASD/ID Total number of all infant vocalization during free interaction Audio, infant −0.56 Synchrony ratio (infant response to mother) Audio, synchrony −0.55 Ratio of all infant vocalization during free interaction Audio, infant −0.55 Motherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 Non-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 HM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 HM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 HM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 Curvature max (side view camera) during the giraffe interaction Video, infant −0.37 Relative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 p-value 0.012 0.014 0.023 0.032 0.034 <0.001 <0.001 0.001 0.002 0.005 0.008 0.01 0.014 0.039 0.04HM hand movement, ASD autism spectrum disorder, ID intellectual disability, SD standard deviation.',\n   'sentences': ['2| o Confusion Machine learning classification Decision stump (leave one out) matrices I West vs. TD (n=51) 90 Clas: ied West T 80 as D West 32 70 el 12 60 50 West with ID/ASD vs. 40 I West with out ID/ASD 30 (N=32) West with West with 20 ID/ASD out ID/ASD West with 5 5 10 ID/ASD West with 1 21 Mutimodal Video Audio out ID/ASDFig.',\n    '2 Machine learning classiﬁcation of WS vs. TD and WS+ vs. WS− based on uni- and multimodal features extracted during early infant–mother interaction.',\n    'Table 1 Best features for classiﬁcation (based on signiﬁcant Pearson’s correlation between feature and class).Feature characteristics Pearson r West vs. Typical developing Ratio of all maternal audio intervention during free interaction Audio, mother 0.35 Total number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 Total number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 Vertical amplitude of the giraffe (front view camera) Video, infant −0.30 Movement acceleration max (side view camera) during free interaction Video, infant 0.29 West with ASD/ID vs. West without ASD/ID Total number of all infant vocalization during free interaction Audio, infant −0.56 Synchrony ratio (infant response to mother) Audio, synchrony −0.55 Ratio of all infant vocalization during free interaction Audio, infant −0.55 Motherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 Non-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 HM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 HM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 HM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 Curvature max (side view camera) during the giraffe interaction Video, infant −0.37 Relative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 p-value 0.012 0.014 0.023 0.032 0.034 <0.001 <0.001 0.001 0.002 0.005 0.008 0.01 0.014 0.039 0.04HM hand movement, ASD autism spectrum disorder, ID intellectual disability, SD standard deviation.'],\n   'page_sentence_count_spacy': 3},\n  {'page_number': 6,\n   'text': 'synchrony vocalization features (N = 3) and infant HM features (N = 5), the last of which showed lower correla- tion scores.DiscussionTo the best of our knowledge, this is the ﬁrst study to apply multimodal to mother–infant interactions in the context of WS. Com- bining an infant–mother interaction at 9 months signiﬁcantly pre- dicted the development of ASD or severe to moderate ID at 4 years of age in the high-risk children with WS. Confusion matrices showed that the classiﬁcation errors were not random, enhancing the interest of the compu- tational method proposed here. In addition, the best contributing features for the performed classiﬁcations differed when classifying WS vs. TD and WS+ vs. WS−. Infant HMs were the most signiﬁcant features to distin- guish WS versus TD, probably reﬂecting the motor impact due to acute WS encephalopathy. For classifying WS+ vs. WS−, the contribution of infant audio features and synchrony features became much more relevant combined with several HM features.We believe that the importance of synchrony and reciprocity during early interactions is in line with recent studies that have investigated the risk of ASD or NDD during the ﬁrst year of life from home movies (e.g., refs. 11,24), from prospective follow-up of high-risk infants such as siblings (e.g., refs. 4,28) or infants with WS (e.g., ref. 14), and from prospective studies assessing tools to screen risk for autism (e.g., ref. 29). In the ﬁeld of ASD, synchrony, reciprocity, parental sensitivity, and emotional engagement are now proposed as targets of early interventions30, which could prevent early inter- active vicious circles. Parents of at-risk infants try to compensate for the lack of interactivity of their child by modifying their stimulation and therefore sometimes interactions24. Early reinforcing identiﬁcation of these interactive targets is especially useful among babies with neurological comorbidities because delays and impairments in early social interactions are not sufﬁ- cient to predict ASD.Similarly, we believe that the importance of HM in distinguishing WS vs. TD on one hand, and WS+ vs. WS− on the other hand, is also in line with the studies that investigated the importance of non-social behaviors for investigating the risk of ASD or NDD during the ﬁrst year of life. For example, studying home movies, Purpura et al. found more bilateral HM and ﬁnger movements in infants who will later develop ASD31. Similarly, several prospective follow-up studies of high-risk siblings32–35 or retrospective studies on home movies36,37 reported spe- ciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been assessed with automatized and computational procedures, focusing on eye tracking at early stages38–40, vocal pro- ductions41, analysis of acoustics of ﬁrst utterances or cry episodes42, but none was done in an interactive setting. Our study proposed a paradigm shift from the assessment of infant behavior to dyadic assessment of interactions, as previously achieved in retrospective approaches using home movies24. The aim is not to implement studies of social signal processing in routine clinical work but rather to decompose clinical intuitions and signs and validate the most relevant cues of these clinical features. From clinical work, back to clinics, social signal processing is a rigorous step to help clinicians better identify and assess early targets of interventions.Given the exploratory nature of both our approach and method, our results should be interpreted with caution taking into account strengths (prospective follow-up, automatized multimodal social signal processing, and ecological standardized assessment) and limitations. These limitations include (1) the overall sample size knowing that WS is a rare disease; (2) the high rate of missing data during video recording due to the ecological conditions of the infant–mother interaction (mothers interposing between the camera and the infant); the ﬁnal sample size of WS+ (N = 10) that limited the power of machine learning methods.We conclude that the method proposed here combining multimodal automatized assessment of social signal pro- cessing during early interaction with infants at risk for NDD is a promising tool to decipher clinical features that remain difﬁcult to identify and assess. In the context of WS, we showed that such a method we proposed to label ‘behavioral and interaction imaging’ was able to sig- niﬁcantly predict the development of ASD or ID at 4 years of age in high-risk children who had WS and were assessed at 9 months of age.Acknowledgements The authors thank all of the patients and families who participated in this study. The study was funded by the EADS foundation (PILE), by the Agence Nationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de Recherche en Psychiatrie (GDR-3557). It was partially performed in the Labex SMART (ANR-11-LABX-65), which is supported by French state funds and managed by the ANR in the Investissements d’Avenir program under reference ANR-11-IDEX-0004-02. The sponsors had no involvement in the study design, data analysis, or interpretation of the results.Author details 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France. 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France. 3Département de Psychiatrie de l’Enfant et de l’Adolescent, AP-HP, Hôpital Pitié-Salpêtrière, 47-83, Boulevard de l’Hôpital, 75651 Paris, Cedex 13, France. 4Ariana Pharmaceuticals, Research Department, Paris, France. 5Service de Neuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris, France',\n   'sentences': ['synchrony vocalization features (N = 3) and infant HM features (N = 5), the last of which showed lower correla- tion scores.',\n    'DiscussionTo the best of our knowledge, this is the ﬁrst study to apply multimodal to mother–infant interactions in the context of WS.',\n    'Com- bining an infant–mother interaction at 9 months signiﬁcantly pre- dicted the development of ASD or severe to moderate ID at 4 years of age in the high-risk children with WS.',\n    'Confusion matrices showed that the classiﬁcation errors were not random, enhancing the interest of the compu- tational method proposed here.',\n    'In addition, the best contributing features for the performed classiﬁcations differed when classifying WS vs. TD and WS+ vs. WS−. Infant HMs were the most signiﬁcant features to distin- guish WS versus TD, probably reﬂecting the motor impact due to acute WS encephalopathy.',\n    'For classifying WS+ vs. WS−, the contribution of infant audio features and synchrony features became much more relevant combined with several HM features.',\n    'We believe that the importance of synchrony and reciprocity during early interactions is in line with recent studies that have investigated the risk of ASD or NDD during the ﬁrst year of life from home movies (e.g., refs.',\n    '11,24), from prospective follow-up of high-risk infants such as siblings (e.g., refs.',\n    '4,28) or infants with WS (e.g., ref.',\n    '14), and from prospective studies assessing tools to screen risk for autism (e.g., ref.',\n    '29).',\n    'In the ﬁeld of ASD, synchrony, reciprocity, parental sensitivity, and emotional engagement are now proposed as targets of early interventions30, which could prevent early inter- active vicious circles.',\n    'Parents of at-risk infants try to compensate for the lack of interactivity of their child by modifying their stimulation and therefore sometimes interactions24.',\n    'Early reinforcing identiﬁcation of these interactive targets is especially useful among babies with neurological comorbidities because delays and impairments in early social interactions are not sufﬁ- cient to predict ASD.Similarly, we believe that the importance of HM in distinguishing WS vs. TD on one hand, and WS+ vs. WS− on the other hand, is also in line with the studies that investigated the importance of non-social behaviors for investigating the risk of ASD or NDD during the ﬁrst year of life.',\n    'For example, studying home movies, Purpura et al.',\n    'found more bilateral HM and ﬁnger movements in infants who will later develop ASD31.',\n    'Similarly, several prospective follow-up studies of high-risk siblings32–35 or retrospective studies on home movies36,37 reported spe- ciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been assessed with automatized and computational procedures, focusing on eye tracking at early stages38–40, vocal pro- ductions41, analysis of acoustics of ﬁrst utterances or cry episodes42, but none was done in an interactive setting.',\n    'Our study proposed a paradigm shift from the assessment of infant behavior to dyadic assessment of interactions, as previously achieved in retrospective approaches using home movies24.',\n    'The aim is not to implement studies of social signal processing in routine clinical work but rather to decompose clinical intuitions and signs and validate the most relevant cues of these clinical features.',\n    'From clinical work, back to clinics, social signal processing is a rigorous step to help clinicians better identify and assess early targets of interventions.',\n    'Given the exploratory nature of both our approach and method, our results should be interpreted with caution taking into account strengths (prospective follow-up, automatized multimodal social signal processing, and ecological standardized assessment) and limitations.',\n    'These limitations include (1) the overall sample size knowing that WS is a rare disease; (2) the high rate of missing data during video recording due to the ecological conditions of the infant–mother interaction (mothers interposing between the camera and the infant); the ﬁnal sample size of WS+ (N = 10) that limited the power of machine learning methods.',\n    'We conclude that the method proposed here combining multimodal automatized assessment of social signal pro- cessing during early interaction with infants at risk for NDD is a promising tool to decipher clinical features that remain difﬁcult to identify and assess.',\n    'In the context of WS, we showed that such a method we proposed to label ‘behavioral and interaction imaging’ was able to sig- niﬁcantly predict the development of ASD or ID at 4 years of age in high-risk children who had WS and were assessed at 9 months of age.',\n    'Acknowledgements The authors thank all of the patients and families who participated in this study.',\n    'The study was funded by the EADS foundation (PILE), by the Agence Nationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de Recherche en Psychiatrie (GDR-3557).',\n    'It was partially performed in the Labex SMART (ANR-11-LABX-65), which is supported by French state funds and managed by the ANR in the Investissements d’Avenir program under reference ANR-11-IDEX-0004-02.',\n    'The sponsors had no involvement in the study design, data analysis, or interpretation of the results.',\n    'Author details 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France.',\n    '2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France.',\n    '3Département de Psychiatrie de l’Enfant et de l’Adolescent, AP-HP, Hôpital Pitié-Salpêtrière, 47-83, Boulevard de l’Hôpital, 75651 Paris, Cedex 13, France.',\n    '4Ariana Pharmaceuticals, Research Department, Paris, France.',\n    '5Service de Neuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris, France'],\n   'page_sentence_count_spacy': 33},\n  {'page_number': 7,\n   'text': 'Conﬂict of interest The authors declare that they have no conﬂict of interest.Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.Supplementary Information accompanies this paper at (https://doi.org/ 10.1038/s41398-020-0743-8).Received: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January 2020Published online: 03 February 2020',\n   'sentences': ['Conﬂict of interest The authors declare that they have no conﬂict of interest.',\n    'Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.',\n    'Supplementary Information accompanies this paper at (https://doi.org/ 10.1038/s41398-020-0743-8).Received: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January 2020Published online: 03 February 2020'],\n   'page_sentence_count_spacy': 3}]}"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Chunking our sentences together**","metadata":{}},{"cell_type":"code","source":"# Define split size to turn groups of sentences into chunks\nnum_sentence_chunk_size = 10\n\n# Create a function that recursively splits a list into desired sizes\ndef split_list(input_list: list,\n               slice_size: int)-> list[list[str]]:\n    return [input_list[i:i + slice_size] for i in range(0, len(input_list),slice_size)]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:22:26.866295Z","iopub.execute_input":"2024-05-01T07:22:26.866692Z","iopub.status.idle":"2024-05-01T07:22:26.874910Z","shell.execute_reply.started":"2024-05-01T07:22:26.866665Z","shell.execute_reply":"2024-05-01T07:22:26.873196Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Loop through pages and texts and split sentences into chunks\nfor pdf in loaded_pdf_and_text:\n    for page in pdf['text']:\n        page[\"sentence_chunks\"] = split_list(input_list=page[\"sentences\"],\n                                             slice_size=num_sentence_chunk_size)\n        page[\"num_chunks\"] = len(page[\"sentence_chunks\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:23:06.266227Z","iopub.execute_input":"2024-05-01T07:23:06.266752Z","iopub.status.idle":"2024-05-01T07:23:06.277419Z","shell.execute_reply.started":"2024-05-01T07:23:06.266715Z","shell.execute_reply":"2024-05-01T07:23:06.275487Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"loaded_pdf_and_text[5]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:23:07.944920Z","iopub.execute_input":"2024-05-01T07:23:07.945402Z","iopub.status.idle":"2024-05-01T07:23:07.965938Z","shell.execute_reply.started":"2024-05-01T07:23:07.945368Z","shell.execute_reply":"2024-05-01T07:23:07.964677Z"},"scrolled":true,"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'filename': '22_Ouss_ASD.pdf',\n 'text': [{'page_number': 1,\n   'text': 'A R T I C L EO p e n A c c e s sBehavior and interaction imaging at 9 months of age predict autism/intellectual disability in high-risk infants with West syndrome Lisa Ouss1, Giuseppe Palestra 2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4, Hugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1, Bernard Golse1, Rima Nabbout5, Isabelle Desguerre5, Mariana Guergova-Kuras4 and David Cohen 2,3; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1Abstract Automated behavior analysis are promising tools to overcome current assessment limitations in psychiatry. At 9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during a standardized mother–infant interaction. We computed infant hand movements (HM), speech turn taking of both partners (vocalization, pause, silences, overlap) and motherese. Then, we assessed whether multimodal social signals and interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectual disability (ID)) of infants with WS at 4 years. At follow-up, 10 infants developed ASD/ID (WS+). The best machine learning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS+ vs. WS−. The 10 best features to distinguish WS+ and WS− included a combination of infant vocalizations and HM features combined with synchrony vocalization features. These data indicate that behavioral and interaction imaging was able to predict ASD/ ID in high-risk children with WS.IntroductionBehavior and interaction imaging is a promising domain of affective computing to explore psychiatric conditions1–3. researchers have Regarding attempted to identify reliable indicators of neurodevelop- mental disorders (NDD) in high-risk populations (e.g., sib- lings of children with autism) during the ﬁrst year of life to recommend early interventions4,5. However, social signals and any alterations of them are very difﬁcult to identify at such a young age6. In addition, exploring the quality and dynamics of early interactions is a complex endeavor. It usually requires (i) the perception and integration of mul- timodal social signals and (ii) an understanding of how twoCorrespondence: Lisa Ouss (lisa.ouss@aphp.fr) or David Cohen (david.cohen@aphp.fr) 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France Full list of author information is available at the end of the articleinteractive partners synchronize and proceed in turn taking7,8.Affective computing offers the possibility to simulta- neously analyze the interaction of several partners while considering the multimodal nature and dynamics of social signals and behaviors9. To date, few seminal studies have attempted to social mother–infant interactions with or without a speciﬁc condition, and these studies have focused on speech turns (e.g., Jaffe et al.10), motherese11, head movements12, hand movements13, movement facial expressions3.Here, we focused on West syndrome (WS), a rare epi- leptic encephalopathy with early onset (before age 1 year) and a high risk of NDD outcomes, including one-third of WS children showing later autism spectrum disorder (ASD) and/or intellectual disability (ID). We recruited 32 infants with WS and 19 typically developing (TD) controls to participate in a standardized early mother–infant© The Author(s) 2020',\n   'sentences': ['A R T I C L EO p e n A c c e s sBehavior and interaction imaging at 9 months of age predict autism/intellectual disability in high-risk infants with West syndrome Lisa Ouss1, Giuseppe Palestra 2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4, Hugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1, Bernard Golse1, Rima Nabbout5, Isabelle Desguerre5, Mariana Guergova-Kuras4 and David Cohen 2,3; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1Abstract Automated behavior analysis are promising tools to overcome current assessment limitations in psychiatry.',\n    'At 9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during a standardized mother–infant interaction.',\n    'We computed infant hand movements (HM), speech turn taking of both partners (vocalization, pause, silences, overlap) and motherese.',\n    'Then, we assessed whether multimodal social signals and interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectual disability (ID)) of infants with WS at 4 years.',\n    'At follow-up, 10 infants developed ASD/ID (WS+).',\n    'The best machine learning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS+ vs. WS−. The 10 best features to distinguish WS+ and WS− included a combination of infant vocalizations and HM features combined with synchrony vocalization features.',\n    'These data indicate that behavioral and interaction imaging was able to predict ASD/ ID in high-risk children with WS.IntroductionBehavior and interaction imaging is a promising domain of affective computing to explore psychiatric conditions1–3.',\n    'researchers have Regarding attempted to identify reliable indicators of neurodevelop- mental disorders (NDD) in high-risk populations (e.g., sib- lings of children with autism) during the ﬁrst year of life to recommend early interventions4,5.',\n    'However, social signals and any alterations of them are very difﬁcult to identify at such a young age6.',\n    'In addition, exploring the quality and dynamics of early interactions is a complex endeavor.',\n    'It usually requires (i) the perception and integration of mul- timodal social signals and (ii) an understanding of how twoCorrespondence: Lisa Ouss (lisa.ouss@aphp.fr) or David Cohen (david.cohen@aphp.fr) 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France Full list of author information is available at the end of the articleinteractive partners synchronize and proceed in turn taking7,8.Affective computing offers the possibility to simulta- neously analyze the interaction of several partners while considering the multimodal nature and dynamics of social signals and behaviors9.',\n    'To date, few seminal studies have attempted to social mother–infant interactions with or without a speciﬁc condition, and these studies have focused on speech turns (e.g., Jaffe et al.10), motherese11, head movements12, hand movements13, movement facial expressions3.Here, we focused on West syndrome (WS), a rare epi- leptic encephalopathy with early onset (before age 1 year) and a high risk of NDD outcomes, including one-third of WS children showing later autism spectrum disorder (ASD) and/or intellectual disability (ID).',\n    'We recruited 32 infants with WS and 19 typically developing (TD) controls to participate in a standardized early mother–infant© The Author(s) 2020'],\n   'page_sentence_count_spacy': 13,\n   'sentence_chunks': [['A R T I C L EO p e n A c c e s sBehavior and interaction imaging at 9 months of age predict autism/intellectual disability in high-risk infants with West syndrome Lisa Ouss1, Giuseppe Palestra 2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4, Hugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1, Bernard Golse1, Rima Nabbout5, Isabelle Desguerre5, Mariana Guergova-Kuras4 and David Cohen 2,3; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1; , : ) ( 0 9 8 7 6 5 4 3 2 1Abstract Automated behavior analysis are promising tools to overcome current assessment limitations in psychiatry.',\n     'At 9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during a standardized mother–infant interaction.',\n     'We computed infant hand movements (HM), speech turn taking of both partners (vocalization, pause, silences, overlap) and motherese.',\n     'Then, we assessed whether multimodal social signals and interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectual disability (ID)) of infants with WS at 4 years.',\n     'At follow-up, 10 infants developed ASD/ID (WS+).',\n     'The best machine learning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS+ vs. WS−. The 10 best features to distinguish WS+ and WS− included a combination of infant vocalizations and HM features combined with synchrony vocalization features.',\n     'These data indicate that behavioral and interaction imaging was able to predict ASD/ ID in high-risk children with WS.IntroductionBehavior and interaction imaging is a promising domain of affective computing to explore psychiatric conditions1–3.',\n     'researchers have Regarding attempted to identify reliable indicators of neurodevelop- mental disorders (NDD) in high-risk populations (e.g., sib- lings of children with autism) during the ﬁrst year of life to recommend early interventions4,5.',\n     'However, social signals and any alterations of them are very difﬁcult to identify at such a young age6.',\n     'In addition, exploring the quality and dynamics of early interactions is a complex endeavor.'],\n    ['It usually requires (i) the perception and integration of mul- timodal social signals and (ii) an understanding of how twoCorrespondence: Lisa Ouss (lisa.ouss@aphp.fr) or David Cohen (david.cohen@aphp.fr) 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France Full list of author information is available at the end of the articleinteractive partners synchronize and proceed in turn taking7,8.Affective computing offers the possibility to simulta- neously analyze the interaction of several partners while considering the multimodal nature and dynamics of social signals and behaviors9.',\n     'To date, few seminal studies have attempted to social mother–infant interactions with or without a speciﬁc condition, and these studies have focused on speech turns (e.g., Jaffe et al.10), motherese11, head movements12, hand movements13, movement facial expressions3.Here, we focused on West syndrome (WS), a rare epi- leptic encephalopathy with early onset (before age 1 year) and a high risk of NDD outcomes, including one-third of WS children showing later autism spectrum disorder (ASD) and/or intellectual disability (ID).',\n     'We recruited 32 infants with WS and 19 typically developing (TD) controls to participate in a standardized early mother–infant© The Author(s) 2020']],\n   'num_chunks': 2},\n  {'page_number': 2,\n   'text': 'interaction protocol and followed infants with WS to assess outcomes at 4 years of age. We aim to explore whether multimodal social signals and interpersonal infant–mother interactions at 9 months synchrony of could predict outcomes.Materials and methods Design, participants, and clinical measuresWe performed a prospective follow-up study of infants with WS14. The Institutional Review Board (Comité de Protection des Personnes from the Groupe-Hospitalier Necker Enfants Malades) approved the study, and both parents gave written informed consent after they received verbal and written information on the study. They were asked to participate to a follow-up study to assess out- come of WS taking into account development, early interaction, genetics and response to pharmacological treatment14. The study was conducted from November 2004 to March 2010 in the Neuro-pediatrics Department Center for Rare Epilepsia of Necker Enfants-Malades Hospital, Paris. Of the 41 patients screened during the study period, we enrolled all but two cases (N = 39) with WS. Seven patients dropped out before the age of 3 leading to a sample of 32 patients with detailed follow-up data. Typical developing infants (N = 19) were recruited from Maternal and Infant Prevention institutions, in pediatric consultations, or by proxy.To assess neurodevelopmental outcomes, we focused on ID and ASD. ID was assessed through the Brunet-Lézine Developmental Examination, performed for all children at the age of 3 years. The Brunet-Lézine Developmental Examination estimates a developmental quotient (DQ) based upon normative data available for 3-year-old French toddlers15. The diagnosis of autism was based upon several measurements and an expert assessment that was blind to other variables: (i) At 3 years of age, all parents completed the Autism Diagnostic Interview- Revised (ADI-R) to assess autism signs by dimensions and developmental delay16. (ii) At 2 and 3 years of age, all patients were assessed with the Children’s Autism Rating Scale (CARS)17. (iii) An expert clinician (LR) who was blind to child history assessed autism and ID from 20-min videotapes of child/mother play at 2 years of age. Finally, diagnoses of ASD and/or ID at age 4 were based upon a consensus approach using direct assessment of the child by a clinician with expertise in autism (LO) as well as by clinical information from the CARS, ADI-R, and DQ.Video recordingsInfant–mother interactions were assessed between 9 and 12 months of age during a play session (Fig. 1). Two synchronized cameras (face and proﬁle; Fig. S1A) recor- ded the movements in two dimensions while the infant was sitting in a baby chair. Audio interactions were alsoAround 9 months —_—————— At4years WS with Lab recording of infant-mother 10) - = ID/ASD interaction Audio features extraction vs. WS Infant: typical vocalization, without fifant atypical vocalization, pause WS with Motheﬁ ID/ASD (N ID/ASD 32) Mother: vocalization, = motherese, pause (N Synchrony: overlap, silence, infant synchrony ratio 22) = = West syndrome chidlren Machine 3 sequences of interaction learning WS without classification ID/ASD (N Free interaction 19) = = 19) Interaction = = — with the giraffe West syndrome controls (N: Typical developing Mother singing controls (N: vs. TD Typical developing Fig. 1 Pipeline of our machine learning approach to classify WS vs. TD.Infant  Mother Fig. 1 Pipeline of our machine learning approach to classify WS vs. TD.',\n   'sentences': ['interaction protocol and followed infants with WS to assess outcomes at 4 years of age.',\n    'We aim to explore whether multimodal social signals and interpersonal infant–mother interactions at 9 months synchrony of could predict outcomes.',\n    'Materials and methods Design, participants, and clinical measuresWe performed a prospective follow-up study of infants with WS14.',\n    'The Institutional Review Board (Comité de Protection des Personnes from the Groupe-Hospitalier Necker Enfants Malades) approved the study, and both parents gave written informed consent after they received verbal and written information on the study.',\n    'They were asked to participate to a follow-up study to assess out- come of WS taking into account development, early interaction, genetics and response to pharmacological treatment14.',\n    'The study was conducted from November 2004 to March 2010 in the Neuro-pediatrics Department Center for Rare Epilepsia of Necker Enfants-Malades Hospital, Paris.',\n    'Of the 41 patients screened during the study period, we enrolled all but two cases (N = 39) with WS.',\n    'Seven patients dropped out before the age of 3 leading to a sample of 32 patients with detailed follow-up data.',\n    'Typical developing infants (N = 19) were recruited from Maternal and Infant Prevention institutions, in pediatric consultations, or by proxy.',\n    'To assess neurodevelopmental outcomes, we focused on ID and ASD.',\n    'ID was assessed through the Brunet-Lézine Developmental Examination, performed for all children at the age of 3 years.',\n    'The Brunet-Lézine Developmental Examination estimates a developmental quotient (DQ) based upon normative data available for 3-year-old French toddlers15.',\n    'The diagnosis of autism was based upon several measurements and an expert assessment that was blind to other variables: (i) At 3 years of age, all parents completed the Autism Diagnostic Interview- Revised (ADI-R) to assess autism signs by dimensions and developmental delay16. (',\n    'ii) At 2 and 3 years of age, all patients were assessed with the Children’s Autism Rating Scale (CARS)17. (',\n    'iii) An expert clinician (LR) who was blind to child history assessed autism and ID from 20-min videotapes of child/mother play at 2 years of age.',\n    'Finally, diagnoses of ASD and/or ID at age 4 were based upon a consensus approach using direct assessment of the child by a clinician with expertise in autism (LO) as well as by clinical information from the CARS, ADI-R, and DQ.Video recordingsInfant–mother interactions were assessed between 9 and 12 months of age during a play session (Fig.',\n    '1).',\n    'Two synchronized cameras (face and proﬁle; Fig.',\n    'S1A) recor- ded the movements in two dimensions while the infant was sitting in a baby chair.',\n    'Audio interactions were alsoAround 9 months —_—————— At4years WS with Lab recording of infant-mother 10) - = ID/ASD interaction Audio features extraction vs. WS Infant: typical vocalization, without fifant atypical vocalization, pause WS with Motheﬁ ID/ASD (N ID/ASD 32) Mother: vocalization, = motherese, pause (N Synchrony: overlap, silence, infant synchrony ratio 22) = = West syndrome chidlren Machine 3 sequences of interaction learning WS without classification ID/ASD (N Free interaction 19) = = 19) Interaction = = — with the giraffe West syndrome controls (N: Typical developing Mother singing controls (N: vs. TD Typical developing Fig.',\n    '1 Pipeline of our machine learning approach to classify WS vs. TD.Infant  Mother Fig.',\n    '1 Pipeline of our machine learning approach to classify WS vs. TD.'],\n   'page_sentence_count_spacy': 22,\n   'sentence_chunks': [['interaction protocol and followed infants with WS to assess outcomes at 4 years of age.',\n     'We aim to explore whether multimodal social signals and interpersonal infant–mother interactions at 9 months synchrony of could predict outcomes.',\n     'Materials and methods Design, participants, and clinical measuresWe performed a prospective follow-up study of infants with WS14.',\n     'The Institutional Review Board (Comité de Protection des Personnes from the Groupe-Hospitalier Necker Enfants Malades) approved the study, and both parents gave written informed consent after they received verbal and written information on the study.',\n     'They were asked to participate to a follow-up study to assess out- come of WS taking into account development, early interaction, genetics and response to pharmacological treatment14.',\n     'The study was conducted from November 2004 to March 2010 in the Neuro-pediatrics Department Center for Rare Epilepsia of Necker Enfants-Malades Hospital, Paris.',\n     'Of the 41 patients screened during the study period, we enrolled all but two cases (N = 39) with WS.',\n     'Seven patients dropped out before the age of 3 leading to a sample of 32 patients with detailed follow-up data.',\n     'Typical developing infants (N = 19) were recruited from Maternal and Infant Prevention institutions, in pediatric consultations, or by proxy.',\n     'To assess neurodevelopmental outcomes, we focused on ID and ASD.'],\n    ['ID was assessed through the Brunet-Lézine Developmental Examination, performed for all children at the age of 3 years.',\n     'The Brunet-Lézine Developmental Examination estimates a developmental quotient (DQ) based upon normative data available for 3-year-old French toddlers15.',\n     'The diagnosis of autism was based upon several measurements and an expert assessment that was blind to other variables: (i) At 3 years of age, all parents completed the Autism Diagnostic Interview- Revised (ADI-R) to assess autism signs by dimensions and developmental delay16. (',\n     'ii) At 2 and 3 years of age, all patients were assessed with the Children’s Autism Rating Scale (CARS)17. (',\n     'iii) An expert clinician (LR) who was blind to child history assessed autism and ID from 20-min videotapes of child/mother play at 2 years of age.',\n     'Finally, diagnoses of ASD and/or ID at age 4 were based upon a consensus approach using direct assessment of the child by a clinician with expertise in autism (LO) as well as by clinical information from the CARS, ADI-R, and DQ.Video recordingsInfant–mother interactions were assessed between 9 and 12 months of age during a play session (Fig.',\n     '1).',\n     'Two synchronized cameras (face and proﬁle; Fig.',\n     'S1A) recor- ded the movements in two dimensions while the infant was sitting in a baby chair.',\n     'Audio interactions were alsoAround 9 months —_—————— At4years WS with Lab recording of infant-mother 10) - = ID/ASD interaction Audio features extraction vs. WS Infant: typical vocalization, without fifant atypical vocalization, pause WS with Motheﬁ ID/ASD (N ID/ASD 32) Mother: vocalization, = motherese, pause (N Synchrony: overlap, silence, infant synchrony ratio 22) = = West syndrome chidlren Machine 3 sequences of interaction learning WS without classification ID/ASD (N Free interaction 19) = = 19) Interaction = = — with the giraffe West syndrome controls (N: Typical developing Mother singing controls (N: vs. TD Typical developing Fig.'],\n    ['1 Pipeline of our machine learning approach to classify WS vs. TD.Infant  Mother Fig.',\n     '1 Pipeline of our machine learning approach to classify WS vs. TD.']],\n   'num_chunks': 3},\n  {'page_number': 3,\n   'text': 'recorded. The standardized situation encompassed three sequences of 3 min: (sequence 1) free play after instruct- ing the mother to interact “as usual” without any toy; (sequence 2) free play using the help of a toy (Sophie the giraffe); (sequence 3) mother singing to her baby. Due to the position of the baby chair on the ﬂoor and the mother’s seated position, the mother was positioned slightly higher in all of the recordings. The mother’s indicated position was on the left of the child as shown on the picture, but exceptions were sometimes observed during the recordings. For infant hand movement (HM) features, 1 min was extracted from each 3-min video and all recordings, according to two criteria: the child’s hands should be visible for at least part of the sequence (e.g., the mother is not leaning on the child), and the minute represented the greatest amount of interaction between the mother and the child. For audio and speech turn- taking computing, we only used the 3-min audio recording of sequence 1.Vision computing (Fig. S1B, vision computing panel)To process infant hand movements (HM), we used the methods developed in Ouss et al.13. Here, we summarize the successive steps to calculate HM features. In step 1 (hand trajectory extraction and data processing), the two- dimensional coordinates of the hand were extracted from each of the video recordings by tracking a wristband on the right hand (yellow in Fig. S1A, video-audio recording panel). The tracking framework comprised three steps: prediction, observation, and estimation as proposed in ref. 18. As the hand motion was highly nonlinear, we developed an approach using a bootstrap-based particle ﬁlter with a ﬁrst-order model to address abrupt changes in direction and speed19,20. To address hand occlusion, we implemented an approach combining tracking with detection by adding a boolean variable to the state vector associated with each particle18.Each extracted trajectory consisted of 1500 pairs of x and y coordinates (25 frames per second, generating 1500 pairs of coordinates in the 60 s; see Fig. S1 left panel, vision computing). The frames where the hand was not visible were clearly indicated in each trajectory as missing coordinates for these time points. To account for differ- ences in the camera zoom parameters, the trajectories obtained were normalized using a ﬁxed reference system present in the settings of each video recording. The nor- malization was performed on all trajectories, and 95% of the normalization factors ranged between 0.8 and 1.22 with a few outlier trajectories that required greater cor- rection. Forty-one percent of the trajectories required <5% correction. Although the recordings between the two cameras were synchronized and in principle allowed 3D reconstruction of the trajectory, the accumulation of missing data prevented such reconstruction. However, 2Dmotion capture with appropriately deﬁned movement descriptors can be powerful for detecting clinically rele- vant changes21, thereby justifying the independent ana- lysis of the 2D-trajectory videos (see Fig. S1B, vision computing, 2d panel on the left).In step 2, the descriptors of the HM were calculated from the planar trajectories (Fig. S1B, table shown in the vision computing panel). Descriptors covered those already reported in the literature as important in char- acterizing infants’ HM21. (1) To describe the space explored by the hand, we calculated the maximum dis- tance observed on the two axes (xRange, yRange) and the standard deviation of the X and Y coordinates observed during the 60 s (xSd, ySd). We also calculated the max- imum distance between any two points of the trajectory using (http://algs4.cs. java princeton.edu/code/) (Fig. S1B, vision computing panel, red line in the third panel from the left). (2) To evaluate HM dynamics, we calculated the velocity and accelera- tion. (3) Also related to HM dynamics, we calculated HM pauses deﬁned as part of the trajectory in which the velocity was lower than a speciﬁc threshold for a mini- mum duration of 4 s. (4) Finally, the curvature of the trajectories was calculated using a standard deﬁnition of the curvature (κ) of plane curves in Cartesian coordinates as γ(t) = (x(t), y(t)). The curvature calculated at each point of the trajectory is presented in the right panel of Fig. S1B (video computing), where the ﬁrst 1.2 s of the trajectory are plotted and the associated calculated curvatures at each point (and respective time, indicated on the axis) are presented as columns.Audio computing (Fig. S1C, audio computing)We extracted two types of audio social signals from the audio channel of the mother–infant interaction: speech turn taking (STT) and motherese. For STT extraction, we followed the methods developed by Weisman et al.22 and Bourvis et al.23 (Fig. S1, audio computing). First, we used ELAN to segment the infants’ and mothers’ speech turns and annotate the dialog acts. Mothers’ audio interactions were categorized as mother vocalization (meaningful laugh, singing, animal sounds) or other vocalizations, noise (clap hands, snap ﬁngers or snap the tongue, mouth infants’ audio production was noise, etc.). Similarly, deﬁned as infant vocalization (babbling vocalizations, laugh, and cry) or atypical vocalization (other noise such as “rale”). The infant’s and mother’s utterances were labeled by two annotators (blind to group status). Cohen’s kappa between the two annotators was calculated for each dyad, each task and each item of the grid. For all items, the kappa values were between 0.82 and 1.From the annotation, we extracted all the speech turns of the infant and the mother. A speech turn is a con- tinuous stream of speech with <150 ms of silence. We',\n   'sentences': ['recorded.',\n    'The standardized situation encompassed three sequences of 3 min: (sequence 1) free play after instruct- ing the mother to interact “as usual” without any toy; (sequence 2) free play using the help of a toy (Sophie the giraffe); (sequence 3) mother singing to her baby.',\n    'Due to the position of the baby chair on the ﬂoor and the mother’s seated position, the mother was positioned slightly higher in all of the recordings.',\n    'The mother’s indicated position was on the left of the child as shown on the picture, but exceptions were sometimes observed during the recordings.',\n    'For infant hand movement (HM) features, 1 min was extracted from each 3-min video and all recordings, according to two criteria: the child’s hands should be visible for at least part of the sequence (e.g., the mother is not leaning on the child), and the minute represented the greatest amount of interaction between the mother and the child.',\n    'For audio and speech turn- taking computing, we only used the 3-min audio recording of sequence 1.Vision computing (Fig.',\n    'S1B, vision computing panel)To process infant hand movements (HM), we used the methods developed in Ouss et al.13.',\n    'Here, we summarize the successive steps to calculate HM features.',\n    'In step 1 (hand trajectory extraction and data processing), the two- dimensional coordinates of the hand were extracted from each of the video recordings by tracking a wristband on the right hand (yellow in Fig.',\n    'S1A, video-audio recording panel).',\n    'The tracking framework comprised three steps: prediction, observation, and estimation as proposed in ref.',\n    '18.',\n    'As the hand motion was highly nonlinear, we developed an approach using a bootstrap-based particle ﬁlter with a ﬁrst-order model to address abrupt changes in direction and speed19,20.',\n    'To address hand occlusion, we implemented an approach combining tracking with detection by adding a boolean variable to the state vector associated with each particle18.Each extracted trajectory consisted of 1500 pairs of x and y coordinates (25 frames per second, generating 1500 pairs of coordinates in the 60 s; see Fig.',\n    'S1 left panel, vision computing).',\n    'The frames where the hand was not visible were clearly indicated in each trajectory as missing coordinates for these time points.',\n    'To account for differ- ences in the camera zoom parameters, the trajectories obtained were normalized using a ﬁxed reference system present in the settings of each video recording.',\n    'The nor- malization was performed on all trajectories, and 95% of the normalization factors ranged between 0.8 and 1.22 with a few outlier trajectories that required greater cor- rection.',\n    'Forty-one percent of the trajectories required <5% correction.',\n    'Although the recordings between the two cameras were synchronized and in principle allowed 3D reconstruction of the trajectory, the accumulation of missing data prevented such reconstruction.',\n    'However, 2Dmotion capture with appropriately deﬁned movement descriptors can be powerful for detecting clinically rele- vant changes21, thereby justifying the independent ana- lysis of the 2D-trajectory videos (see Fig.',\n    'S1B, vision computing, 2d panel on the left).In step 2, the descriptors of the HM were calculated from the planar trajectories (Fig.',\n    'S1B, table shown in the vision computing panel).',\n    'Descriptors covered those already reported in the literature as important in char- acterizing infants’ HM21. (',\n    '1) To describe the space explored by the hand, we calculated the maximum dis- tance observed on the two axes (xRange, yRange) and the standard deviation of the X and Y coordinates observed during the 60 s (xSd, ySd).',\n    'We also calculated the max- imum distance between any two points of the trajectory using (http://algs4.cs.',\n    'java princeton.edu/code/) (Fig.',\n    'S1B, vision computing panel, red line in the third panel from the left). (',\n    '2) To evaluate HM dynamics, we calculated the velocity and accelera- tion. (',\n    '3) Also related to HM dynamics, we calculated HM pauses deﬁned as part of the trajectory in which the velocity was lower than a speciﬁc threshold for a mini- mum duration of 4 s. (4) Finally, the curvature of the trajectories was calculated using a standard deﬁnition of the curvature (κ) of plane curves in Cartesian coordinates as γ(t) = (x(t), y(t)).',\n    'The curvature calculated at each point of the trajectory is presented in the right panel of Fig.',\n    'S1B (video computing), where the ﬁrst 1.2 s of the trajectory are plotted and the associated calculated curvatures at each point (and respective time, indicated on the axis) are presented as columns.',\n    'Audio computing (Fig.',\n    'S1C, audio computing)We extracted two types of audio social signals from the audio channel of the mother–infant interaction: speech turn taking (STT) and motherese.',\n    'For STT extraction, we followed the methods developed by Weisman et al.22 and Bourvis et al.23 (Fig.',\n    'S1, audio computing).',\n    'First, we used ELAN to segment the infants’ and mothers’ speech turns and annotate the dialog acts.',\n    'Mothers’ audio interactions were categorized as mother vocalization (meaningful laugh, singing, animal sounds) or other vocalizations, noise (clap hands, snap ﬁngers or snap the tongue, mouth infants’ audio production was noise, etc.).',\n    'Similarly, deﬁned as infant vocalization (babbling vocalizations, laugh, and cry) or atypical vocalization (other noise such as “rale”).',\n    'The infant’s and mother’s utterances were labeled by two annotators (blind to group status).',\n    'Cohen’s kappa between the two annotators was calculated for each dyad, each task and each item of the grid.',\n    'For all items, the kappa values were between 0.82 and 1.From the annotation, we extracted all the speech turns of the infant and the mother.',\n    'A speech turn is a con- tinuous stream of speech with <150 ms of silence.',\n    'We'],\n   'page_sentence_count_spacy': 44,\n   'sentence_chunks': [['recorded.',\n     'The standardized situation encompassed three sequences of 3 min: (sequence 1) free play after instruct- ing the mother to interact “as usual” without any toy; (sequence 2) free play using the help of a toy (Sophie the giraffe); (sequence 3) mother singing to her baby.',\n     'Due to the position of the baby chair on the ﬂoor and the mother’s seated position, the mother was positioned slightly higher in all of the recordings.',\n     'The mother’s indicated position was on the left of the child as shown on the picture, but exceptions were sometimes observed during the recordings.',\n     'For infant hand movement (HM) features, 1 min was extracted from each 3-min video and all recordings, according to two criteria: the child’s hands should be visible for at least part of the sequence (e.g., the mother is not leaning on the child), and the minute represented the greatest amount of interaction between the mother and the child.',\n     'For audio and speech turn- taking computing, we only used the 3-min audio recording of sequence 1.Vision computing (Fig.',\n     'S1B, vision computing panel)To process infant hand movements (HM), we used the methods developed in Ouss et al.13.',\n     'Here, we summarize the successive steps to calculate HM features.',\n     'In step 1 (hand trajectory extraction and data processing), the two- dimensional coordinates of the hand were extracted from each of the video recordings by tracking a wristband on the right hand (yellow in Fig.',\n     'S1A, video-audio recording panel).'],\n    ['The tracking framework comprised three steps: prediction, observation, and estimation as proposed in ref.',\n     '18.',\n     'As the hand motion was highly nonlinear, we developed an approach using a bootstrap-based particle ﬁlter with a ﬁrst-order model to address abrupt changes in direction and speed19,20.',\n     'To address hand occlusion, we implemented an approach combining tracking with detection by adding a boolean variable to the state vector associated with each particle18.Each extracted trajectory consisted of 1500 pairs of x and y coordinates (25 frames per second, generating 1500 pairs of coordinates in the 60 s; see Fig.',\n     'S1 left panel, vision computing).',\n     'The frames where the hand was not visible were clearly indicated in each trajectory as missing coordinates for these time points.',\n     'To account for differ- ences in the camera zoom parameters, the trajectories obtained were normalized using a ﬁxed reference system present in the settings of each video recording.',\n     'The nor- malization was performed on all trajectories, and 95% of the normalization factors ranged between 0.8 and 1.22 with a few outlier trajectories that required greater cor- rection.',\n     'Forty-one percent of the trajectories required <5% correction.',\n     'Although the recordings between the two cameras were synchronized and in principle allowed 3D reconstruction of the trajectory, the accumulation of missing data prevented such reconstruction.'],\n    ['However, 2Dmotion capture with appropriately deﬁned movement descriptors can be powerful for detecting clinically rele- vant changes21, thereby justifying the independent ana- lysis of the 2D-trajectory videos (see Fig.',\n     'S1B, vision computing, 2d panel on the left).In step 2, the descriptors of the HM were calculated from the planar trajectories (Fig.',\n     'S1B, table shown in the vision computing panel).',\n     'Descriptors covered those already reported in the literature as important in char- acterizing infants’ HM21. (',\n     '1) To describe the space explored by the hand, we calculated the maximum dis- tance observed on the two axes (xRange, yRange) and the standard deviation of the X and Y coordinates observed during the 60 s (xSd, ySd).',\n     'We also calculated the max- imum distance between any two points of the trajectory using (http://algs4.cs.',\n     'java princeton.edu/code/) (Fig.',\n     'S1B, vision computing panel, red line in the third panel from the left). (',\n     '2) To evaluate HM dynamics, we calculated the velocity and accelera- tion. (',\n     '3) Also related to HM dynamics, we calculated HM pauses deﬁned as part of the trajectory in which the velocity was lower than a speciﬁc threshold for a mini- mum duration of 4 s. (4) Finally, the curvature of the trajectories was calculated using a standard deﬁnition of the curvature (κ) of plane curves in Cartesian coordinates as γ(t) = (x(t), y(t)).'],\n    ['The curvature calculated at each point of the trajectory is presented in the right panel of Fig.',\n     'S1B (video computing), where the ﬁrst 1.2 s of the trajectory are plotted and the associated calculated curvatures at each point (and respective time, indicated on the axis) are presented as columns.',\n     'Audio computing (Fig.',\n     'S1C, audio computing)We extracted two types of audio social signals from the audio channel of the mother–infant interaction: speech turn taking (STT) and motherese.',\n     'For STT extraction, we followed the methods developed by Weisman et al.22 and Bourvis et al.23 (Fig.',\n     'S1, audio computing).',\n     'First, we used ELAN to segment the infants’ and mothers’ speech turns and annotate the dialog acts.',\n     'Mothers’ audio interactions were categorized as mother vocalization (meaningful laugh, singing, animal sounds) or other vocalizations, noise (clap hands, snap ﬁngers or snap the tongue, mouth infants’ audio production was noise, etc.).',\n     'Similarly, deﬁned as infant vocalization (babbling vocalizations, laugh, and cry) or atypical vocalization (other noise such as “rale”).',\n     'The infant’s and mother’s utterances were labeled by two annotators (blind to group status).'],\n    ['Cohen’s kappa between the two annotators was calculated for each dyad, each task and each item of the grid.',\n     'For all items, the kappa values were between 0.82 and 1.From the annotation, we extracted all the speech turns of the infant and the mother.',\n     'A speech turn is a con- tinuous stream of speech with <150 ms of silence.',\n     'We']],\n   'num_chunks': 5},\n  {'page_number': 4,\n   'text': 'obtained a list of triples: speaker label (infant or mother), start time, and duration of speech turn. From these triples, we also deduced the start time and duration of the time segments when the mother or the infant were not speaking (pauses). Therefore, we extracted Mother Vocalizations; Mother Other Noise; Infant Vocalizations; Infant Infant Atypical Vocalizations; Mother Pauses; Pauses. We also extracted three dyadic features: (1) Silence deﬁned as sequences of time during which neither participant was speaking for more than 150 ms; (2) Overlap Ratio deﬁned as the duration of vocalization overlaps between mothers and infants divided by the duration of the total interaction. This ratio measures the proportion of interactional time in which both partici- pants were simultaneously vocalizing; (3) Infant Syn- chrony Ratio deﬁned as the number of infants’ responses to their mother’s vocalization within a time limit of 3 s divided by the number of mother vocalizations during the time paradigm. The 3-s window was based on the avail- able literature on synchrony7,24.From the mother vocalizations, we also computed affective speech analysis, as previous work has shown that motherese may shape parent-infant interactions25. The segments of mother vocalizations were analyzed using a computerized classiﬁer for categorization as “motherese” or “non-motherese/other speech” initially developed to analyze home movies11. The system exploits the fusion of two classiﬁers, namely, segmental and suprasegmental26. Consequently, the utterances are characterized by both frequency cepstrum coefﬁcients) and segmental suprasegmental/prosodics (e.g., statistics with regard to fundamental frequency, energy, and duration) features. The detector used the GMM (Gaussian mixture model) classiﬁer for both segmental and suprasegmental features (M, number of Gaussians for the GMM Classiﬁer: M = 12 and 15, respectively, and λ = weighting coefﬁcient used in the equation fusion: λ = 0.4). For the purpose of the current study, we explored the performance of our motherese classiﬁer in French mothers. We analyzed 200 sequences from French mothers (100 motherese vs. 100 other speech) that were blindly validated by two psycholinguists. We calculated the Intraclass correlation (ICC) between the two raters (the expert and the algo- rithm) and found a good and very signiﬁcant ICC (ICC = 0.79 (95% CI: 0.59–0.90), p < 0.001). This level of predic- tion made it suitable for further analysis of the entire data set.Based on this automatic detection of motherese, we created two subclasses for mother vocalizations: mother- ese vs. non-motherese. Two variables were derived: Motherese Ratio (duration of motherese vocalization/ duration of interaction) and Non-motherese Ratio (dura- tion of non-motherese vocalization/duration of interac- tion). We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio, which reﬂect the ratio of time during which the infant vocalizes in response to his/her mother motherese and other speech (non-motherese).Prediction of the outcome using machine learningThe pipeline of our approach is shown in Fig. 1. First, a data quality analysis was performed to ensure the validity of the data. As expected, all data were available for audio analysis. However, a substantial proportion of the data were discarded due to video recording or vision com- puting issues. We ﬁnally kept 18 video recordings for the WS and 17 videos for the TD groups. Second, given the number of features (21 infant HM for each camera and each sequence; 16 STT) compared with the data set (32 WS and 19 TD), we reduced our data using principal component analysis (PCA). Third, we tested several algorithms to classify WS vs. TD based on the whole data set available for both vision and audio computing features (leave one out) (Table S1). The best algorithm was deci- sion stump27. All results presented here are based on the classiﬁcation with a decision stump algorithm. We also analyzed WS with ID/ASD (WS+) vs. WS without ID/ ASD (WS−). For each classiﬁcation, we also extracted a confusion matrix and explored which individual features to a given classiﬁcation using contributed the most Pearson correlations.ResultsTable S2 summarizes the demographic and clinical characteristics of children with WS. At follow-up, 10 infants out of 32 children with WS developed ASD/ID (WS+). Eight children had ASD and ID, whereas 2 had only ID. As expected, all variables related to ASD and ID were signiﬁcantly different in WS+ compared with WS−. Figure 2a summarizes the best classiﬁcation models using the decision stump algorithm (leave one out). As shown, multimodal classiﬁcation outperformed unimodal classiﬁcation to distinguish WS and TD. Therefore, we only used the multimodal approach to classify WS+ vs. WS−. The best model reached 76.47% accuracy classify- ing WS vs. TD and 81.25% accuracy classifying WS+ vs. WS− based on multimodal features extracted during early interactions. Interestingly, the confusion matrices (Fig. 2b) show that when classifying WS vs. TD, all errors came from TD being misclassiﬁed as WS (N = 12); when clas- sifying WS+ vs. WS−, most errors came from WS+ being misclassiﬁed as WS− (N = 5).Table 1 lists the best features for each multimodal classiﬁcation based on the Pearson correlation values. The best features to distinguish WS and TD included four infant HM features, 1 mother audio feature. In contrast, the best features to distinguish WS+ and WS− included a (N = 2), combination of',\n   'sentences': ['obtained a list of triples: speaker label (infant or mother), start time, and duration of speech turn.',\n    'From these triples, we also deduced the start time and duration of the time segments when the mother or the infant were not speaking (pauses).',\n    'Therefore, we extracted Mother Vocalizations; Mother Other Noise; Infant Vocalizations; Infant Infant Atypical Vocalizations; Mother Pauses; Pauses.',\n    'We also extracted three dyadic features: (1) Silence deﬁned as sequences of time during which neither participant was speaking for more than 150 ms; (2) Overlap Ratio deﬁned as the duration of vocalization overlaps between mothers and infants divided by the duration of the total interaction.',\n    'This ratio measures the proportion of interactional time in which both partici- pants were simultaneously vocalizing; (3) Infant Syn- chrony Ratio deﬁned as the number of infants’ responses to their mother’s vocalization within a time limit of 3 s divided by the number of mother vocalizations during the time paradigm.',\n    'The 3-s window was based on the avail- able literature on synchrony7,24.From the mother vocalizations, we also computed affective speech analysis, as previous work has shown that motherese may shape parent-infant interactions25.',\n    'The segments of mother vocalizations were analyzed using a computerized classiﬁer for categorization as “motherese” or “non-motherese/other speech” initially developed to analyze home movies11.',\n    'The system exploits the fusion of two classiﬁers, namely, segmental and suprasegmental26.',\n    'Consequently, the utterances are characterized by both frequency cepstrum coefﬁcients) and segmental suprasegmental/prosodics (e.g., statistics with regard to fundamental frequency, energy, and duration) features.',\n    'The detector used the GMM (Gaussian mixture model) classiﬁer for both segmental and suprasegmental features (M, number of Gaussians for the GMM Classiﬁer: M = 12 and 15, respectively, and λ = weighting coefﬁcient used in the equation fusion: λ = 0.4).',\n    'For the purpose of the current study, we explored the performance of our motherese classiﬁer in French mothers.',\n    'We analyzed 200 sequences from French mothers (100 motherese vs. 100 other speech) that were blindly validated by two psycholinguists.',\n    'We calculated the Intraclass correlation (ICC) between the two raters (the expert and the algo- rithm) and found a good and very signiﬁcant ICC (ICC = 0.79 (95% CI: 0.59–0.90), p < 0.001).',\n    'This level of predic- tion made it suitable for further analysis of the entire data set.',\n    'Based on this automatic detection of motherese, we created two subclasses for mother vocalizations: mother- ese vs. non-motherese.',\n    'Two variables were derived: Motherese Ratio (duration of motherese vocalization/ duration of interaction) and Non-motherese Ratio (dura- tion of non-motherese vocalization/duration of interac- tion).',\n    'We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio, which reﬂect the ratio of time during which the infant vocalizes in response to his/her mother motherese and other speech (non-motherese).Prediction of the outcome using machine learningThe pipeline of our approach is shown in Fig.',\n    '1.',\n    'First, a data quality analysis was performed to ensure the validity of the data.',\n    'As expected, all data were available for audio analysis.',\n    'However, a substantial proportion of the data were discarded due to video recording or vision com- puting issues.',\n    'We ﬁnally kept 18 video recordings for the WS and 17 videos for the TD groups.',\n    'Second, given the number of features (21 infant HM for each camera and each sequence; 16 STT) compared with the data set (32 WS and 19 TD), we reduced our data using principal component analysis (PCA).',\n    'Third, we tested several algorithms to classify WS vs. TD based on the whole data set available for both vision and audio computing features (leave one out) (Table S1).',\n    'The best algorithm was deci- sion stump27.',\n    'All results presented here are based on the classiﬁcation with a decision stump algorithm.',\n    'We also analyzed WS with ID/ASD (WS+) vs. WS without ID/ ASD (WS−).',\n    'For each classiﬁcation, we also extracted a confusion matrix and explored which individual features to a given classiﬁcation using contributed the most Pearson correlations.',\n    'ResultsTable S2 summarizes the demographic and clinical characteristics of children with WS.',\n    'At follow-up, 10 infants out of 32 children with WS developed ASD/ID (WS+).',\n    'Eight children had ASD and ID, whereas 2 had only ID.',\n    'As expected, all variables related to ASD and ID were signiﬁcantly different in WS+ compared with WS−. Figure 2a summarizes the best classiﬁcation models using the decision stump algorithm (leave one out).',\n    'As shown, multimodal classiﬁcation outperformed unimodal classiﬁcation to distinguish WS and TD.',\n    'Therefore, we only used the multimodal approach to classify WS+ vs. WS−. The best model reached 76.47% accuracy classify- ing WS vs. TD and 81.25% accuracy classifying WS+ vs. WS− based on multimodal features extracted during early interactions.',\n    'Interestingly, the confusion matrices (Fig.',\n    '2b) show that when classifying WS vs. TD, all errors came from TD being misclassiﬁed as WS (N = 12); when clas- sifying WS+ vs. WS−, most errors came from WS+ being misclassiﬁed as WS− (N = 5).Table 1 lists the best features for each multimodal classiﬁcation based on the Pearson correlation values.',\n    'The best features to distinguish WS and TD included four infant HM features, 1 mother audio feature.',\n    'In contrast, the best features to distinguish WS+ and WS− included a (N = 2), combination of'],\n   'page_sentence_count_spacy': 38,\n   'sentence_chunks': [['obtained a list of triples: speaker label (infant or mother), start time, and duration of speech turn.',\n     'From these triples, we also deduced the start time and duration of the time segments when the mother or the infant were not speaking (pauses).',\n     'Therefore, we extracted Mother Vocalizations; Mother Other Noise; Infant Vocalizations; Infant Infant Atypical Vocalizations; Mother Pauses; Pauses.',\n     'We also extracted three dyadic features: (1) Silence deﬁned as sequences of time during which neither participant was speaking for more than 150 ms; (2) Overlap Ratio deﬁned as the duration of vocalization overlaps between mothers and infants divided by the duration of the total interaction.',\n     'This ratio measures the proportion of interactional time in which both partici- pants were simultaneously vocalizing; (3) Infant Syn- chrony Ratio deﬁned as the number of infants’ responses to their mother’s vocalization within a time limit of 3 s divided by the number of mother vocalizations during the time paradigm.',\n     'The 3-s window was based on the avail- able literature on synchrony7,24.From the mother vocalizations, we also computed affective speech analysis, as previous work has shown that motherese may shape parent-infant interactions25.',\n     'The segments of mother vocalizations were analyzed using a computerized classiﬁer for categorization as “motherese” or “non-motherese/other speech” initially developed to analyze home movies11.',\n     'The system exploits the fusion of two classiﬁers, namely, segmental and suprasegmental26.',\n     'Consequently, the utterances are characterized by both frequency cepstrum coefﬁcients) and segmental suprasegmental/prosodics (e.g., statistics with regard to fundamental frequency, energy, and duration) features.',\n     'The detector used the GMM (Gaussian mixture model) classiﬁer for both segmental and suprasegmental features (M, number of Gaussians for the GMM Classiﬁer: M = 12 and 15, respectively, and λ = weighting coefﬁcient used in the equation fusion: λ = 0.4).'],\n    ['For the purpose of the current study, we explored the performance of our motherese classiﬁer in French mothers.',\n     'We analyzed 200 sequences from French mothers (100 motherese vs. 100 other speech) that were blindly validated by two psycholinguists.',\n     'We calculated the Intraclass correlation (ICC) between the two raters (the expert and the algo- rithm) and found a good and very signiﬁcant ICC (ICC = 0.79 (95% CI: 0.59–0.90), p < 0.001).',\n     'This level of predic- tion made it suitable for further analysis of the entire data set.',\n     'Based on this automatic detection of motherese, we created two subclasses for mother vocalizations: mother- ese vs. non-motherese.',\n     'Two variables were derived: Motherese Ratio (duration of motherese vocalization/ duration of interaction) and Non-motherese Ratio (dura- tion of non-motherese vocalization/duration of interac- tion).',\n     'We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio, which reﬂect the ratio of time during which the infant vocalizes in response to his/her mother motherese and other speech (non-motherese).Prediction of the outcome using machine learningThe pipeline of our approach is shown in Fig.',\n     '1.',\n     'First, a data quality analysis was performed to ensure the validity of the data.',\n     'As expected, all data were available for audio analysis.'],\n    ['However, a substantial proportion of the data were discarded due to video recording or vision com- puting issues.',\n     'We ﬁnally kept 18 video recordings for the WS and 17 videos for the TD groups.',\n     'Second, given the number of features (21 infant HM for each camera and each sequence; 16 STT) compared with the data set (32 WS and 19 TD), we reduced our data using principal component analysis (PCA).',\n     'Third, we tested several algorithms to classify WS vs. TD based on the whole data set available for both vision and audio computing features (leave one out) (Table S1).',\n     'The best algorithm was deci- sion stump27.',\n     'All results presented here are based on the classiﬁcation with a decision stump algorithm.',\n     'We also analyzed WS with ID/ASD (WS+) vs. WS without ID/ ASD (WS−).',\n     'For each classiﬁcation, we also extracted a confusion matrix and explored which individual features to a given classiﬁcation using contributed the most Pearson correlations.',\n     'ResultsTable S2 summarizes the demographic and clinical characteristics of children with WS.',\n     'At follow-up, 10 infants out of 32 children with WS developed ASD/ID (WS+).'],\n    ['Eight children had ASD and ID, whereas 2 had only ID.',\n     'As expected, all variables related to ASD and ID were signiﬁcantly different in WS+ compared with WS−. Figure 2a summarizes the best classiﬁcation models using the decision stump algorithm (leave one out).',\n     'As shown, multimodal classiﬁcation outperformed unimodal classiﬁcation to distinguish WS and TD.',\n     'Therefore, we only used the multimodal approach to classify WS+ vs. WS−. The best model reached 76.47% accuracy classify- ing WS vs. TD and 81.25% accuracy classifying WS+ vs. WS− based on multimodal features extracted during early interactions.',\n     'Interestingly, the confusion matrices (Fig.',\n     '2b) show that when classifying WS vs. TD, all errors came from TD being misclassiﬁed as WS (N = 12); when clas- sifying WS+ vs. WS−, most errors came from WS+ being misclassiﬁed as WS− (N = 5).Table 1 lists the best features for each multimodal classiﬁcation based on the Pearson correlation values.',\n     'The best features to distinguish WS and TD included four infant HM features, 1 mother audio feature.',\n     'In contrast, the best features to distinguish WS+ and WS− included a (N = 2), combination of']],\n   'num_chunks': 4},\n  {'page_number': 5,\n   'text': '2| o Confusion Machine learning classification Decision stump (leave one out) matrices I West vs. TD (n=51) 90 Clas: ied West T 80 as D West 32 70 el 12 60 50 West with ID/ASD vs. 40 I West with out ID/ASD 30 (N=32) West with West with 20 ID/ASD out ID/ASD West with 5 5 10 ID/ASD West with 1 21 Mutimodal Video Audio out ID/ASDFig. 2 Machine learning classiﬁcation of WS vs. TD and WS+ vs. WS− based on uni- and multimodal features extracted during early infant–mother interaction.Table 1 Best features for classiﬁcation (based on signiﬁcant Pearson’s correlation between feature and class).Feature characteristics Pearson r West vs. Typical developing Ratio of all maternal audio intervention during free interaction Audio, mother 0.35 Total number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 Total number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 Vertical amplitude of the giraffe (front view camera) Video, infant −0.30 Movement acceleration max (side view camera) during free interaction Video, infant 0.29 West with ASD/ID vs. West without ASD/ID Total number of all infant vocalization during free interaction Audio, infant −0.56 Synchrony ratio (infant response to mother) Audio, synchrony −0.55 Ratio of all infant vocalization during free interaction Audio, infant −0.55 Motherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 Non-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 HM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 HM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 HM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 Curvature max (side view camera) during the giraffe interaction Video, infant −0.37 Relative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 p-value 0.012 0.014 0.023 0.032 0.034 <0.001 <0.001 0.001 0.002 0.005 0.008 0.01 0.014 0.039 0.04HM hand movement, ASD autism spectrum disorder, ID intellectual disability, SD standard deviation.',\n   'sentences': ['2| o Confusion Machine learning classification Decision stump (leave one out) matrices I West vs. TD (n=51) 90 Clas: ied West T 80 as D West 32 70 el 12 60 50 West with ID/ASD vs. 40 I West with out ID/ASD 30 (N=32) West with West with 20 ID/ASD out ID/ASD West with 5 5 10 ID/ASD West with 1 21 Mutimodal Video Audio out ID/ASDFig.',\n    '2 Machine learning classiﬁcation of WS vs. TD and WS+ vs. WS− based on uni- and multimodal features extracted during early infant–mother interaction.',\n    'Table 1 Best features for classiﬁcation (based on signiﬁcant Pearson’s correlation between feature and class).Feature characteristics Pearson r West vs. Typical developing Ratio of all maternal audio intervention during free interaction Audio, mother 0.35 Total number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 Total number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 Vertical amplitude of the giraffe (front view camera) Video, infant −0.30 Movement acceleration max (side view camera) during free interaction Video, infant 0.29 West with ASD/ID vs. West without ASD/ID Total number of all infant vocalization during free interaction Audio, infant −0.56 Synchrony ratio (infant response to mother) Audio, synchrony −0.55 Ratio of all infant vocalization during free interaction Audio, infant −0.55 Motherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 Non-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 HM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 HM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 HM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 Curvature max (side view camera) during the giraffe interaction Video, infant −0.37 Relative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 p-value 0.012 0.014 0.023 0.032 0.034 <0.001 <0.001 0.001 0.002 0.005 0.008 0.01 0.014 0.039 0.04HM hand movement, ASD autism spectrum disorder, ID intellectual disability, SD standard deviation.'],\n   'page_sentence_count_spacy': 3,\n   'sentence_chunks': [['2| o Confusion Machine learning classification Decision stump (leave one out) matrices I West vs. TD (n=51) 90 Clas: ied West T 80 as D West 32 70 el 12 60 50 West with ID/ASD vs. 40 I West with out ID/ASD 30 (N=32) West with West with 20 ID/ASD out ID/ASD West with 5 5 10 ID/ASD West with 1 21 Mutimodal Video Audio out ID/ASDFig.',\n     '2 Machine learning classiﬁcation of WS vs. TD and WS+ vs. WS− based on uni- and multimodal features extracted during early infant–mother interaction.',\n     'Table 1 Best features for classiﬁcation (based on signiﬁcant Pearson’s correlation between feature and class).Feature characteristics Pearson r West vs. Typical developing Ratio of all maternal audio intervention during free interaction Audio, mother 0.35 Total number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 Total number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 Vertical amplitude of the giraffe (front view camera) Video, infant −0.30 Movement acceleration max (side view camera) during free interaction Video, infant 0.29 West with ASD/ID vs. West without ASD/ID Total number of all infant vocalization during free interaction Audio, infant −0.56 Synchrony ratio (infant response to mother) Audio, synchrony −0.55 Ratio of all infant vocalization during free interaction Audio, infant −0.55 Motherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 Non-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 HM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 HM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 HM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 Curvature max (side view camera) during the giraffe interaction Video, infant −0.37 Relative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 p-value 0.012 0.014 0.023 0.032 0.034 <0.001 <0.001 0.001 0.002 0.005 0.008 0.01 0.014 0.039 0.04HM hand movement, ASD autism spectrum disorder, ID intellectual disability, SD standard deviation.']],\n   'num_chunks': 1},\n  {'page_number': 6,\n   'text': 'synchrony vocalization features (N = 3) and infant HM features (N = 5), the last of which showed lower correla- tion scores.DiscussionTo the best of our knowledge, this is the ﬁrst study to apply multimodal to mother–infant interactions in the context of WS. Com- bining an infant–mother interaction at 9 months signiﬁcantly pre- dicted the development of ASD or severe to moderate ID at 4 years of age in the high-risk children with WS. Confusion matrices showed that the classiﬁcation errors were not random, enhancing the interest of the compu- tational method proposed here. In addition, the best contributing features for the performed classiﬁcations differed when classifying WS vs. TD and WS+ vs. WS−. Infant HMs were the most signiﬁcant features to distin- guish WS versus TD, probably reﬂecting the motor impact due to acute WS encephalopathy. For classifying WS+ vs. WS−, the contribution of infant audio features and synchrony features became much more relevant combined with several HM features.We believe that the importance of synchrony and reciprocity during early interactions is in line with recent studies that have investigated the risk of ASD or NDD during the ﬁrst year of life from home movies (e.g., refs. 11,24), from prospective follow-up of high-risk infants such as siblings (e.g., refs. 4,28) or infants with WS (e.g., ref. 14), and from prospective studies assessing tools to screen risk for autism (e.g., ref. 29). In the ﬁeld of ASD, synchrony, reciprocity, parental sensitivity, and emotional engagement are now proposed as targets of early interventions30, which could prevent early inter- active vicious circles. Parents of at-risk infants try to compensate for the lack of interactivity of their child by modifying their stimulation and therefore sometimes interactions24. Early reinforcing identiﬁcation of these interactive targets is especially useful among babies with neurological comorbidities because delays and impairments in early social interactions are not sufﬁ- cient to predict ASD.Similarly, we believe that the importance of HM in distinguishing WS vs. TD on one hand, and WS+ vs. WS− on the other hand, is also in line with the studies that investigated the importance of non-social behaviors for investigating the risk of ASD or NDD during the ﬁrst year of life. For example, studying home movies, Purpura et al. found more bilateral HM and ﬁnger movements in infants who will later develop ASD31. Similarly, several prospective follow-up studies of high-risk siblings32–35 or retrospective studies on home movies36,37 reported spe- ciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been assessed with automatized and computational procedures, focusing on eye tracking at early stages38–40, vocal pro- ductions41, analysis of acoustics of ﬁrst utterances or cry episodes42, but none was done in an interactive setting. Our study proposed a paradigm shift from the assessment of infant behavior to dyadic assessment of interactions, as previously achieved in retrospective approaches using home movies24. The aim is not to implement studies of social signal processing in routine clinical work but rather to decompose clinical intuitions and signs and validate the most relevant cues of these clinical features. From clinical work, back to clinics, social signal processing is a rigorous step to help clinicians better identify and assess early targets of interventions.Given the exploratory nature of both our approach and method, our results should be interpreted with caution taking into account strengths (prospective follow-up, automatized multimodal social signal processing, and ecological standardized assessment) and limitations. These limitations include (1) the overall sample size knowing that WS is a rare disease; (2) the high rate of missing data during video recording due to the ecological conditions of the infant–mother interaction (mothers interposing between the camera and the infant); the ﬁnal sample size of WS+ (N = 10) that limited the power of machine learning methods.We conclude that the method proposed here combining multimodal automatized assessment of social signal pro- cessing during early interaction with infants at risk for NDD is a promising tool to decipher clinical features that remain difﬁcult to identify and assess. In the context of WS, we showed that such a method we proposed to label ‘behavioral and interaction imaging’ was able to sig- niﬁcantly predict the development of ASD or ID at 4 years of age in high-risk children who had WS and were assessed at 9 months of age.Acknowledgements The authors thank all of the patients and families who participated in this study. The study was funded by the EADS foundation (PILE), by the Agence Nationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de Recherche en Psychiatrie (GDR-3557). It was partially performed in the Labex SMART (ANR-11-LABX-65), which is supported by French state funds and managed by the ANR in the Investissements d’Avenir program under reference ANR-11-IDEX-0004-02. The sponsors had no involvement in the study design, data analysis, or interpretation of the results.Author details 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France. 2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France. 3Département de Psychiatrie de l’Enfant et de l’Adolescent, AP-HP, Hôpital Pitié-Salpêtrière, 47-83, Boulevard de l’Hôpital, 75651 Paris, Cedex 13, France. 4Ariana Pharmaceuticals, Research Department, Paris, France. 5Service de Neuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris, France',\n   'sentences': ['synchrony vocalization features (N = 3) and infant HM features (N = 5), the last of which showed lower correla- tion scores.',\n    'DiscussionTo the best of our knowledge, this is the ﬁrst study to apply multimodal to mother–infant interactions in the context of WS.',\n    'Com- bining an infant–mother interaction at 9 months signiﬁcantly pre- dicted the development of ASD or severe to moderate ID at 4 years of age in the high-risk children with WS.',\n    'Confusion matrices showed that the classiﬁcation errors were not random, enhancing the interest of the compu- tational method proposed here.',\n    'In addition, the best contributing features for the performed classiﬁcations differed when classifying WS vs. TD and WS+ vs. WS−. Infant HMs were the most signiﬁcant features to distin- guish WS versus TD, probably reﬂecting the motor impact due to acute WS encephalopathy.',\n    'For classifying WS+ vs. WS−, the contribution of infant audio features and synchrony features became much more relevant combined with several HM features.',\n    'We believe that the importance of synchrony and reciprocity during early interactions is in line with recent studies that have investigated the risk of ASD or NDD during the ﬁrst year of life from home movies (e.g., refs.',\n    '11,24), from prospective follow-up of high-risk infants such as siblings (e.g., refs.',\n    '4,28) or infants with WS (e.g., ref.',\n    '14), and from prospective studies assessing tools to screen risk for autism (e.g., ref.',\n    '29).',\n    'In the ﬁeld of ASD, synchrony, reciprocity, parental sensitivity, and emotional engagement are now proposed as targets of early interventions30, which could prevent early inter- active vicious circles.',\n    'Parents of at-risk infants try to compensate for the lack of interactivity of their child by modifying their stimulation and therefore sometimes interactions24.',\n    'Early reinforcing identiﬁcation of these interactive targets is especially useful among babies with neurological comorbidities because delays and impairments in early social interactions are not sufﬁ- cient to predict ASD.Similarly, we believe that the importance of HM in distinguishing WS vs. TD on one hand, and WS+ vs. WS− on the other hand, is also in line with the studies that investigated the importance of non-social behaviors for investigating the risk of ASD or NDD during the ﬁrst year of life.',\n    'For example, studying home movies, Purpura et al.',\n    'found more bilateral HM and ﬁnger movements in infants who will later develop ASD31.',\n    'Similarly, several prospective follow-up studies of high-risk siblings32–35 or retrospective studies on home movies36,37 reported spe- ciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been assessed with automatized and computational procedures, focusing on eye tracking at early stages38–40, vocal pro- ductions41, analysis of acoustics of ﬁrst utterances or cry episodes42, but none was done in an interactive setting.',\n    'Our study proposed a paradigm shift from the assessment of infant behavior to dyadic assessment of interactions, as previously achieved in retrospective approaches using home movies24.',\n    'The aim is not to implement studies of social signal processing in routine clinical work but rather to decompose clinical intuitions and signs and validate the most relevant cues of these clinical features.',\n    'From clinical work, back to clinics, social signal processing is a rigorous step to help clinicians better identify and assess early targets of interventions.',\n    'Given the exploratory nature of both our approach and method, our results should be interpreted with caution taking into account strengths (prospective follow-up, automatized multimodal social signal processing, and ecological standardized assessment) and limitations.',\n    'These limitations include (1) the overall sample size knowing that WS is a rare disease; (2) the high rate of missing data during video recording due to the ecological conditions of the infant–mother interaction (mothers interposing between the camera and the infant); the ﬁnal sample size of WS+ (N = 10) that limited the power of machine learning methods.',\n    'We conclude that the method proposed here combining multimodal automatized assessment of social signal pro- cessing during early interaction with infants at risk for NDD is a promising tool to decipher clinical features that remain difﬁcult to identify and assess.',\n    'In the context of WS, we showed that such a method we proposed to label ‘behavioral and interaction imaging’ was able to sig- niﬁcantly predict the development of ASD or ID at 4 years of age in high-risk children who had WS and were assessed at 9 months of age.',\n    'Acknowledgements The authors thank all of the patients and families who participated in this study.',\n    'The study was funded by the EADS foundation (PILE), by the Agence Nationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de Recherche en Psychiatrie (GDR-3557).',\n    'It was partially performed in the Labex SMART (ANR-11-LABX-65), which is supported by French state funds and managed by the ANR in the Investissements d’Avenir program under reference ANR-11-IDEX-0004-02.',\n    'The sponsors had no involvement in the study design, data analysis, or interpretation of the results.',\n    'Author details 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France.',\n    '2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France.',\n    '3Département de Psychiatrie de l’Enfant et de l’Adolescent, AP-HP, Hôpital Pitié-Salpêtrière, 47-83, Boulevard de l’Hôpital, 75651 Paris, Cedex 13, France.',\n    '4Ariana Pharmaceuticals, Research Department, Paris, France.',\n    '5Service de Neuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris, France'],\n   'page_sentence_count_spacy': 33,\n   'sentence_chunks': [['synchrony vocalization features (N = 3) and infant HM features (N = 5), the last of which showed lower correla- tion scores.',\n     'DiscussionTo the best of our knowledge, this is the ﬁrst study to apply multimodal to mother–infant interactions in the context of WS.',\n     'Com- bining an infant–mother interaction at 9 months signiﬁcantly pre- dicted the development of ASD or severe to moderate ID at 4 years of age in the high-risk children with WS.',\n     'Confusion matrices showed that the classiﬁcation errors were not random, enhancing the interest of the compu- tational method proposed here.',\n     'In addition, the best contributing features for the performed classiﬁcations differed when classifying WS vs. TD and WS+ vs. WS−. Infant HMs were the most signiﬁcant features to distin- guish WS versus TD, probably reﬂecting the motor impact due to acute WS encephalopathy.',\n     'For classifying WS+ vs. WS−, the contribution of infant audio features and synchrony features became much more relevant combined with several HM features.',\n     'We believe that the importance of synchrony and reciprocity during early interactions is in line with recent studies that have investigated the risk of ASD or NDD during the ﬁrst year of life from home movies (e.g., refs.',\n     '11,24), from prospective follow-up of high-risk infants such as siblings (e.g., refs.',\n     '4,28) or infants with WS (e.g., ref.',\n     '14), and from prospective studies assessing tools to screen risk for autism (e.g., ref.'],\n    ['29).',\n     'In the ﬁeld of ASD, synchrony, reciprocity, parental sensitivity, and emotional engagement are now proposed as targets of early interventions30, which could prevent early inter- active vicious circles.',\n     'Parents of at-risk infants try to compensate for the lack of interactivity of their child by modifying their stimulation and therefore sometimes interactions24.',\n     'Early reinforcing identiﬁcation of these interactive targets is especially useful among babies with neurological comorbidities because delays and impairments in early social interactions are not sufﬁ- cient to predict ASD.Similarly, we believe that the importance of HM in distinguishing WS vs. TD on one hand, and WS+ vs. WS− on the other hand, is also in line with the studies that investigated the importance of non-social behaviors for investigating the risk of ASD or NDD during the ﬁrst year of life.',\n     'For example, studying home movies, Purpura et al.',\n     'found more bilateral HM and ﬁnger movements in infants who will later develop ASD31.',\n     'Similarly, several prospective follow-up studies of high-risk siblings32–35 or retrospective studies on home movies36,37 reported spe- ciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been assessed with automatized and computational procedures, focusing on eye tracking at early stages38–40, vocal pro- ductions41, analysis of acoustics of ﬁrst utterances or cry episodes42, but none was done in an interactive setting.',\n     'Our study proposed a paradigm shift from the assessment of infant behavior to dyadic assessment of interactions, as previously achieved in retrospective approaches using home movies24.',\n     'The aim is not to implement studies of social signal processing in routine clinical work but rather to decompose clinical intuitions and signs and validate the most relevant cues of these clinical features.',\n     'From clinical work, back to clinics, social signal processing is a rigorous step to help clinicians better identify and assess early targets of interventions.'],\n    ['Given the exploratory nature of both our approach and method, our results should be interpreted with caution taking into account strengths (prospective follow-up, automatized multimodal social signal processing, and ecological standardized assessment) and limitations.',\n     'These limitations include (1) the overall sample size knowing that WS is a rare disease; (2) the high rate of missing data during video recording due to the ecological conditions of the infant–mother interaction (mothers interposing between the camera and the infant); the ﬁnal sample size of WS+ (N = 10) that limited the power of machine learning methods.',\n     'We conclude that the method proposed here combining multimodal automatized assessment of social signal pro- cessing during early interaction with infants at risk for NDD is a promising tool to decipher clinical features that remain difﬁcult to identify and assess.',\n     'In the context of WS, we showed that such a method we proposed to label ‘behavioral and interaction imaging’ was able to sig- niﬁcantly predict the development of ASD or ID at 4 years of age in high-risk children who had WS and were assessed at 9 months of age.',\n     'Acknowledgements The authors thank all of the patients and families who participated in this study.',\n     'The study was funded by the EADS foundation (PILE), by the Agence Nationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de Recherche en Psychiatrie (GDR-3557).',\n     'It was partially performed in the Labex SMART (ANR-11-LABX-65), which is supported by French state funds and managed by the ANR in the Investissements d’Avenir program under reference ANR-11-IDEX-0004-02.',\n     'The sponsors had no involvement in the study design, data analysis, or interpretation of the results.',\n     'Author details 1Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres, 75015 Paris, France.',\n     '2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France.'],\n    ['3Département de Psychiatrie de l’Enfant et de l’Adolescent, AP-HP, Hôpital Pitié-Salpêtrière, 47-83, Boulevard de l’Hôpital, 75651 Paris, Cedex 13, France.',\n     '4Ariana Pharmaceuticals, Research Department, Paris, France.',\n     '5Service de Neuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris, France']],\n   'num_chunks': 4},\n  {'page_number': 7,\n   'text': 'Conﬂict of interest The authors declare that they have no conﬂict of interest.Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.Supplementary Information accompanies this paper at (https://doi.org/ 10.1038/s41398-020-0743-8).Received: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January 2020Published online: 03 February 2020',\n   'sentences': ['Conﬂict of interest The authors declare that they have no conﬂict of interest.',\n    'Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.',\n    'Supplementary Information accompanies this paper at (https://doi.org/ 10.1038/s41398-020-0743-8).Received: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January 2020Published online: 03 February 2020'],\n   'page_sentence_count_spacy': 3,\n   'sentence_chunks': [['Conﬂict of interest The authors declare that they have no conﬂict of interest.',\n     'Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.',\n     'Supplementary Information accompanies this paper at (https://doi.org/ 10.1038/s41398-020-0743-8).Received: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January 2020Published online: 03 February 2020']],\n   'num_chunks': 1}]}"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Splitting each chunk into its own item**","metadata":{}},{"cell_type":"code","source":"import re\n# Split each chunk into its own item\npages_and_chunks = []\nfor pdf in loaded_pdf_and_text:\n    for page in pdf['text']:\n      for sentence_chunk in page[\"sentence_chunks\"]:\n        chunk_dict = {}\n        chunk_dict[\"pdf_name\"] = pdf['filename']\n        chunk_dict[\"page_number\"] = page[\"page_number\"]\n\n        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\" \", \" \"). strip()\n        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1',joined_sentence_chunk)\n        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n        # Get stats about the chunk\n        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n        pages_and_chunks.append(chunk_dict)\n\n# How many chunks do we have?\nlen(pages_and_chunks)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:24:34.566480Z","iopub.execute_input":"2024-05-01T07:24:34.566956Z","iopub.status.idle":"2024-05-01T07:24:34.610615Z","shell.execute_reply.started":"2024-05-01T07:24:34.566923Z","shell.execute_reply":"2024-05-01T07:24:34.609065Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"517"},"metadata":{}}]},{"cell_type":"code","source":"#Let's print some random sample\nimport random\nrandom.sample(pages_and_chunks, k=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:26:36.351414Z","iopub.execute_input":"2024-05-01T07:26:36.352067Z","iopub.status.idle":"2024-05-01T07:26:36.361680Z","shell.execute_reply.started":"2024-05-01T07:26:36.352013Z","shell.execute_reply":"2024-05-01T07:26:36.360320Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[{'pdf_name': 'Patten_Audio.pdf',\n  'page_number': 1,\n  'sentence_chunk': '\", NAUo% 3 P HENPublished in final edited form as:J Autism Dev Disord.2014 October ; 44(10): 2413–2428.doi:10.1007/s10803-014-2047-4. Vocal patterns in infants with Autism Spectrum Disorder: Canonical babbling status and vocalization frequencyElena Patten, Ph. D.1, Katie Belardi, M. S.2, Grace T. Baranek, Ph. D.2, Linda R. Watson, Ed. D. 2, Jeffrey D. Labban, Ph. D.1, and D. Kimbrough Oller, Ph. D.3 1Univ.of North Carolina, Greensboro2Univ.of North Carolina, Chapel Hill3Univ.',\n  'chunk_char_count': 480,\n  'chunk_word_count': 68,\n  'chunk_token_count': 120.0}]"},"metadata":{}}]},{"cell_type":"code","source":"# Get stats about our chunks\nimport pandas as pd\ndf = pd.DataFrame(pages_and_chunks)\ndf.describe().round(2)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:55.989882Z","iopub.execute_input":"2024-05-01T07:27:55.990321Z","iopub.status.idle":"2024-05-01T07:27:56.047403Z","shell.execute_reply.started":"2024-05-01T07:27:55.990291Z","shell.execute_reply":"2024-05-01T07:27:56.045718Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"       page_number  chunk_char_count  chunk_word_count  chunk_token_count\ncount       517.00            517.00            517.00             517.00\nmean          9.48           1187.22            182.33             296.80\nstd           7.16            710.37            114.55             177.59\nmin           1.00              5.00              1.00               1.25\n25%           4.00            546.00             77.00             136.50\n50%           7.00           1197.00            184.00             299.25\n75%          16.00           1666.00            256.00             416.50\nmax          35.00           4075.00            647.00            1018.75","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_number</th>\n      <th>chunk_char_count</th>\n      <th>chunk_word_count</th>\n      <th>chunk_token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>517.00</td>\n      <td>517.00</td>\n      <td>517.00</td>\n      <td>517.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>9.48</td>\n      <td>1187.22</td>\n      <td>182.33</td>\n      <td>296.80</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.16</td>\n      <td>710.37</td>\n      <td>114.55</td>\n      <td>177.59</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.00</td>\n      <td>5.00</td>\n      <td>1.00</td>\n      <td>1.25</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.00</td>\n      <td>546.00</td>\n      <td>77.00</td>\n      <td>136.50</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.00</td>\n      <td>1197.00</td>\n      <td>184.00</td>\n      <td>299.25</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>16.00</td>\n      <td>1666.00</td>\n      <td>256.00</td>\n      <td>416.50</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>35.00</td>\n      <td>4075.00</td>\n      <td>647.00</td>\n      <td>1018.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Show random chunks with under 30 tokens in length\nmin_token_length = 30\nfor row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n  print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text:{row[1][\"sentence_chunk\"]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:28:22.306166Z","iopub.execute_input":"2024-05-01T07:28:22.306789Z","iopub.status.idle":"2024-05-01T07:28:22.324027Z","shell.execute_reply.started":"2024-05-01T07:28:22.306720Z","shell.execute_reply":"2024-05-01T07:28:22.322771Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Chunk token count: 3.0 | Text:Through this\nChunk token count: 13.0 | Text:Author manuscript; available in PMC 2014 October 01.\nChunk token count: 28.75 | Text:What did the researchers do and find?PLOS Medicine | https://doi.org/10.1371/journal.pmed.1002705 November 27, 2018\nChunk token count: 7.5 | Text:Automatic diagnostic procedure\nChunk token count: 29.25 | Text:For the above, despite the novelty@ SpringerContent courtesy of Springer Nature, terms of use apply. Rights reserved.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Let’s filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length.**","metadata":{}},{"cell_type":"code","source":"pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] >= min_token_length].to_dict(orient=\"records\")\npages_and_chunks_over_min_token_len[:2]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:32:25.601133Z","iopub.execute_input":"2024-05-01T07:32:25.601662Z","iopub.status.idle":"2024-05-01T07:32:25.620081Z","shell.execute_reply.started":"2024-05-01T07:32:25.601630Z","shell.execute_reply":"2024-05-01T07:32:25.618139Z"},"scrolled":true,"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[{'pdf_name': 'LEE.pdf',\n  'page_number': 1,\n  'sentence_chunk': '.sensorsbyLetterLetter Deep-Learning-Based Detection of Infants with Autism Spectrum Disorder Using Auto-Encoder Feature RepresentationJung Hyuk Lee 1, Geon Woo Lee 1, Guiyoung Bong 2, Hee Jeong Yoo 2,3 and Hong Kook Kim 1,*1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, Korea; ljh0412@gist.ac.kr (J. H. L.); geonwoo0801@gist.ac.kr (G. W. L.) 2 Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam-si,Gyeonggi-do 13620, Korea; 20409@snubh.org (G. B.); hjyoo@snu.ac.kr (H. J. Y.)3 Department of Psychiatry, College of Medicine, Seoul National University, Seoul 03980, Korea * Correspondence: hongkook@gist.ac.krReceived: 29 October 2020; Accepted: 24 November 2020; Published: 26 November 2020check for v updatesAbstract: Autism spectrum disorder (ASD) is a developmental disorder with a life-span disability. While diagnostic instruments have been developed and qualiﬁed based on the accuracy of the discrimination of children with ASD from typical development (TD) children, the stability of such procedures can be disrupted by limitations pertaining to time expenses and the subjectivity of clinicians. Consequently, automated diagnostic methods have been developed for acquiring objective measures of autism, and in various ﬁelds of research, vocal characteristics have not only been reported as distinctive characteristics by clinicians, but have also shown promising performance in several studies utilizing deep learning models based on the automated discrimination of children with ASD from children with TD. However, diﬃculties still exist in terms of the characteristics of the data, the complexity of the analysis, and the lack of arranged data caused by the low accessibility for diagnosis and the need to secure anonymity. In order to address these issues, we introduce a pre-trained feature extraction auto-encoder model and a joint optimization scheme, which can achieve robustness for widely distributed and unreﬁned data using a deep-learning-based method for the detection of autism that utilizes various models. By adopting this auto-encoder-based feature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic parameter set (eGeMAPS) speech feature data set, we acquire improved performance in the detection of ASD in infants compared to the raw data set. Keywords: auto-encoder; bidirectional long short-term memory (BLSTM); joint optimization; acoustic feature extraction; autism spectrum disorder1. IntroductionAutism spectrum disorder (ASD) is a developmental disorder with a high probability of causing diﬃculties in social interactions with other people [1]. According to the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), ASD involves several characteristics such as being conﬁned to speciﬁc interests or behaviors, delayed linguistic development, and poor functionality in terms of communicating or functioning in social situations [2].',\n  'chunk_char_count': 3028,\n  'chunk_word_count': 414,\n  'chunk_token_count': 757.0},\n {'pdf_name': 'LEE.pdf',\n  'page_number': 1,\n  'sentence_chunk': 'As there is wide variation in terms of the types and severities of ASD based on its characteristics, the disorder is referred to as a spectrum [1]. Not only does ASD have the characteristics of a developmental disorder with a life-span disability, but its prevalence is also increasing—from 1 in 150 children in 2000 to 1 in 54 children in 2016 [3]. As diverse evidence has been obtained from previous research showing that the chance of improvement in theSensors 2020, 20, 6762; doi:10.3390/s20236762www.mdpi.com/journal/sensors',\n  'chunk_char_count': 529,\n  'chunk_word_count': 83,\n  'chunk_token_count': 132.25}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Embedding our text chunks**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nembedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:33:11.365724Z","iopub.execute_input":"2024-05-01T07:33:11.366149Z","iopub.status.idle":"2024-05-01T07:33:22.415371Z","shell.execute_reply.started":"2024-05-01T07:33:11.366119Z","shell.execute_reply":"2024-05-01T07:33:22.413761Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dcfcc67db14dbd9b7e5acf1f257924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fcef29e4c4469c934e608f71102564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5127c1d18b4131a1b9324f148979dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53124accb1834171a809195702955021"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6c4476ba7c749b5a8e40ac8670b46d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe5252b7643480791bb02053fddbd97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d003a30ae7f48e0a9c0af4124ca82a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e96d34376ee4fb097a575438ac6d484"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e88962ebef4b38807958f792e28eb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5147eda73b419d9ec37e9915abf220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc17916f67c0470096888a14fd372fd6"}},"metadata":{}}]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:33:44.346387Z","iopub.execute_input":"2024-05-01T07:33:44.347312Z","iopub.status.idle":"2024-05-01T07:33:44.355896Z","shell.execute_reply.started":"2024-05-01T07:33:44.347271Z","shell.execute_reply":"2024-05-01T07:33:44.354371Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Create embeddings one by one\nfor item in pages_and_chunks_over_min_token_len:\n  item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:33:57.635894Z","iopub.execute_input":"2024-05-01T07:33:57.636469Z","iopub.status.idle":"2024-05-01T07:38:55.791901Z","shell.execute_reply.started":"2024-05-01T07:33:57.636430Z","shell.execute_reply":"2024-05-01T07:38:55.790505Z"},"scrolled":true,"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bb2fd9163bb44cf963074b1e5139975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cfe082c9a29411bb446c4ef7ebb2f54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd89d396ac5f411dbc3ae2a594832923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b017286b52449b8cd363d419f3b10d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77d9ec7b485c4ca798f793f21802b312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fef0caf65219405d9a3d61f07e1e5e1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f5d21248104cf9bab552e3b14d5021"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88be528b744a48148e1e58b546cc9135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a6087853464f699c71751851afa478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd2ead7f7c84bbab788a0ae94829980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf9f57f520bf4da4b17ac97421728fc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96397ab769b9430b8a29b679c361ed20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547d1607971847c482e730528c7d329b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c54de124284dc9b9d1c596a928febe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160de945d5d846b28919b0bc84da0bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d4cbb428bf45ac96c59769ab635966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759ce621c5d34c0fbbd0642a45403645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ef1f6437d64f9fb63d94cc9c2d301b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ced18bd8694ff8a7a4b3965b60b6fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86034f16be774ee8b8b3e84a83eec7f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bba658879634bea91cf087e4e9310d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fc2a1fc4884424b545877cb257e227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19fa40ec53e54010a325543088e0f74b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7e2fa43de3411f9947c214863fc084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d7576685294f2288eb76a786be8322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233a45c574fc490c913b4164dea272b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf8b5c051ba496497f958ff2a51a45d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a981da3ce1f49ec973d63566ef174dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4be120b5c64b3cb7576f1167aba2ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba8393d75e447318d8f05b9179f21e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe90929087e24041b0a670301e8c099c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"600a4aafbf1d4c2e8f615b82f814cf29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cea5bb9abef41ecafdcc577850a552f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b081736712d4a6bb41bc4b34cbc6c44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c592ee7460e0499a9eed09d3678a7a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f97da3476734dd1ba69445633b16995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a36407f6f6a4111b0997032441d3526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead3dfe9994e4b27bfeb3aef81e1ec9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcfebf0b13f04fa7bda8d896303913e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0692984153d54f1aa2302bee50e239e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b7dc19fe4124179ad71e556b538b6c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3cbab4b2a94bd78bea0f15a1713b46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4d57275cae4c138973a4663a83d8bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691011a6abd6494fb2cc5bc0362bade3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f9b751f48b4b85901a8b7a3fb02e09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb53b7bccc1942baacfb37beaa153096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb1212282bf4077af239fd069547e4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f874af2ff994100ab7c208b574575eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92166086ae74cc0b3656c7706bcf2f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd1b76a2e3b432cb007958a2e80ad51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37de99d7408341ba9d33ae615d12db61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc33ce59cab8469489858bea554f87bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0184839e460241f5aca5f7083bb50a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cce565562184c79ad9ffe82ff55f465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3188bffa1493480798977fba3c606afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e9b1b159e414b4ca63e2f6df0377af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf31bf26bed48c5b87fa7d45c7c9ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7d29fd5be84447ab0149013af109ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"104c71057fb9477aaa29d27fc9c4de3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed452e0a00f8470cb43b76378d959e49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a568958c704b1e9f74435a58dd2fbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9359a81a8e8144e1a7cfa4e902461043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fed3c4c52fd4fb0b4850bb7b41301d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"083e668c4de646e8a555ca5927fb0368"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24176c4dee34a8ba170b00400c65b1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5f7a54ad4d545de8f8b93a6b084420f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d060dbdae4bb4fb6a592c9fedad60c68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aff5a423b7848e08b84e3a75483b22c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88e19f15c344886b4bea3ac128a8d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be56d97181794ef2afebccf618ff9a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac845821c5894c778f42fd54721d1e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08dbd9b12101453f8225b9b85b90d0a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17467281f17401aab224fd66b40994a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d391c0158449b9b7801ae3adeafa08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31c318bbb534492db73943d53283ab84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c9e7153e5a4ee88ce049784b79a180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6444d071cfa84c46bab26204f4741c45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2980cf68dae34fdbbd20629c243368d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59294de634e9475992bc6ca41b5c172e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da53eeb0e0b242e1a82eb23e421c8295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d599a5af52d46348270bfa7c8ed98e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00abedfa22954267a47ea6b87b9488a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2177986537104c74a1b1e4e12b20bb21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db36462c9e9d4b3db3615b6557b4d119"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99c2166dd626490ab95b2e6ad363a4a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46cd0c64080e430b83830797cbe7f0a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14379a1a84a74ed4ac76a07761b4ce09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677c285b99e442ec9121eaf2f96ac5d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ee7ad512bc4fdcb35e7ae66e098952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6cb88f61fa4f538c6d6eb87467c9dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12680ddff66f466cb32a1cb25589da34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091d7f3284af4db2b3a5596fe8f67f8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9a459afe8e4cbcbd0563ea52be80d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d38925be8774d32a7967ec774c73fd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e33f1a48564f0c9845771782dc2a5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6375d4196153419b887719085184b881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0d9d916f50464a85f1b56517391fb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5b1f28a88d4007a16fd31aea8b9efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0044c239a1b745ab9ebfc02b8ee91d82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aee710bbb9c4e05916e7575471e0b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f3d0219fbe47ce9e5a8e4d5ec2e849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e4d2117e874c7cb8f0e6d7c549cb34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d2277df84d4e3d96469b41058c42bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e247726db92541b292e9afc0c224e75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c1e90b198c428ba3d3d77341ec47b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81e7bea65c640c2afbdb7414aa45def"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f990a54c6bef466283ae253da3493f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65c72ecc99b4972990105bed8a3519c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b2100004c54a2bb13c7cb40fb2129a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abcecd5cf0ea427e86182f4cfa4eaf8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab65aafad48476d97a74aaae76d9c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c54e072fe2343998a9ce71da2dc2f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6ee6668ab64bfbbdc28e77b0e882f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4166c905c0b34fbba8e40b4357f5f9b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7387f785fbc54b39aa6b9379c78426a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c4e661a6d54f5b8e0bd8dedee78381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a418b56aaac047738c734c5ec4b1e725"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349bac52f5254c27abd84d2b9776b80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"483ce70691f24b52a52b2b4eb4b02973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768f3bb9ff6844d1a97f129f28e8ef5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7606a8bc446b4bc998b8aeea65c08f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa1decda4b3484da516d7cc5ed916c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b02af694634ae18946499fc2ff30b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03e49ef4e864489b278fedc031197ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2786ab9df8d54d66a90c91345008c18d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23dccab0c12f4a7ebd77994e066035ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b140bc979264b9d8f4ea499c1cb2a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eca6c6aa4954d4a8bd6c1e4c98c7e7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c4a07036fe4d4f816ae715773eb8f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53d4987954e844e28cf6829915f76f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6883b6d41b841d482fddceb181cba65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dad1299c7d6647fe82feb7ba3dddf8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b6d481bc9541a284c55654605176a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb90418fa0014b2785b1967d60695128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08e666cb4c04dd2bd0e08c967cdb620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e1cfdd42dc472ba81975fac1a6c2d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a92ae41b1714e8f8b3ff3f9da4f3d21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0538fe0deafe4370befd0de0e3ac03b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370fa5f5fda64eb6a75d1a15b3b57535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a617a4b7df094b7ca6a60a38ae0aad44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d8c2b247634944bb3fedc8ade9025d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6c4b815f5d45349ac7da049879448d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca88c380f8604fa2ad7ab81545536c54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a8e204a8344da58e9bd3c7f0a7c018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a72ecaf465f44089870e5711bd5332e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d29e0640c1346aab0014ddb02c32baa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8100b928180341afb4424cfa2812ef61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1696b407c741ed863adac875ff3c72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11aa946a87e743e38713333db3bf15db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52b196646a348a2b801eccbfbd131c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edbaac07ed1b4156accebd08bf010673"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58caadc81d6b4143b003635bfaf2c809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005af677a8ba4d1b8a9cfa9cad7536e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14f1429553f4df6bc9dc6595081dfc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb2baf99fc44e8988d43e8649a4a32f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c16dd1a641464f9578a69e6928bc82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739ea243a6f34b38a8006e33d0a323d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c84d13b3d3a4e56b6dae404e40c1825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f812a6cac3574b94823b67ae08b139e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42aef86aeb7471ead34439e42be3a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9667573417494b04ba6c6960a2f52e8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd540b78f174be1b238207bcde1883f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd6c4b35013483fb37562216d7f2d0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ee6a221cf344a3b8f0826b077b2bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c090a097d27c4a8887d1dc55a60a5703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c99c7800f7145beb4f07ade83e6ff8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ace746969d64c68bdf17b7761a8d258"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da102b0c30a947058f3782e52525712e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ed87e64e094ea188485472c546503b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"054b185f7cd040d7bb1de25b4c9e4df5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b2965b836e41c99ec076a00c1407fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3942050bf0294b20b854c077f40425c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5efaa3a1674db39917a04cbd760dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"135f9907c6af4227a549ad60903f75e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e3756627214473c84c0d7da6e22e9f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7520604b0c8b46bca62caf92ff1a72ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad7ccead5e04311b898bb89c0404ce8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f7f741a6964a888bc308bffa1eb7e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ef382ee530421991b81d6d64389176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3b4dbfc4e746adb57754cdea3be1d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a295f7a90a24f9a9f70c1bbe9da49e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ca0e1b4de742c8beabcc8419e2e681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1facfb9792a4496b8ce291ab114c6129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bac292db99044e591f12979305c6280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8eb4e4383f241ba83145caeaaedd15f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85074a2d980a4ac79b08e7992d43245c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bed11f73d7a465e9a1aaa935186cb4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aa169987c9a447f9981033459533bd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b75d2291013348a2be6ff2a937ad14b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3c732a3d144f748042abc5a650b7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091f9acc31d3436db13cfc20149eb794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd36e33aed084d75bafdf36e94377744"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5586674a074bdc92164ec70ef40303"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f2c6ab59a64f98bc816bb956c7643a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f062f762d0d94701bc37aefc1feafcf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccdefe8eee9e4bb998e4aa72aaec7cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c528a4681f4b42a587b5f1deed03db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9162ad82520e48288654438bb4f77b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6533d4837614e008bda3a0fbc4f0144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fdda430370a47d89b22ce03015a8a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"103d735fb8af4d19a7ec33644ef2674c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34874b601e1641a1affcb2d598781a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59b6ae8b88734e38beebed115b0c9e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b292e5f6854b9da0069449662dfba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f3f7ee330a24f07a264e278c66e5d28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50851a0a3d1a4854bc4d6f031a1f6067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c08091dca240c599362e1c34c78682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5c22fbfe8946c680cf9efc95807ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed83179344194a38a7f00f3ab7f84366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c550e975a3104a25875d9b337356205b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d40478d628843a6a83119e0b7222a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2364e365b53548c09ce9dadabf931e2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a99a97db3e4f0ebf37ff19c67e4dfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e05863c4bb6e447abaf354d1ee5c23a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e927f57ad74d6e9487149dc3650dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9a4facf0294bcca14c1d84a54839cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9d79acc45904f3880c692b483c2b992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b063bc0cd7e14dada91f70dd78285732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055a9089878443e1ad50d6474d5d07fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db0b843df60e4cda8e6945822310e3ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a538f38e49d74a7987316d1191ba27fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa07e8a89e54a4e85522acf352655eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b8462e0c29e435b94b1f34e63e252fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b437fcddfbe54eadb60daecae638bf4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9865a1c7ad8940b5baa1a5972252b85c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ee68de1149849caa0ae4905bd5ed978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdff6b68726442e9c3d21bd17b2d73e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5946836c9ec24fa4a21e7b7c11060553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ecdbc8d97664baa8886c3496356459a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd08a67fc2843bda1b483fe3b5c26c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410a8d998f524b53b55316ed5f28c8d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88e6ddc448ad43f9a2448f5141fa9a49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ae9a4c2a164c92be2602da5372f620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ebe243b0f44c1381c456b5d66fca2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e2d2e010234f35bc3b3c0213b64781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33f9dcc6488471386cf86a821a1613b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161cfaff1f4e4a19a93842773681e012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"006c6e98310b4d67a7fcad6fea4184d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baca2881f17f4a53ada66bfbc1e32f6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"914eeb8111a046969cb7f6f7856608fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fad15ce3d5c472d93d6a8f1e4809b73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75363e8699584824bd411bd3f2de229c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5ab117a50f4b3ea0a2d0b2d24544dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88e44dd0bf994befbf2a6adc426a2495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff23c052e0e9463b955538f3fd395200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6ed52f2e824ec49bc3171e9596103c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01be4a4284840dfbc578493f65bb527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef552406974491cb6e5054573f29bec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45cafb67c6e4ef88a2d13869ad5f470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408b30cc40174f36994e6601c2167826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d31c9e09e34868ab669cda981db6a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db02d32aacf4d01aa04643f38035a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d71d0d2363a485a815aa1f45b76db57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f998d442ad40fea38cb3351b668eff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc722c7807b443084a3ac32b534e499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db0e63e6fd704c0cb891b2003c9cfdca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0f8b85e1e044a90acec87ce6449651e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81dc18c2bb64472a90b82484fa2d3056"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc0f9dec5a54fd98734f3bc05cb88ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f98bb7b4c70450995aad62d795ed03b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae43d078b125424f8e20077058435afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a6d0571fb943598c3dcd2e97292a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66f9803403614cb692b8972f33ede73b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b353c71e87a245ffb9da47f0ab605532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad12abcc910e42a495011bfada55973b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d5ac8084dd1417785fe4608e9b17695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"886e2af74ecc4ffb8a8cbffca9b60226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32856e64b53d45999c743b270de67562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b161f29271b94b3aa2c689b23ed9462a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1904bfbf2acb40db8d4d2ecc33329390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3157dea02024cd2b85ad1f5ae7dd0df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82da7dfd35f483b911572059da16f0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dde19948aa9943788c64f8c434005d2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c07df1864454de2aa9cb1bf0f2c4738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c99a77ed6d4715a13b056b9a761dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82fd57c72da94a9b870997bc994eecea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36dd44dd0db14250ae091326b8ab59ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc99feb22b54bda9ae75ee670fb3420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7641c9d10de4f95b15b4ce9d76773a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09a3529534f4a7e97451620190e6ce6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db1696fd763453685e2416209911649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11c9ed0f337743fa81fad43179a8fbec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d8dcb4cbec4e73ae2ab0e5b99ec83f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a91204a9dff049e49ace1fa2951d2285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd73a234362d465bb0f4393ba5db98b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c363f3312ad4a4c99724ff15e5ccd66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00c95cd767c488fa93168988379c4b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f4dac65d0f4daba38f6e676b7d092a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac97cd008cfb413eacdbe41922dd1395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b190e20ec874138a832037267134530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4263bfb5a854135bd14a9be35d18b6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec19c0c74d77439eafd1701a230d3813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa27821c1304d65b6c1d49f7f4941ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c9c8caaa71c4d8fb322a931eec15763"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f5a4e125724e64a389c2828e35de9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e02b578a0374752a80d7725b03dc738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2e8707ea5e41f1bb0c2994049bbe34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd7937f8517f4859b66ed66482234bf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172d316f99e742fcb0b5993eb05b7abc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d4ce3e884d47ab94c910720a25af34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa5f3accbcc4bf2861b290c4101f7f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe45a4ddb8044b7bd9b88f95725c52b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588cbe87ff454e0d9636965668194188"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63eed59f9bbd4081a8876329caa18e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0352612ba2d4256a734daf835c3dedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d16052d51254651ba3359f2faa49088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2267c57d353e451c98716bd9f878a567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958986a475d94ad09c8fea12007bf45b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c28a109f6584cdeb8ab66532ba9ea83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59746a9dde3a46a08ac739fa6378f347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd09f3d94b4468b9ef880673681e995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"049471c80ef04d00b14420b8b849fdaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1117f4fc6e9c4dc8a6aeef79de3f56a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e3a6e09598470da2a5b7f980f3be32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc97ed56b38a41ddabbd174084986556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"193afcdb0c1748868075f8f967a4e6f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128dc36afa764756bd8597af046ea1e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a4641c8794d4907a58e72bc8ae565d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61044027acfd49fa86632513085a4ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8318c65f5afc47f1b85e403967d4e33a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ab0a7a1e1b43b6980b35869fb71014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cd7f5e18ec545d8ab28c5ff1d53df79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67bf17f090344028d7f7b95027fde1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"657e4c4b18974ea58fc92363d2fa1847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e867c8ff4994639b3a077a4dd69ba2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ac35f3d8f84bce92ba7621e9352235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26c58ec194c44ac8d16ff372002a898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331a4aa0b6904a8192cf4f29675a5aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9879462e929548bcb0043dbbf2394304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a3ec9a853945a182a49df4507c897d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236696d94c9c4b4c846d6c61c5a64dbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a43431dcf30b4ea8bf01b32619b91fc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd6d1e4de6949158664335c982f408a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa142c4b0564f91827779c85ae71e81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d91a4d06a140368bbf6bd1e289c35a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a8f260232b54041af82fe11932bc7e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"323a0a61f9de4680a156bc027348e467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dab2cc41e9940c4912a11a452768439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19817dbd06e349e9b603895e996b8819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa8b6614a9c4436a127247c19f4fe6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba4a64af02444a9aa0d281e2188b7ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84034ea20b647ef9f8ade6f654527fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639e89e8e3c24db9a7404d404d863ca6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6873d32b6d04abb941e0acbd77e06d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15c11a95f5444657bef700762c173a8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b43149a9537480a8d493b53fed9fef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d937ebcf0d2f42079eca5204eb3ab6a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a92f8a230a5143d8b3e996cb1242e529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff449522068e4cd38663d28cbeb0d5b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61ce6772e5f4f19b7dfa83e51fd17d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2917dd329341ac8523c94c622d2ea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e973c666f414457b93e4b931029dbc06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dfa6a96ce664dd29506f7fa0171040f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"affffac30c9545ccae864e48b276cbba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d563a6a9f847ee87c59219fad34bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c501d19445d440ceb7c4d0956f916996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41eeaac9e15448e2941f427fd9804f25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec0a3034b68402d952d7dc160ba2e15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da54f7c1f1254ac3ba5ba8f76fee60c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08060624152945e0adfcfcdb753414cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581374ce85b948faa814e374a3e5237f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6338605e1e467ba6d2ab14cdd75ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09dd606159914b7a8e5efe604a401873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8f1fce06d2489983b966568cc89a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0121ca9d25c54b2f81e7e3ea6ccd9bb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5380d601d72c46f6b2f970b5fb28d417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4c55a8da514d6090b20ce249c4a8e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14dc89fd2c8d4e588c86c62fbd536fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c884667641344727841cf1823f8fe9f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bef708bc45a4bff9b85640017f7f40e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef02a15dade4f668c9b7b0e82b5e655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ec860749314e199753372c9012ec06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3fc04dc2de4c419934197796b4fb54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"052c49f7745d4be09431ef5a4abb5223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9316722243704cc59853b51ba17a8149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf1bce88255451c89be11f05bdb348a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3ab9809e974fc48d057578f35fd6cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0944f4fe3a31492e885d41f601a0c5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e409c9b2beaa47a683dbeb3ead63f1c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c42a00249843298a943e5c71a999b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee0386ef8d542448dc8de3dd9582507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1357eb504f074b4dbd89e53dc2ff3966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e0e43078cc495abec7e29ae637df7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c8d7503595a464cb45ba70efd51d659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc4c62e292947b68132bef408bd3349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc465a0717947b18bde58feab84edbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87eb953b1834fb4b27180670601b547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d3864b1089846e3a265c157d2908d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e29171ea1a4e95a575f1d02123a6f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9984a7e7b04e40a9983e23492446f0ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cac4ea8b71d444f885ac3fd3d9e0899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3bd06b3bcee45159cdb99d30bac7d35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8737dcfa424d638c32b82fa06bd4c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"733aed6d83c74929b6f5f47dd4eacdb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8b1290e5194453aa8059dedeaa05ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53098fd77ff043fab4ae1d2cc899b456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094b82a1d2484cc296e381e694c0144d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4ab353369541f582c9c0c8beb6cf99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06975b16047c4e279df3217b40ab013e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4013b87ab01340e7af90be167719c538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4587d218a3483a9116a7ee46d13a07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a486b70798464128acd197d837fd1d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279cde95a6ad41f5a2af298a70a0f60c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6997e4f515d84620833d5130c9b1286b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933b41727e16432794b780395fb1d0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a768263f92db4c41843b5115053abcb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a054a32b4ba7473fbd30f0de03eb6a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7704485062884d2eb8d8a34382b3c731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17d0bf8ee3d4da3aa07025f670815af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e27535aa5034225a5b1940d6b6239da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95f459d887c401bbebc6c04366a8670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a604ccd511d4bf9b5f9760a1c4e2d3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b57e5be3de349cfab692a2c7b0b26cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d83416ab4d45fdbac49ab6cb3c4ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528b89c78ab64f9caea7c29d4b8fd3f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe56d9c77054a62b8f622407d026a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e61282d6a2ca4e1a97ee033bed14d8e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a3d20e49a7b4e68b8d0f7c1a950d19a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fe8790942f4843bcc4e8d17b5b5724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4877084e5c3343bbb430cdd805c4a885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f70623de544a28a00bfd36933fdfc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33bb675de6004b8793db43bb583e872a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04817356c0734e02957df3b1573e6a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7616139ed04c53968e0ad11511536d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e5109aa520491d89aa8812fb331fce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d0fbc34e9c4f8b9edb647c64aec906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98fd30c2c7254b8d865de537cda96828"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d33aa6ca5aa4293beb82ace9cd2924c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e615031d11a74cbab06c45855b19a890"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de222ad984014ff590cbd0b695e7a1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76354b25302f47329bf71aa7616a7541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1506e1d0ef420da858ce79ee3b89dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b9d04665b45436a8785450ad706620e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bade68ba3e0462c949a36b23777b348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a112dc5bb24a19a254ed325aa58614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0372cc3f974c83aeafe4d345d8852c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07596d2762c84ba38c046a605442bfb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb6e9585e324da2bebb5894b179e0a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650d91575873448484cb79f8964b2d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e7ff509a914532b57dcd5192c89c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c6399cbb3894511aea2ee06640c70f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21cdc1e6ab84c5db3d71e83863e742e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf19564117d64ab1b3968e82c4017cc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d86843542f472a8407476703b1254e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f676cda8b674a198a8a9bc602582098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333d58ab62a24b5fb787a264b3e3ccc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c728345e14b46fb981ca7a376b5f0e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cabb585dd07c4bbe95a258e7e6d1597b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c977ba4e5f4148987c3ca5d601c294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99b942e62ba4cca90b14f7ef90cf3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca7d8ab36034f7ea4d6374f7b43233e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6498b458f22f4cfcb1fec4361e14fbef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2370a9cc914c48b6a270ed8681b05264"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad8a23594d74026beaa063c4e443775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb6ef8c1ad143dc925f3b44646a70e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f01ecaf7de4c1a879b95c0603e245f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e0df8af11b1485c9a048238c07ecd1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36547bc163404a8aaac3f6f242770b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ef04fa1148416e946cfcd41941d5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f67472a5e9654cfcb98f4b7c255d1322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f11430c06e46e3910abc338cec1c85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78b627210af4788a1a77d53d92ed849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c05a50e24a42498aa958c5c86b2514da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a19b0de836429fa06cc2f3bf0a2e95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf94a7c2b1c4ef88a139ce33f859e4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e90b51192dc447229725ec5e74f8af5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2acf00b457e4f0b827a1602bf3dc159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f6f531289cf4998a6aef7276e9c662a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fedcbad4ce5a475280ae976613c140d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405d10f1efe141beb9bcedcb313bad8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a57956ba2b4d1a809c6f9107468abb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a94cd699a4c419281c5d0d3757ec42b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38499622a0541bbbf893ba5cdd7a4b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb09b982aa44e03ab8d646d527f71fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6588604b6dd949c881ad861c9a7111ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be13a79c84c410c82dc1d9c93585737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0cffd3193b44860b54b5818418e946d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db6d18f64524f089417417ab915e22e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a92acaa41e442bda7a7ab67226315e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb670bdecb14ebbbcace74969b10bf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb92b13be48d4bb796f846dccd583ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb25847a33c141378cc72327353060e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e82867c74cb49009066786238efb281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f37d6fa768bd4838aa602b6419e1b80c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a497b12f003485fa9e64271fa92f640"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba738d7b4f004e6d91d33dbc086bbbcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"407867b402e147afbb5b22a3fbf57d8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"828d1b52150d4435ade2915ff965c094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a817662a290e4f98a0c91a3b9343c76a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39196dd6fa54cb99c8f2231cd745262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"446f3b3f8e504652bde31b61d7d35b59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76523e8773764a8ebb2bba622970f2d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32279b15cca949b9a4d4c7ae0206383d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40190042be64d0689368ebf3412ccdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b525dffb69bb4945a1771a6e7e990a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b9dd58ee3c4a01a00ae1a0a48d6b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0fbb6263c24f65ba434570e33515b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40571fac995d4596bc6de0aa5327740d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48bd78715bcf45e7a553f259a475e5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd82295ba0cb40428cd86c24d3364c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e80364175b44fb81de07887700cb04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82075a634f57401aaf483a605c5f11be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687cf837e2ae43e9bcfd53daa261365b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# **We can perform batched operations by turning our target text samples into a single list and then passing that list to our embedding model.**","metadata":{}},{"cell_type":"code","source":"# Turn text chunks into a single list\ntext_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:40:57.331303Z","iopub.execute_input":"2024-05-01T07:40:57.332997Z","iopub.status.idle":"2024-05-01T07:40:57.339857Z","shell.execute_reply.started":"2024-05-01T07:40:57.332941Z","shell.execute_reply":"2024-05-01T07:40:57.338224Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"%%time\n# Embed all texts in batches\ntext_chunk_embeddings = embedding_model.encode(text_chunks, batch_size=32,convert_to_tensor=True)\ntext_chunk_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:41:18.665766Z","iopub.execute_input":"2024-05-01T07:41:18.666255Z","iopub.status.idle":"2024-05-01T07:47:29.316158Z","shell.execute_reply.started":"2024-05-01T07:41:18.666216Z","shell.execute_reply":"2024-05-01T07:47:29.314723Z"},"scrolled":true,"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55abcfa5909b4485b4ac61ccb9fda2aa"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 9min 24s, sys: 2min 49s, total: 12min 14s\nWall time: 6min 10s\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0206,  0.0329, -0.0465,  ..., -0.0200, -0.0548, -0.0453],\n        [-0.0653, -0.0180, -0.0537,  ..., -0.0223, -0.0255, -0.0312],\n        [-0.0456,  0.0325, -0.0454,  ...,  0.0114, -0.0225, -0.0467],\n        ...,\n        [ 0.0057, -0.0318, -0.0421,  ..., -0.0134, -0.0218, -0.0245],\n        [ 0.0036, -0.0277, -0.0399,  ..., -0.0138, -0.0211, -0.0223],\n        [-0.0039, -0.0518, -0.0254,  ..., -0.0199, -0.0702, -0.0446]])"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Save embeddings to file**","metadata":{}},{"cell_type":"markdown","source":"### Since creating embeddings can be a timely process, let’s turn our pages_and_chunks_over_min_token_len list of dictionaries into a DataFrame and save it.","metadata":{}},{"cell_type":"code","source":"# Save embeddings to file\ntext_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\nembeddings_df_save_path = \"/kaggle/working/text_chunks_and_embeddings_df.csv\"\ntext_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:49:12.026369Z","iopub.execute_input":"2024-05-01T07:49:12.026847Z","iopub.status.idle":"2024-05-01T07:49:16.638841Z","shell.execute_reply.started":"2024-05-01T07:49:12.026817Z","shell.execute_reply":"2024-05-01T07:49:16.637817Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Import saved file and view\nimport pandas as pd\nembeddings_df_save_path = \"/kaggle/working/text_chunks_and_embeddings_df.csv\"\ntext_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\ntext_chunks_and_embedding_df_load.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:25.831002Z","iopub.execute_input":"2024-05-01T07:50:25.831465Z","iopub.status.idle":"2024-05-01T07:50:25.936856Z","shell.execute_reply.started":"2024-05-01T07:50:25.831433Z","shell.execute_reply":"2024-05-01T07:50:25.935359Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"  pdf_name  page_number                                     sentence_chunk  \\\n0  LEE.pdf            1  .sensorsbyLetterLetter Deep-Learning-Based Det...   \n1  LEE.pdf            1  As there is wide variation in terms of the typ...   \n2  LEE.pdf            2  social abilities of people with ASD increases ...   \n3  LEE.pdf            2  For examples of machine learning classiﬁcation...   \n4  LEE.pdf            3  diﬃculties pertaining to the need to secure th...   \n\n   chunk_char_count  chunk_word_count  chunk_token_count  \\\n0              3028               414             757.00   \n1               529                83             132.25   \n2              3061               450             765.25   \n3              1790               255             447.50   \n4              1718               264             429.50   \n\n                                           embedding  \n0  [-2.05749348e-02  3.28779072e-02 -4.64697927e-...  \n1  [-6.53321519e-02 -1.79831404e-02 -5.37393503e-...  \n2  [-4.55831960e-02  3.25354822e-02 -4.53654565e-...  \n3  [-4.83638458e-02  3.05312667e-02 -3.35919224e-...  \n4  [-3.53161991e-02  3.77684757e-02 -4.86659035e-...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pdf_name</th>\n      <th>page_number</th>\n      <th>sentence_chunk</th>\n      <th>chunk_char_count</th>\n      <th>chunk_word_count</th>\n      <th>chunk_token_count</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LEE.pdf</td>\n      <td>1</td>\n      <td>.sensorsbyLetterLetter Deep-Learning-Based Det...</td>\n      <td>3028</td>\n      <td>414</td>\n      <td>757.00</td>\n      <td>[-2.05749348e-02  3.28779072e-02 -4.64697927e-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LEE.pdf</td>\n      <td>1</td>\n      <td>As there is wide variation in terms of the typ...</td>\n      <td>529</td>\n      <td>83</td>\n      <td>132.25</td>\n      <td>[-6.53321519e-02 -1.79831404e-02 -5.37393503e-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LEE.pdf</td>\n      <td>2</td>\n      <td>social abilities of people with ASD increases ...</td>\n      <td>3061</td>\n      <td>450</td>\n      <td>765.25</td>\n      <td>[-4.55831960e-02  3.25354822e-02 -4.53654565e-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LEE.pdf</td>\n      <td>2</td>\n      <td>For examples of machine learning classiﬁcation...</td>\n      <td>1790</td>\n      <td>255</td>\n      <td>447.50</td>\n      <td>[-4.83638458e-02  3.05312667e-02 -3.35919224e-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LEE.pdf</td>\n      <td>3</td>\n      <td>diﬃculties pertaining to the need to secure th...</td>\n      <td>1718</td>\n      <td>264</td>\n      <td>429.50</td>\n      <td>[-3.53161991e-02  3.77684757e-02 -4.86659035e-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Similarity search**","metadata":{}},{"cell_type":"markdown","source":"### **Let’s import our embeddings we created earlier (tk-link to embedding file) and prepare them for use by turning them into a tensor.**","metadata":{}},{"cell_type":"code","source":"import random\nimport torch\nimport numpy as np\nimport pandas as pd\n\n# Import texts and embedding df\ntext_chunks_and_embedding_df = pd.read_csv(\"/kaggle/working/text_chunks_and_embeddings_df.csv\")\n# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\ntext_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n# Convert texts and embedding df to list of dicts\npages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n# # Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\nembeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32)\nembeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:52:29.387021Z","iopub.execute_input":"2024-05-01T07:52:29.387613Z","iopub.status.idle":"2024-05-01T07:52:29.637697Z","shell.execute_reply.started":"2024-05-01T07:52:29.387576Z","shell.execute_reply":"2024-05-01T07:52:29.635987Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"torch.Size([503, 768])"},"metadata":{}}]},{"cell_type":"code","source":"text_chunks_and_embedding_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:52:40.166190Z","iopub.execute_input":"2024-05-01T07:52:40.166639Z","iopub.status.idle":"2024-05-01T07:52:40.193494Z","shell.execute_reply.started":"2024-05-01T07:52:40.166609Z","shell.execute_reply":"2024-05-01T07:52:40.192093Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"  pdf_name  page_number                                     sentence_chunk  \\\n0  LEE.pdf            1  .sensorsbyLetterLetter Deep-Learning-Based Det...   \n1  LEE.pdf            1  As there is wide variation in terms of the typ...   \n2  LEE.pdf            2  social abilities of people with ASD increases ...   \n3  LEE.pdf            2  For examples of machine learning classiﬁcation...   \n4  LEE.pdf            3  diﬃculties pertaining to the need to secure th...   \n\n   chunk_char_count  chunk_word_count  chunk_token_count  \\\n0              3028               414             757.00   \n1               529                83             132.25   \n2              3061               450             765.25   \n3              1790               255             447.50   \n4              1718               264             429.50   \n\n                                           embedding  \n0  [-0.0205749348, 0.0328779072, -0.0464697927, -...  \n1  [-0.0653321519, -0.0179831404, -0.0537393503, ...  \n2  [-0.045583196, 0.0325354822, -0.0453654565, 0....  \n3  [-0.0483638458, 0.0305312667, -0.0335919224, 0...  \n4  [-0.0353161991, 0.0377684757, -0.0486659035, 0...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pdf_name</th>\n      <th>page_number</th>\n      <th>sentence_chunk</th>\n      <th>chunk_char_count</th>\n      <th>chunk_word_count</th>\n      <th>chunk_token_count</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LEE.pdf</td>\n      <td>1</td>\n      <td>.sensorsbyLetterLetter Deep-Learning-Based Det...</td>\n      <td>3028</td>\n      <td>414</td>\n      <td>757.00</td>\n      <td>[-0.0205749348, 0.0328779072, -0.0464697927, -...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LEE.pdf</td>\n      <td>1</td>\n      <td>As there is wide variation in terms of the typ...</td>\n      <td>529</td>\n      <td>83</td>\n      <td>132.25</td>\n      <td>[-0.0653321519, -0.0179831404, -0.0537393503, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LEE.pdf</td>\n      <td>2</td>\n      <td>social abilities of people with ASD increases ...</td>\n      <td>3061</td>\n      <td>450</td>\n      <td>765.25</td>\n      <td>[-0.045583196, 0.0325354822, -0.0453654565, 0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LEE.pdf</td>\n      <td>2</td>\n      <td>For examples of machine learning classiﬁcation...</td>\n      <td>1790</td>\n      <td>255</td>\n      <td>447.50</td>\n      <td>[-0.0483638458, 0.0305312667, -0.0335919224, 0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LEE.pdf</td>\n      <td>3</td>\n      <td>diﬃculties pertaining to the need to secure th...</td>\n      <td>1718</td>\n      <td>264</td>\n      <td>429.50</td>\n      <td>[-0.0353161991, 0.0377684757, -0.0486659035, 0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"embeddings[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:52:52.486077Z","iopub.execute_input":"2024-05-01T07:52:52.486542Z","iopub.status.idle":"2024-05-01T07:52:52.509148Z","shell.execute_reply.started":"2024-05-01T07:52:52.486510Z","shell.execute_reply":"2024-05-01T07:52:52.507942Z"},"scrolled":true,"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor([-2.0575e-02,  3.2878e-02, -4.6470e-02, -9.3290e-03,  2.2779e-02,\n         2.9414e-02,  4.6374e-02, -3.1967e-02, -6.6183e-02, -1.5472e-02,\n         6.5602e-03, -5.9365e-02,  6.3316e-02,  6.2994e-02, -5.6251e-02,\n        -2.7083e-02, -8.2688e-03,  3.3644e-03,  3.7265e-02, -8.6292e-03,\n         3.4178e-02,  1.0568e-02, -6.3263e-02,  7.3406e-02, -4.4935e-02,\n         3.0851e-02, -3.3910e-03, -6.0136e-02,  7.7574e-03, -6.9380e-02,\n        -1.0476e-03,  8.3198e-03,  2.3188e-02,  6.4961e-02,  2.1899e-06,\n        -2.7092e-02,  1.1488e-02, -2.1731e-02, -3.3694e-02, -6.7440e-02,\n        -3.4016e-02, -2.0057e-02,  2.5766e-03,  3.3616e-02, -3.1582e-02,\n        -8.3078e-02,  3.9643e-02,  2.4284e-02,  2.3357e-03,  2.6866e-02,\n        -1.3909e-02,  1.7111e-02,  6.4790e-02,  1.7834e-02,  1.4242e-02,\n        -1.0971e-02,  7.7212e-03, -4.0157e-02,  1.0227e-02, -4.5842e-02,\n        -4.0227e-02,  3.4358e-02, -9.8436e-03,  1.1578e-02,  2.8985e-02,\n        -1.0710e-02, -5.1550e-03, -7.0833e-02, -7.9531e-03,  5.5576e-02,\n         6.5233e-02,  3.6808e-02,  1.2630e-02,  2.1378e-02, -2.5656e-02,\n        -2.2481e-02, -2.5487e-02,  1.3879e-02,  2.4961e-04,  2.3939e-02,\n        -1.6155e-02,  1.0639e-02, -8.1041e-03, -6.0489e-02,  2.9155e-02,\n         7.7753e-03, -2.7845e-02,  2.6396e-02, -3.0354e-03, -2.1695e-03,\n         2.0621e-02, -7.4493e-03,  3.9614e-02,  2.2232e-02, -5.1900e-02,\n        -1.2570e-02, -6.8271e-02, -5.8859e-02, -8.3609e-03,  4.6870e-02,\n        -9.7581e-03, -2.1877e-02, -6.7945e-02, -2.5251e-02, -2.9048e-02,\n         5.5710e-02,  4.0973e-03, -1.0147e-03, -3.6789e-02,  1.3450e-02,\n         8.0660e-04,  2.3622e-02,  1.7422e-03,  2.7656e-03,  7.1559e-02,\n        -2.8429e-02, -7.7080e-02, -8.7857e-03, -9.7261e-03, -1.1967e-02,\n        -1.4766e-04,  4.8303e-02, -2.3753e-02,  6.4569e-02,  1.5716e-02,\n         2.1628e-02,  7.0274e-03,  2.6115e-02,  1.8574e-03, -2.6593e-02,\n        -1.0740e-02, -2.6220e-03,  5.2512e-02,  4.1975e-02,  5.8415e-04,\n         1.8094e-02,  8.2413e-02,  6.0421e-03,  3.3156e-02,  3.1979e-02,\n        -2.8922e-02,  1.1672e-02,  3.5141e-02, -2.5643e-02,  3.0980e-04,\n         3.6174e-02,  7.2161e-04, -2.9451e-02,  3.1448e-03, -3.9861e-02,\n         8.0228e-02, -1.9446e-02, -1.1915e-02,  2.1233e-03,  2.3937e-02,\n        -2.7930e-02,  3.6772e-02,  4.1132e-02,  5.5125e-02, -6.9711e-02,\n         2.1080e-02, -6.3639e-02, -1.8086e-02,  1.7592e-02, -2.6750e-02,\n        -3.6126e-02, -2.0252e-03, -6.9899e-02, -4.0183e-02,  4.8303e-02,\n        -2.6887e-03,  3.6741e-03,  4.0855e-02, -8.1944e-03, -2.5518e-05,\n         7.4415e-02,  3.3829e-02, -6.4120e-02, -2.0504e-02,  3.2299e-03,\n         4.4087e-03, -3.1259e-02,  5.0820e-02,  3.2110e-02, -1.9663e-02,\n        -4.3919e-02,  2.7564e-03,  2.7715e-02, -7.8673e-03, -3.7913e-02,\n        -4.0924e-02, -5.5524e-02, -2.7134e-02,  7.0631e-02,  5.0223e-02,\n        -2.4892e-02,  2.5332e-02, -1.6923e-02, -1.6408e-03,  1.7062e-02,\n        -8.0336e-05, -6.6697e-02, -2.6616e-02, -3.3511e-02, -3.9765e-02,\n        -1.5957e-02,  4.6858e-03, -5.6100e-02, -4.4937e-02, -4.1214e-02,\n        -2.6599e-02,  1.8789e-02, -9.0024e-03, -5.6289e-03, -6.7955e-02,\n         5.9373e-03,  1.9448e-02,  8.1627e-03, -1.8430e-02,  7.9068e-02,\n        -4.3435e-03,  1.5593e-02,  6.1469e-02,  3.1515e-02,  1.5991e-02,\n        -2.8709e-02,  6.5856e-02,  2.5143e-02,  4.4487e-02,  2.5140e-02,\n         1.8170e-02, -2.8355e-02,  5.3229e-03,  1.1790e-02, -1.1862e-02,\n         1.7762e-02, -1.9734e-04, -2.6371e-02, -1.8812e-02, -1.5541e-02,\n        -3.6347e-02,  7.8707e-02, -7.3073e-03, -3.5192e-02,  1.2279e-02,\n        -6.9986e-02,  1.3264e-02, -4.1331e-03,  4.6021e-02, -3.0409e-02,\n         6.5568e-02,  1.3867e-02, -4.2632e-02,  3.2671e-02, -3.0075e-02,\n        -6.0203e-02, -3.4610e-03,  3.2508e-02, -4.5722e-02, -5.9631e-02,\n        -2.5063e-02, -4.1722e-02,  2.1728e-02,  2.5803e-02, -1.5824e-02,\n        -5.0593e-03,  2.5663e-02, -6.8325e-02,  9.3793e-03,  1.2100e-02,\n        -5.4410e-02, -5.9047e-02,  2.1432e-02,  3.8516e-02, -6.7971e-03,\n         1.2629e-02,  6.2192e-02,  1.1909e-02,  3.9655e-02, -1.5339e-02,\n        -2.4279e-02, -5.2542e-02,  4.9082e-03, -2.0803e-03, -2.3840e-02,\n         9.0446e-04, -4.8609e-03,  7.0000e-03,  2.8653e-02, -1.7419e-02,\n        -4.1699e-02,  1.2148e-02,  1.0195e-02,  3.3493e-03,  7.9535e-02,\n        -3.5319e-03, -2.8211e-02,  1.0341e-01,  2.1295e-02,  3.9031e-02,\n         3.0045e-02,  4.6092e-02, -1.2262e-02, -8.9824e-03,  1.3539e-02,\n         2.8139e-02,  1.1338e-02,  2.6801e-03, -4.8890e-02,  4.1370e-02,\n         4.7925e-02,  1.2813e-02, -2.9507e-03,  1.4340e-02, -2.6284e-02,\n        -2.2712e-02, -4.0819e-02, -5.7426e-02, -3.8210e-02,  1.7221e-02,\n         4.5466e-02, -3.5068e-02,  1.5429e-02,  1.6459e-02, -2.3310e-02,\n        -1.7079e-02, -5.1139e-02, -2.0886e-02,  1.8153e-02, -6.1533e-03,\n         6.5301e-03, -3.0844e-02, -3.8711e-02,  2.0479e-02,  4.2535e-02,\n         3.9218e-03, -6.4314e-02, -2.4494e-02, -1.5652e-02, -8.4938e-03,\n        -2.8363e-02, -3.4382e-03, -6.7517e-02,  5.0820e-02, -2.3790e-02,\n        -1.0719e-02, -3.4074e-02, -4.0596e-02, -6.5370e-03,  2.0820e-02,\n         1.6376e-02,  6.3990e-02,  3.5816e-02,  7.8747e-03,  2.2636e-03,\n         2.2216e-02,  8.4075e-02, -1.5161e-02,  2.7224e-02, -1.8828e-03,\n         4.6076e-03,  1.0205e-02, -5.6022e-02,  7.7857e-02, -5.6910e-02,\n         3.3549e-02, -1.4880e-02,  5.9431e-02,  6.9138e-03,  6.6918e-03,\n         1.7687e-02, -2.1351e-02,  2.4010e-02,  6.6050e-02, -1.9379e-03,\n        -6.2387e-02,  9.2201e-03,  2.0728e-02, -5.4968e-02, -3.4562e-02,\n        -5.4029e-02, -1.3138e-03,  3.1051e-03,  4.4494e-02,  1.6297e-02,\n        -1.7245e-02, -2.5171e-04,  1.7718e-02,  3.6828e-02,  1.7506e-02,\n        -1.8014e-02,  1.4214e-02, -1.2811e-02, -4.3986e-02,  3.3834e-02,\n         2.0021e-02,  6.7304e-02, -8.0551e-02,  5.9015e-03, -3.2280e-02,\n         2.1788e-02, -1.0729e-01, -4.1351e-02,  7.7957e-02, -1.6983e-02,\n         6.2264e-02,  6.7843e-02,  2.2120e-03, -1.6698e-02, -3.1319e-02,\n        -5.1997e-02,  4.5931e-02, -1.5370e-02, -2.1040e-02,  1.0018e-02,\n         2.3136e-02, -2.1816e-02,  1.2926e-02,  7.5811e-02, -7.8506e-02,\n         5.6956e-02,  2.1097e-02,  7.2449e-02, -3.4212e-02, -5.1260e-02,\n        -4.9043e-02, -9.9510e-03,  1.9365e-02,  3.7506e-02,  2.4483e-02,\n        -7.3995e-02,  3.3199e-03,  2.4554e-02,  2.7141e-02, -1.4414e-02,\n        -2.5922e-02,  1.6602e-02,  5.3551e-02, -1.0917e-01, -2.5932e-02,\n         1.7863e-02,  2.4242e-02,  2.0891e-02,  4.9813e-02,  7.7265e-03,\n        -2.6358e-03,  2.8963e-02, -2.2044e-02,  6.1917e-02,  6.9309e-02,\n         2.2264e-02, -3.6317e-02, -9.0572e-03,  4.3532e-02, -3.8315e-02,\n        -5.0513e-02, -6.8330e-02,  1.2168e-02, -2.6492e-02,  6.8981e-02,\n        -4.5591e-02,  5.8373e-03,  1.5929e-02, -3.9251e-02, -3.3305e-02,\n         6.0449e-02, -4.9306e-03, -2.9464e-04,  9.5247e-03,  3.0050e-02,\n        -6.9178e-02, -1.7409e-02,  1.4804e-02, -2.7513e-02,  5.8696e-02,\n        -7.0098e-02, -1.2958e-02,  7.0048e-03, -6.5640e-02,  2.2013e-02,\n         8.2572e-04,  6.2169e-02,  2.0745e-02, -2.2125e-02, -2.3158e-02,\n        -1.1270e-02,  1.9324e-02,  6.9733e-02, -3.8143e-02, -4.8050e-02,\n         2.8382e-02,  1.9725e-02, -2.7919e-02,  2.8754e-02, -8.9622e-03,\n        -7.5089e-02,  2.0756e-03,  1.2414e-01, -2.0108e-02,  8.1890e-02,\n        -4.3505e-03,  1.7116e-02, -7.1590e-03,  3.3729e-02,  2.6662e-02,\n         1.9097e-02, -1.8581e-04,  7.9112e-04, -4.2909e-03, -1.2594e-02,\n         1.8923e-02,  3.1145e-02,  1.4269e-02, -1.0579e-02, -3.1407e-02,\n        -1.8561e-02, -4.1905e-02, -1.0694e-02,  6.4689e-03,  2.8228e-02,\n         8.4760e-03, -2.6760e-02,  2.4092e-03, -6.9729e-02,  2.3121e-02,\n         4.5761e-02, -9.5333e-03,  2.4443e-02,  5.9941e-04, -6.1767e-04,\n        -1.7765e-02,  2.6409e-03, -2.8392e-02, -5.8725e-04, -1.5095e-02,\n         2.4023e-02,  3.7996e-02, -3.6332e-02, -2.3345e-02,  2.6283e-02,\n         2.5858e-02, -4.3495e-02,  1.9747e-02, -4.2185e-02,  1.1453e-02,\n         1.5553e-02, -1.4669e-02, -8.1585e-03, -1.9433e-02, -4.3742e-02,\n         6.5356e-03, -1.2682e-02,  1.7927e-03, -2.7383e-02, -4.7899e-03,\n        -5.5257e-33, -6.6879e-03, -1.7007e-02, -1.1973e-02,  1.1705e-02,\n        -2.5753e-02,  5.5998e-02,  1.2167e-02,  5.2910e-02, -3.5370e-02,\n        -2.5710e-02, -2.6362e-03, -1.9915e-02,  2.2006e-02, -4.5409e-03,\n         3.0181e-02, -2.2691e-02,  2.2855e-02, -3.5890e-02,  2.1701e-02,\n        -1.3312e-03,  5.4414e-03,  3.9403e-03,  8.0106e-02, -7.6395e-02,\n        -2.8341e-02,  5.3958e-02,  1.2376e-02, -4.8538e-02,  3.7952e-02,\n         6.8424e-04, -1.0652e-02,  1.3664e-02,  3.8249e-04,  5.4214e-02,\n         6.9714e-03,  9.0640e-03, -6.7273e-02, -2.5609e-03, -7.0502e-02,\n         3.8416e-02, -6.3117e-02,  1.6781e-02,  7.5201e-02, -1.1479e-02,\n        -3.3353e-02, -1.0558e-02,  4.9580e-02, -3.2960e-02,  1.4573e-02,\n        -2.6200e-02, -2.4146e-03, -5.9786e-03, -1.2046e-02, -1.5075e-02,\n         5.1204e-02,  4.2488e-02, -5.1668e-02, -1.1917e-03, -3.9055e-02,\n         1.8226e-02,  4.4821e-02,  1.7298e-02,  9.6132e-03,  8.5234e-03,\n         6.7495e-02,  2.5469e-02,  6.1250e-02, -3.5350e-02, -5.2576e-03,\n         5.9310e-03,  5.7702e-02,  7.1102e-03,  3.0668e-02, -1.0360e-02,\n         1.9408e-02, -2.5786e-02, -7.2911e-02,  4.8559e-02, -1.6762e-02,\n         3.9819e-02,  3.3233e-03, -7.6013e-03, -7.9810e-02,  2.8594e-02,\n        -1.8562e-02,  4.0246e-02, -1.6942e-02, -2.8469e-02,  1.7323e-02,\n        -2.3825e-02, -5.0965e-02, -1.0849e-02,  1.9295e-02, -6.5963e-03,\n         2.2670e-02, -3.5517e-02,  1.1191e-01,  5.3301e-03,  3.5446e-02,\n         8.9107e-03, -1.4356e-03, -5.4501e-02,  5.6265e-02,  1.3525e-03,\n         6.9223e-02, -1.5568e-02, -2.8458e-02,  1.6539e-02, -7.7397e-03,\n        -2.3679e-02,  3.4587e-02,  8.3195e-04,  2.2314e-02,  1.8731e-02,\n        -1.5854e-02, -2.0625e-02, -3.9120e-03,  1.1938e-02, -8.7973e-03,\n        -5.4329e-04, -9.5267e-02,  1.2402e-05,  5.9279e-03,  8.7424e-03,\n        -1.9634e-02, -2.2033e-02, -5.4507e-02, -5.5146e-02,  3.3753e-02,\n        -3.4869e-02,  2.5878e-02, -4.4991e-02,  2.8291e-07, -5.8799e-02,\n         8.2761e-03,  1.7987e-02, -2.8159e-02,  3.0217e-02,  2.7607e-02,\n        -1.7815e-02,  1.3617e-02,  3.9300e-02, -2.1017e-02,  2.7954e-02,\n         3.3207e-02,  2.0454e-02,  3.2351e-02, -1.0821e-02, -1.7495e-03,\n        -1.1889e-02,  6.7653e-02, -7.3867e-03,  3.9430e-02, -1.2415e-02,\n         5.4017e-02, -1.2774e-02, -5.4741e-02,  5.3151e-02,  3.0406e-04,\n        -9.6145e-03,  3.4701e-02,  4.0834e-02, -2.0143e-02, -1.5076e-03,\n        -4.1403e-02,  7.2810e-02,  7.6652e-03, -2.5012e-02,  4.6883e-02,\n        -3.7657e-02, -2.4387e-02, -6.7411e-02, -5.2313e-02,  5.0960e-03,\n         7.9739e-02, -6.6768e-03,  1.8942e-03,  9.3179e-02, -2.5475e-02,\n        -2.2310e-02, -6.5265e-02,  9.8046e-03, -2.7648e-02,  7.4665e-02,\n        -1.8539e-02,  3.8185e-02,  3.9247e-03,  1.2617e-03, -3.6228e-02,\n        -1.6230e-02, -1.8062e-02,  8.7821e-03,  9.1345e-03, -5.4996e-02,\n        -1.7317e-02,  2.5722e-02,  3.6936e-02, -5.0455e-02,  2.8000e-02,\n        -6.1680e-03,  2.1387e-34, -1.0410e-02, -1.7201e-02, -3.1504e-04,\n        -4.8171e-02,  1.7879e-03, -4.3859e-03, -7.4677e-03,  1.2815e-02,\n        -1.9996e-02, -5.4800e-02, -4.5315e-02])"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Now let’s prepare another instance of our embedding model. Not because we have to but because we’d like to make it so you can start the notebook from the cell above.**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import util, SentenceTransformer\nembedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:53:44.990511Z","iopub.execute_input":"2024-05-01T07:53:44.990896Z","iopub.status.idle":"2024-05-01T07:53:46.350797Z","shell.execute_reply.started":"2024-05-01T07:53:44.990869Z","shell.execute_reply":"2024-05-01T07:53:46.349438Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# 1. Define the query\n# Note: This could be anything. But since we're working with a specific set of pdfs, we'll stick with queries related to those pdfs.\nquery = \"Autism spectrum disorder\"\nprint(f\"Query: {query}\")\n# 2. Embed the query to the same numerical space as the text examples\n# Note: It's important to embed your query with the same model you embedded your examples with.\nquery_embedding = embedding_model.encode(query, convert_to_tensor=True)\n# 3. Get similarity scores with the dot product (we'll time this for fun)\nfrom time import perf_counter as timer\nstart_time = timer()\ndot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\nend_time = timer()\n\nprint(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n\n# 4. Get the top-k results (we'll keep this to 5)\ntop_results_dot_product = torch.topk(dot_scores, k=5)\ntop_results_dot_product","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:54:09.591081Z","iopub.execute_input":"2024-05-01T07:54:09.591583Z","iopub.status.idle":"2024-05-01T07:54:09.720959Z","shell.execute_reply.started":"2024-05-01T07:54:09.591550Z","shell.execute_reply":"2024-05-01T07:54:09.719101Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Query: Autism spectrum disorder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3e8e4c8b925402f911b6625facdf0c3"}},"metadata":{}},{"name":"stdout","text":"Time take to get scores on 503 embeddings: 0.00051 seconds.\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"torch.return_types.topk(\nvalues=tensor([0.6785, 0.6635, 0.6534, 0.6270, 0.6262]),\nindices=tensor([394, 497, 390,  55, 232]))"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Let’s check the results of our similarity search.**\n#### First, we’ll define a small helper function to print out wrapped text (so it doesn’t print a whole text chunk as a single line).","metadata":{}},{"cell_type":"code","source":"# Define helper function to print wrapped text\nimport textwrap\ndef print_wrapped(text, wrap_length=80):\n  wrapped_text = textwrap.fill(text, wrap_length)\n  print(wrapped_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:55:17.174897Z","iopub.execute_input":"2024-05-01T07:55:17.175423Z","iopub.status.idle":"2024-05-01T07:55:17.182592Z","shell.execute_reply.started":"2024-05-01T07:55:17.175388Z","shell.execute_reply":"2024-05-01T07:55:17.181260Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print(f\"Query: '{query}'\\n\")\nprint(\"Results:\")\n# Loop through zipped together scores and indicies from torch.topk\nfor score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n  print(f\"Score: {score:.4f}\")\n  # Print relevant sentence chunk (since the scores are in descending order,the most relevant chunk will be first)\n  print(\"Text:\")\n  print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n  # Print the page number too so we can reference the textbook further (and check the results)\n  print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n  print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:55:27.945959Z","iopub.execute_input":"2024-05-01T07:55:27.946912Z","iopub.status.idle":"2024-05-01T07:55:27.961716Z","shell.execute_reply.started":"2024-05-01T07:55:27.946870Z","shell.execute_reply":"2024-05-01T07:55:27.960795Z"},"scrolled":true,"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Query: 'Autism spectrum disorder'\n\nResults:\nScore: 0.6785\nText:\nPMCID: PMC3424065. Lord C, Rutter M, Goode S, Heemsbergen J, Jordan H, Mawhood\nL, et al. Austism diagnostic observa- tion schedule: A standardized observation\nof communicative and social behavior. Journal of autism and developmental\ndisorders.1989; 19(2):185–212. PMID: 2745388. Lord C, Rutter M, Le Couteur A.\nAutism Diagnostic Interview-Revised: a revised version of a diagnostic interview\nfor caregivers of individuals with possible pervasive developmental disorders.\nJournal of autism and developmental disorders.1994; 24(5):659–85. PMID:\n7814313.10. Association AP. Diagnostic and statistical manual of mental\ndisorders (DSM-5®).\nPage number: 18\n\n\nScore: 0.6635\nText:\nPatten et al. Page 30Table 1Participant DemographicsASD; n=23 TD; n=14 Age at\n9–12months; mean (SD) 10.89 (1.39) 10.63 (.53) Age at 15–18 months; mean (SD)\n16.33 (.83) 16.28 (.70) Sex 19 males, 4 females 11 males, 3 females Race 23\nWhite, 1 Black 13 White, 1 Asian Maternal education 1 5.48 5.82 Childhood Autism\nRating Scale; mean (SD) 34.17 (1.52) 16.15 (.39)31Maternal education: 1=6th\ngrade or lower; 2=7th to 9th grade; 3=partial high school; 4=high school\ngraduate/GED; 5=associate of arts/associate of science or technical training or\npartial college training; 6=bachelor of arts/science; 7=master of arts/science\nor doctorate or other professional degree completed2missing information for two\nparticipants3missing information for four participantsJ Autism Dev Disord.\nAuthor manuscript; available in PMC 2014 October 01.\nPage number: 30\n\n\nScore: 0.6534\nText:\n@PLOS | MEDICINEReferences 1. Prince M, Patel V, Saxena S, Maj M, Maselko J,\nPhillips MR, et al. Global mental health 1 - No health without mental health.\nLancet.2007; 370(9590):859–77.https://doi.org/10.1016/S0140-6736(07) 61238-0\nPMID: 178040632. Baio J, Wiggins L, Christensen DL, Maenner MJ, Daniels J,\nWarren Z, et al. Prevalence of Autism Spec- trum Disorder Among Children Aged 8\nYears—Autism and Developmental Disabilities Monitoring Net- work, 11 Sites,\nUnited States, 2014. MMWR Surveillance Summaries.2018; 67(6):1.\nPage number: 18\n\n\nScore: 0.6270\nText:\nTABLE 1 | Comparison of the general conditions between the HR group and the TD\ngroup (mean ± SD). HR group (n = 45) TD group (n = 43) t/χ2 P-value Sex −0.06\n0.08 Male 40 32 Age (months) DQ Female 5 19.71 ± 3.43 11 16.40 ± 4.70 3.80 <0.01\n92.98 ± 7.89 −4.34 <0.01 92.77 ± 8.46 −0.25 93.70 ± 8.29 −2.24 80.29 ± 17.62\n92.02 ± 17.60 86.64 ± 19.03 0.03 60.84 ± 21.27 86.51 ± 8.353 −7.39 <0.01 92.28 ±\n7.18 −4.76 <0.01 Adaptive Gross motor 0.80 Fine motor Language Personal-social\n78.80 ± 17.19HR, high-risk autism spectrum disorder; TD, typical development;\nDQ, developmental quotient of the Gesell Developmental Scale. Diagnostic\nEvaluation of Children in the HR Group at 2 Years of Age At 2 years of age, the\nchildren in the HR group were evaluated using the Autism Diagnosis Interview-R\n(ADI-R) and the Autism Diagnostic Observation Schedule (ADOS). Two pediatric\npsychiatrists then clinically diagnosed the children based on the ASD diagnostic\ncriteria in the DSM-5 and the aforementioned evaluation results. All\nparticipants with conﬁrmed ASD (the ASD group) reached the cut-oﬀ scores for ASD\ndiagnosis for both evaluation scales. Analytic Approach The sex diﬀerence\nbetween the HR and TD groups was compared using the χ 2 test. The diﬀerences\nbetween the HR group and the TD group in social behaviors were determined using\nthe independent samples t-test. The correlations of social behaviors in the HR\ngroup with age, DQ, and symptom severity were analyzed using Pearson’s rho.\nFinally, models for early ASD screening were constructed using machine learning\nmethods based on ASD group and TD group, and the HR group (contained 40 children\nwith conﬁrmed ASD and 5 children with not met the criterions of ASD) would be\nused to veriﬁed the eﬀectiveness of models for early ASD screening. P < 0.05\nindicated that the diﬀerence was statistically signiﬁcant. RESULTSComparison of\nthe General Conditions Between the HR Group and the TD Group Age (months),\nadaptive DQ, language DQ, ﬁne motor DQ, and personal-social DQ were signiﬁcantly\ndiﬀerent (P < 0.05) between the HR group and the TD group, while sex DQ and\ngross motor DQ were not signiﬁcantly diﬀerent (P > 0.05) between the two groups.\nPage number: 4\n\n\nScore: 0.6262\nText:\nLHTbasis of communication and interpersonal relationships with others. Abnormal\nexpression is a prominent manifestation of autism, and it is also one of the\ncriteria for the diagnosis of autism. Doctors can diagnose autism by responding\nto abnormal facial expressions in children.is a representative disease of\ngeneralized developmental disorders. In recent years, the incidence of autism in\nchildren has become higher and higher, experiencing a transition from rare\ndiseases to epidemics. At present, research on autism is still in its infancy at\nhome and abroad, and research methods and tools are still developing. The main\nsymptoms of autism include impaired social and interpersonal communication,\nlanguage retardation, repetitive behavior and sensory dysfunction. It is\ndifficult for autistic patients to correctly recognize faces and explain facial\nemotions. They have different emotional expressions from ordinary people, and\nthey cannot correctly perceive and understand some basic expressions such as\nanger (Yan, 2008). At present, the diagnostic methods for autism spectrum\ndisorders include: traditional standard DSM-IV-TR (Segal, 2010) and ICD-10\n(Organization W H, 1992), various autism diagnostic assessment scales such as\n“Childhood Autism Rating Scale (CARS)”, “the autism child behavior scale (ABC)”\nand autism behavior rating scale and questionnaire interviews (Wang and Lu,\n2015). Most of these methods rely on doctors’ direct observation of the\npatient’s expression, speech and behavior based on their experience. Diagnostic\nresults are easily disturbed by external factors such as hospital level,\nphysician’s subjective level, patient’s education level, age and so on.\nPage number: 2\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Functionizing our semantic search pipeline**","metadata":{}},{"cell_type":"markdown","source":"#### Let’s put all of the steps from above for semantic search into a function or two so we can repeat the workflow.","metadata":{}},{"cell_type":"code","source":"def retrieve_relevant_resources(query: str,\n                                embeddings: torch.tensor,\n                                model: SentenceTransformer=embedding_model,\n                                n_resources_to_return: int=5,\n                                print_time: bool=True):\n  \"\"\" Embeds a query with model and returns top k scores and indices from embeddings. \"\"\"\n  # Embed the query\n  query_embedding = model.encode(query, convert_to_tensor=True)\n  # Get dot product scores on embeddings\n  start_time = timer()\n  dot_scores = util.dot_score(query_embedding, embeddings)[0]\n  end_time = timer()\n\n  if print_time:\n    print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n\n  scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n  return scores, indices","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:59:55.621748Z","iopub.execute_input":"2024-05-01T07:59:55.622238Z","iopub.status.idle":"2024-05-01T07:59:55.632684Z","shell.execute_reply.started":"2024-05-01T07:59:55.622200Z","shell.execute_reply":"2024-05-01T07:59:55.631297Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def print_top_results_and_scores(query: str,\n                                 embeddings: torch.tensor,\n                                 pages_and_chunks: list[dict]=pages_and_chunks,\n                                 n_resources_to_return: int=5):\n  \"\"\" Takes a query, retrieves most relevant resources and prints them out in descending order.\n  Note: Requires pages_and_chunks to be formatted in a specific way (se above for reference). \"\"\"\n  scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings,\n                                                n_resources_to_return=n_resources_to_return)\n\n  print(f\"Query: {query}\\n\")\n  print(\"Results:\")\n  # Loop through zipped together scores and indicies\n\n  for score, index in zip(scores, indices):\n    print(f\"Score: {score:.4f}\")\n    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n    print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n    # Print the page number too so we can reference the textbook further and check the results\n    print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n    print(f\"PDF name: {pages_and_chunks[index]['pdf_name']}\")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:00:14.361343Z","iopub.execute_input":"2024-05-01T08:00:14.361893Z","iopub.status.idle":"2024-05-01T08:00:14.376315Z","shell.execute_reply.started":"2024-05-01T08:00:14.361855Z","shell.execute_reply":"2024-05-01T08:00:14.374458Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# **Now let’s test our functions out.**","metadata":{}},{"cell_type":"code","source":"query = \"Proposed method for detection of Infants with Autism Spectrum Disorder Using Auto-Encoder Feature Representation\"\n# Get just the scores and indices of top related results\nscores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings)\nscores, indices","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:14.205880Z","iopub.execute_input":"2024-05-01T08:01:14.206441Z","iopub.status.idle":"2024-05-01T08:01:14.338326Z","shell.execute_reply.started":"2024-05-01T08:01:14.206403Z","shell.execute_reply":"2024-05-01T08:01:14.334650Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe711d78adad4105ae4df8a081604d9c"}},"metadata":{}},{"name":"stdout","text":"[INFO] Time taken to get scores on 503 embeddings: 0.00033 seconds.\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"(tensor([0.7837, 0.6978, 0.6846, 0.6817, 0.6805]),\n tensor([  0,  17, 164,  47, 158]))"},"metadata":{}}]},{"cell_type":"code","source":"# Print out the texts of the top scores\nprint_top_results_and_scores(query=query, embeddings=embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:28.853216Z","iopub.execute_input":"2024-05-01T08:01:28.853692Z","iopub.status.idle":"2024-05-01T08:01:28.983865Z","shell.execute_reply.started":"2024-05-01T08:01:28.853661Z","shell.execute_reply":"2024-05-01T08:01:28.982611Z"},"scrolled":true,"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"574152784d6843be84301d3b2a50a674"}},"metadata":{}},{"name":"stdout","text":"[INFO] Time taken to get scores on 503 embeddings: 0.00029 seconds.\nQuery: Proposed method for detection of Infants with Autism Spectrum Disorder Using Auto-Encoder Feature Representation\n\nResults:\nScore: 0.7837\n.sensorsbyLetterLetter Deep-Learning-Based Detection of Infants with Autism\nSpectrum Disorder Using Auto-Encoder Feature RepresentationJung Hyuk Lee 1, Geon\nWoo Lee 1, Guiyoung Bong 2, Hee Jeong Yoo 2,3 and Hong Kook Kim 1,*1School of\nElectrical Engineering and Computer Science, Gwangju Institute of Science and\nTechnology, Gwangju 61005, Korea; ljh0412@gist.ac.kr (J. H. L.);\ngeonwoo0801@gist.ac.kr (G. W. L.) 2 Department of Psychiatry, Seoul National\nUniversity Bundang Hospital, Seongnam-si,Gyeonggi-do 13620, Korea;\n20409@snubh.org (G. B.); hjyoo@snu.ac.kr (H. J. Y.)3 Department of Psychiatry,\nCollege of Medicine, Seoul National University, Seoul 03980, Korea *\nCorrespondence: hongkook@gist.ac.krReceived: 29 October 2020; Accepted: 24\nNovember 2020; Published: 26 November 2020check for v updatesAbstract: Autism\nspectrum disorder (ASD) is a developmental disorder with a life-span disability.\nWhile diagnostic instruments have been developed and qualiﬁed based on the\naccuracy of the discrimination of children with ASD from typical development\n(TD) children, the stability of such procedures can be disrupted by limitations\npertaining to time expenses and the subjectivity of clinicians. Consequently,\nautomated diagnostic methods have been developed for acquiring objective\nmeasures of autism, and in various ﬁelds of research, vocal characteristics have\nnot only been reported as distinctive characteristics by clinicians, but have\nalso shown promising performance in several studies utilizing deep learning\nmodels based on the automated discrimination of children with ASD from children\nwith TD. However, diﬃculties still exist in terms of the characteristics of the\ndata, the complexity of the analysis, and the lack of arranged data caused by\nthe low accessibility for diagnosis and the need to secure anonymity. In order\nto address these issues, we introduce a pre-trained feature extraction auto-\nencoder model and a joint optimization scheme, which can achieve robustness for\nwidely distributed and unreﬁned data using a deep-learning-based method for the\ndetection of autism that utilizes various models. By adopting this auto-encoder-\nbased feature extraction and joint optimization in the extended version of the\nGeneva minimalistic acoustic parameter set (eGeMAPS) speech feature data set, we\nacquire improved performance in the detection of ASD in infants compared to the\nraw data set. Keywords: auto-encoder; bidirectional long short-term memory\n(BLSTM); joint optimization; acoustic feature extraction; autism spectrum\ndisorder1. IntroductionAutism spectrum disorder (ASD) is a developmental\ndisorder with a high probability of causing diﬃculties in social interactions\nwith other people [1]. According to the Diagnostic and Statistical Manual of\nMental Disorders, Fifth Edition (DSM-5), ASD involves several characteristics\nsuch as being conﬁned to speciﬁc interests or behaviors, delayed linguistic\ndevelopment, and poor functionality in terms of communicating or functioning in\nsocial situations [2].\nPage number: 1\nPDF name: LEE.pdf\n\n\nScore: 0.6978\nIn addition to ASD detection, this research can be applied to the detection of\ninfants with development delays. Author Contributions: All authors discussed the\ncontents of the manuscript. H. K. K. contributed to the research idea and the\nframework of this study; G. B. and H. J. Y. provided the database and helped\nwith the discussion; J. H. L. performed the experiments; G. W. L. contributed to\nthe data collection and pre-processing. All authors have read and agreed to the\npublished version of the manuscript. Funding: This work was supported by the\nInstitute of Information & communications Technology Planning & evaluation\n(IITP) grant funded by the Korea government (MSIT) (No.2019-0-00330, Development\nof AI Technology for Early Screening of Infant/Child Autism Spectrum Disorders\nbased on Cognition of the Psychological Behavior and Response). Conﬂicts of\nInterest: The authors declare no conﬂict of interest.\nPage number: 9\nPDF name: LEE.pdf\n\n\nScore: 0.6846\nWe used a dataset of 5 children with ASD and 4 TD children older than two years.\nThe accuracy of the proposed method is 96.17% using k- fold cross validation\nwithout considering subject-wise hold out, which is a shortcoming of this study.\nIn other words, it has been overfitted to the available data and may fail to\ncorrectly clas- sify new samples. So, a thorough examination using an unseen\ntest set on cry features is neces- sary to evaluate the results. It should be\nnoted that the data from our previous study [41] could not be used in the study\npresented in this paper due to the differences in data collection\nprocedures.dren with ASD from TD children, are common among all the ASD cases.\nHowever, this may not be the case for all the features. For instance, tiptoe\nwalking, which is one of the repetitive behaviors of children with ASD, appears\nin approximately 25% of these children [42]. Conse- quently, in the current\nstudy, we propose a new cry-based approach for screening children with ASD. Our\nscreening approach makes use of the assumption that all discriminative charac-\nteristics of autism may not appear in all ASD children. This assumption is in\ncontrast with the assumption put forward in the ordinary instance-based machine\nlearning methods, which assumes that all instances of a class include all\ndiscriminative features needed for classification.\nPage number: 3\nPDF name: Asd_Cry_patterns.pdf\n\n\nScore: 0.6817\nThe 45 children in the HR group were reclassiﬁed into two groups after follow-\nup: ﬁve children in the N-ASD group who were not meet the criterion of ASD and\n40 children in the ASD group. The results showed that the accuracy of Support\nVector Machine (SVM) classiﬁcation was 83.35% for the SF episode. Conclusion:\nThe use of the social behavior indicator of the SFP for a child with HR before 2\nyears old can effectively predict the clinical diagnosis of the child at the age\nof 2 years. The screening model constructed using SVM based on the SF episode of\nthe SFP was the best. This also proves that the SFP has certain value in high-\nrisk autism spectrum disorder screening. In addition, because of its convenient,\nit can provide a self-screening mode for use at home. Trial registration:\nChinese Clinical Trial Registry, ChiCTR-OPC-17011995. Keywords: high-risk autism\nspectrum disorder, the Still-Face Paradigm, social behavior, machine learning,\nmodel for early screeningFrontiers in Pediatrics | www.frontiersin.org\nPage number: 1\nPDF name: Qiu.pdf\n\n\nScore: 0.6805\nThen, after preprocessing, the approach was used to train a classifier, using\ndata for 10 boys with ASD and 10 Typically Developed (TD) boys. The trained\nclassifier was tested on the data of 14 boys and 7 girls with ASD and 14 TD boys\nand 7 TD girls. The sensi- tivity, specificity, and precision of the proposed\napproach for boys were 85.71%, 100%, and 92.85%, respectively. These measures\nwere 71.42%, 100%, and 85.71% for girls, respec- tively. It was shown that the\nproposed approach outperforms the common classification methods. Furthermore, it\ndemonstrated better results than the studies which used voice fea- tures for\nscreening ASD. To pilot the practicality of the proposed approach for early\nautism screening, the trained classifier was tested on 57 participants between\n10 to 18 months. These 57 participants consisted of 28 boys and 29 girls and the\nresults were very encourag- ing for the use of the approach in early ASD\nscreening. IntroductionChildren with Autism Spectrum Disorder (ASD) are defined\nby their abnormal or impaired development in social interaction and\ncommunication, as well as restricted and repetitive behaviors, interests, or\nactivities [1]. The rapid growth of ASD in the past 20 years has inspired many\nresearch efforts toward the diagnosis and rehabilitation of ASD [2–5].\nPage number: 1\nPDF name: Asd_Cry_patterns.pdf\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Finally let's set up some query and get the most relevant results.**","metadata":{}},{"cell_type":"code","source":"questions = ['''How does the system architecture in the developed work utilize \nROS for robot-centered systems, and what are the key features of this framework?''',\n             '''How effective is the Video-referenced Infant Rating System for \n             Autism (VIRSA) in identifying autism spectrum disorder (ASD) risk in infancy?''',\n             '''What are the key psychometric properties of the Video-referenced Infant Rating System \n             for Autism (VIRSA) that were examined in the study, and what were the findings?''']","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:02:22.116195Z","iopub.execute_input":"2024-05-01T08:02:22.116730Z","iopub.status.idle":"2024-05-01T08:02:22.126400Z","shell.execute_reply.started":"2024-05-01T08:02:22.116694Z","shell.execute_reply":"2024-05-01T08:02:22.124664Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"for question in questions:\n  print_top_results_and_scores(query=question, embeddings=embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:02:32.506243Z","iopub.execute_input":"2024-05-01T08:02:32.506807Z","iopub.status.idle":"2024-05-01T08:02:32.941786Z","shell.execute_reply.started":"2024-05-01T08:02:32.506767Z","shell.execute_reply":"2024-05-01T08:02:32.940007Z"},"scrolled":true,"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8182634631541bf97ae5e80288cd4e9"}},"metadata":{}},{"name":"stdout","text":"[INFO] Time taken to get scores on 503 embeddings: 0.00089 seconds.\nQuery: How does the system architecture in the developed work utilize \nROS for robot-centered systems, and what are the key features of this framework?\n\nResults:\nScore: 0.5763\nFig.1 Node graph architecture of the proposed ROS-based system. The system is\ncomposed of two interconnected modules, an artificial reasoning module and a\nCRI-channel module. The ONO web server has two way of bidirectional\ncommunication: a websocket and a standard ROS SubscriberROS Manager /ROS_core\n/Kinect2_bridge g /Fusion_Node /kinect2_n/hd/ image_color_rect ry Sound\n/ROS_bridge Controller /tf_transform websocket /Detect_Recognize X /nav_msgs/\nHead_Odomets ONO Web Server g /NFOA_Node /known_face n /std_msgs/header/\n/ONO_Node Servo g vfoa_msgs Controller /CLNF_node 8 Artificial Reasoning Module\nChild-Robot Interaction Modul T Topic Wire Topic Wireless - Local leo & oI\nConnection Connection 5, \",developed in the server-side application to directly\nrun a ROS node and communicate with standard ROS publishers and subscribers. The\nCRI is implemented through the open source platform for social robotics\n(OPSOR0),> which is a promising and straightforward system developed for face to\nface communication composed of a low-cost modular robot called ONO (see Fig.2)\nand web-based applications [40]. Some of the most important requirements and\ncharacteristics that make ONO interesting for this CRI strategy are explained in\nthe following sections. The robot is covered in foam and also fabric to have a\nmore inviting and huggable appearance to the children. The robot has an\noversized head to make its facial expressions more prominent and to highlight\nthe importance for communica- tion and emotional interaction. As a consequence\nof its size and pose, children can interact with the robot at eye height when\nthe robot is placed on a table. The robot ONO has not a predefined identity, as\nthe only element previously conceived is the name.\nPage number: 4\nPDF name: 1_Ramrez-Duque_.pdf\n\n\nScore: 0.5463\nimplemented to provide some level of artificial cognition to enable automated\nrobot behavior. In contrast with the aforementioned researches, other works\nimplemented automated face analysis and artificial cognition through robot-\nmediator and computer vision, which analyzed child’s engagement [36, 37],\nemotions recognition capability [13, 15, 38] and child’s intentions [14, 16]. In\nthese works, two different strategies were implemented, where the most common is\nbased on mono- camera approach using an external RGB or RGBd sensor [15, 36, 37]\nor using on-board RGB cameras mounted on the robotic-platform [13, 16]. Other\nstrategies are based on a highly structured environment composed of an external\ncamera plus an on-board camera [38] or a network of vision sensors attached to a\nsmall table [14]. These strategies based on multi-camera methods improve the\nsystem’s performance, but remain constrained in relation to desired features,\nsuch as flexibility, scalability, and modularity. Thus, despite the potential\nthat these techniques have shown, achieving automated child’s behavior analysis\nin a naturalistic way into unstructured clinical-setups with robots that\ninteract accordingly remains a challenge in CRI.3 System Architecture\nOverviewThe ROS system used in this work is a flexible and scalable open\nframework for writing modular robot- centered systems. Similar to a computing\noperating system, ROS manages the interface between robot hardware and software\nmodules and provides common device drivers, data structures and tool-based\npackages, such as visualization and debugging tools. In addition, ROS uses an\ninterface definition language (IDL) to describe the messages sent between\nprocess or nodes, this feature facilitates the multi- language (C++, Python and\nLisp) development [39]. The overall system developed here was built using a node\ngraph architecture, taking advantages of the principal ROS design criteria. As\nwith ROS, our system consists of a number of nodes to local video processing\ntogether a robot’s behavior estimation, distributed around a number of different\nhosts and connected at runtime in a peer-to- peer topology. The inter-node\nconnection is implemented as a hand-shaking and occurs in XML-RPC protocol along\nwith a web-socket communication for robot’s web- based node (/ONO_node, see Fig.\nPage number: 3\nPDF name: 1_Ramrez-Duque_.pdf\n\n\nScore: 0.5416\n1). The node structure is flexible, scalable and can be dynamically modified,\ni.e., each node can be started and left running along an experimental session or\nresumed and connected to each other at runtime. In addition, from a general\nperspective, any robotic platform with web-socket communication can be\nintegrated. The developed system is composed of two interconnected modules as\nshown in Fig.1: an artificialreasoning module and a CRI-channel module. The\nmodule architectures are detailed in the following subsections.3.1 Architecture\nof Reasoning ModuleIn this module, a distributed architecture for local video\nprocessing is implemented. The data of each RGBD sensor in the multi-camera\nsystem are processed for two nodes, in which the first is a driver level node\nand the second 1 node transforms the is a processing node. The driver streaming\ndata of the RGBD sensor into the ROS messages format. The driver addresses the\ndata through a specialized transport provided by plugings to publishes images in\na compressed representations while the receptor node only sees sensor_msgs/Image\nmessages. The data processing node executes the face analysis algorithm.\nPage number: 3\nPDF name: 1_Ramrez-Duque_.pdf\n\n\nScore: 0.5395\nThis node uses a image_transport subscriber and a ROS packages called CvBridge\nto turn the data into a image format supported for the typical computer vision\nalgorithms. Later, the same node publishes the head pose and eye gaze direction\nby means of a ROS navigation message defined as nav_msgs/Odometry. An additional\nnode hosted in the most powerful workstation carries out a data fusion of all\nnavigation messages that were generated in the local processing stage. In\naddition to the fusion, this node computes the visual focus of attention (VFOA)\nand publishes this as a std_msgs/Header, in which the time stamp and the target\nname of the VFOA estimation are registered.3.2 Architecture of CRI-ChannelThe\nsystem proposed here has two bidirectional communi- cation channels, a robot-\ndevice, and a web-based applica- tion to interact with both the child and the\ntherapist. The robot device can interact with the CwASD executing differ- ent\nphysical actions, such as facial expression, upper limb poses, and verbal\ncommunication. Thus, according to the child’s performance, the reasoning module\ncan modify the robot’s behavior through automatic gaze shifting, chang- ing the\nfacial expression and providing sound rewards. The client-side application was\ndeveloped to allow the therapist to control and register all step of the\nintervention proto- col. This interface was also used to supervise and control\nthe robot’s behavior and to offer feedback to the therapist about the child’s\nperformance along the intervention. This App has two channels of communication\nfor interacting with the reasoning module. The first connection uses a web-\nsocket protocol and a RosBridge_suite package to support the interpretation of\nROS messages, as well as, JSON-based commands in ROS.\nPage number: 3\nPDF name: 1_Ramrez-Duque_.pdf\n\n\nScore: 0.4940\nThe second one uses a ROS module!Tools for using the Kinect One (Kinect V2) in\nROS, https://github.com/code-iai/iai _kinect2.@ SpringerContent courtesy of\nSpringer Nature, terms of use apply. Rights reserved.\nPage number: 3\nPDF name: 1_Ramrez-Duque_.pdf\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ba7761f7ffa4ddeb76632dfd7e1622a"}},"metadata":{}},{"name":"stdout","text":"[INFO] Time taken to get scores on 503 embeddings: 0.00024 seconds.\nQuery: How effective is the Video-referenced Infant Rating System for \n             Autism (VIRSA) in identifying autism spectrum disorder (ASD) risk in infancy?\n\nResults:\nScore: 0.8611\nHHS Public Access Author manuscript J Child Psychol Psychiatry. Author\nmanuscript; available in PMC 2021 January 01. Published in final edited form\nas:J Child Psychol Psychiatry.2020 January ; 61(1):\n88–94.doi:10.1111/jcpp.13105. A Video-Based Measure to Identify Autism Risk in\nInfancyGregory S. Young, PhDa, John N. Constantino, MDb, Simon Dvorak, BSc,\nAshleigh Belding, MPHa, Devon Gangi, PhDa, Alesha Hill, BAa, Monique Hill, MAa,\nMeghan Miller, PhDa, Chandni Parikh, PhDa, AJ Schwichtenberg, PhDd, Erika Solis,\nBSa, Sally Ozonoff, PhDa aDepartment of Psychiatry & Behavioral Sciences, MIND\nInstitute, University of California-DavisbDepartment of Psychiatry, Washington\nUniversity-St. Louis School of MedicinecInformation and Educational Technology,\nUniversity of California-DavisdDepartment of Human Development & Family Studies,\nPurdue UniversityBackground: Signs of autism are present in the first two years\nof life, but the average age of diagnosis lags far behind. Instruments that\nimprove detection of autism risk in infancy are needed. This study developed and\ntested the psychometric properties of a novel video-based approach to detecting\nASD in infancy. Methods: A prospective longitudinal study of children at\nelevated or lower risk for autism spectrum disorder was conducted. Participants\nwere 76 infants with an older sibling with ASD and 37 infants with no known\nfamily history of autism. The Video-referenced Infant Rating System for Autism\n(VIRSA) is a web-based application that presents pairs of videos of parents and\ninfants playing together and requires forced-choice judgments of which video is\nmost similar to the child being rated. Parents rated participants on the VIRSA\nat 6, 9, 12, and 18 months of age.\nPage number: 1\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.8260\nThe VIRSA app also asked for confirmation of the child’s age in order to present\nvideos from a matching age group. Since multiple video exemplars of each scale\npoint (1 to 10) were available, videos were sampled from the pool without\nreplacement. The UC Davis Institutional Review Board approved the study\nprocedures. Parents signed an informed consent form prior to participation. They\ncompleted VIRSA ratings when their child was 6-, 9-, 12-, and 18-months-old and\nagain two weeks later to examine test-retest reliability. An automated email\ninvited parents to the online VIRSA app, which could be accessed by computer or\nmobile device (e.g., smartphones, tablets). VIRSA ratings were always done prior\nto in-person assessments, which were conducted at 6, 12, 18, 24, and 36 months\nby examiners unaware of risk group or previous test results. The VIRSA\nvalidation sample consisted of 110 infants (73 with an older sibling with ASD,\n37 with no known family history of autism), none of whom supplied videos used in\ninstrument development. Twenty-one children in the familial high-risk (HR) group\nreceived a diagnosis of ASD, whereas none of the low-risk (LR) children did. ASD\ndiagnoses were made at any age that a child met DSM-5 criteria, based on all\ninformation available.\nPage number: 4\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.8053\n2011) using a parent- report measure (Wetherby, Brosnan-Maddox, Peace & Newton,\n2008) identified 32 infants with ASD. This represents significant under-\nidentification, even after accounting for cases with later onset (Barger,\nCampbell & McDonough, 2013), since current prevalence studies estimate that 170\nof 10,000 children have ASD (Baio et al.,2018). The lower sensitivity of early\nscreening measures may be due to the subtlety of initial ASD symptoms and the\ndifficulty of accurately conveying them to parents through written descriptions.\nMajor sources of error in parent questionnaires include comprehension and\ninterpretation problems (Koriat, Goldsmith & Pansky, 2000; Krosnick & Presser,\n2010), such as limited understanding of the queried constructs, inadequate\nknowledge of developmental milestones, and bias due to post-event information\n(e.g., eventual diagnosis). The current study moves beyond verbal descriptions\nby employing video examples to reduce subjective interpretations. The use of\nvideos has been shown to dramatically increase clarity in other fields, from\nmusic instruction to motor vehicle repair (Arguel & Jamet, 2009). Recently,\nvideo was incorporated in ASD screening by Marrus and colleagues (2015), who had\nparents complete ratings after watching a video of a socially competent toddler,\nin order to “reduce discrepant interpretations of items by providing informants\nwith a common naturalistic standard for comparison” (p. 1340). Here we describe\nthe development of a new instrument, the Video-referenced Infant Rating System\nfor Autism (VIRSA). It extends previous approaches (Marrus et al.,2015) byJ\nChild Psychol Psychiatry. Author manuscript; available in PMC 2021 January 01.\nPage number: 2\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.8002\nYoung et al.stratified by familial risk to yield the HR Non-ASD and LR Non-ASD\ncomparison groups. Descriptive statistics are shown in Table 1. Mullen Scales of\nEarly Learning (MSEL; Mullen, 1995) is an assessment of cognitive, motor, and\nlanguage development for children aged 1 to 68 months. MSEL scores were used to\ndescribe the sample. Scores on the Fine Motor and Visual Reception subtests were\nused to evaluate discriminant validity since these developmental domains are\ntheoretically unrelated to the social-communicative focus of the VIRSA. Autism\nDiagnostic Observation Schedule, 2nd edition (ADOS-2; Lord et al.,2012) is an\nobservational measure that assesses ASD symptoms through semi-standardized\ninteractions between the clinician and child. It is comprised of five modules\nappropriate for various ages and language levels; the Toddler module was\nadministered at 18 and 24 months and either module 1 or 2 was used at 36 months,\ndepending upon the child’s verbal level. Analyses utilized the overall total\nalgorithm (Social Affect + Restricted Repetitive Behavior or SARRB) score.\nScores on the ADOS at 18 months were also used to examine convergent validity\nwith VIRSA ratings at 18 months. Psychometric properties of the VIRSA were\nexamined in several ways. Split-half reliability was analyzed by comparing the\nfirst half of the ratings within a given session to the second half.\nPage number: 5\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.7900\nInfant Behavior and Development, 38, 107–115. [PubMed: 25656952]Gangi DN,\nBoterberg S, Schwichtenberg AJ, Solis E, Young GS, Iosif A, & Ozonoff S (2019).\nUse of prospective longitudinal gaze measurements in defining regression Paper\npresented at the annual meeting of the International Society for Autism\nResearch, Montreal, 5. Grimes DA, & Schulz KF (2002). Uses and abuses of\nscreening tests. Lancet, 359, 881–884. [PubMed: 11897304]Abbreviations:VIRSA\nVideo-referenced Infant Rating System for Autism\nPage number: 8\nPDF name: Young_Behavior.pdf\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d1cdf2a3c14859a370d18af9967c44"}},"metadata":{}},{"name":"stdout","text":"[INFO] Time taken to get scores on 503 embeddings: 0.00022 seconds.\nQuery: What are the key psychometric properties of the Video-referenced Infant Rating System \n             for Autism (VIRSA) that were examined in the study, and what were the findings?\n\nResults:\nScore: 0.8379\nHHS Public Access Author manuscript J Child Psychol Psychiatry. Author\nmanuscript; available in PMC 2021 January 01. Published in final edited form\nas:J Child Psychol Psychiatry.2020 January ; 61(1):\n88–94.doi:10.1111/jcpp.13105. A Video-Based Measure to Identify Autism Risk in\nInfancyGregory S. Young, PhDa, John N. Constantino, MDb, Simon Dvorak, BSc,\nAshleigh Belding, MPHa, Devon Gangi, PhDa, Alesha Hill, BAa, Monique Hill, MAa,\nMeghan Miller, PhDa, Chandni Parikh, PhDa, AJ Schwichtenberg, PhDd, Erika Solis,\nBSa, Sally Ozonoff, PhDa aDepartment of Psychiatry & Behavioral Sciences, MIND\nInstitute, University of California-DavisbDepartment of Psychiatry, Washington\nUniversity-St. Louis School of MedicinecInformation and Educational Technology,\nUniversity of California-DavisdDepartment of Human Development & Family Studies,\nPurdue UniversityBackground: Signs of autism are present in the first two years\nof life, but the average age of diagnosis lags far behind. Instruments that\nimprove detection of autism risk in infancy are needed. This study developed and\ntested the psychometric properties of a novel video-based approach to detecting\nASD in infancy. Methods: A prospective longitudinal study of children at\nelevated or lower risk for autism spectrum disorder was conducted. Participants\nwere 76 infants with an older sibling with ASD and 37 infants with no known\nfamily history of autism. The Video-referenced Infant Rating System for Autism\n(VIRSA) is a web-based application that presents pairs of videos of parents and\ninfants playing together and requires forced-choice judgments of which video is\nmost similar to the child being rated. Parents rated participants on the VIRSA\nat 6, 9, 12, and 18 months of age.\nPage number: 1\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.7901\nInfant Behavior and Development, 38, 107–115. [PubMed: 25656952]Gangi DN,\nBoterberg S, Schwichtenberg AJ, Solis E, Young GS, Iosif A, & Ozonoff S (2019).\nUse of prospective longitudinal gaze measurements in defining regression Paper\npresented at the annual meeting of the International Society for Autism\nResearch, Montreal, 5. Grimes DA, & Schulz KF (2002). Uses and abuses of\nscreening tests. Lancet, 359, 881–884. [PubMed: 11897304]Abbreviations:VIRSA\nVideo-referenced Infant Rating System for Autism\nPage number: 8\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.7734\nYoung et al.stratified by familial risk to yield the HR Non-ASD and LR Non-ASD\ncomparison groups. Descriptive statistics are shown in Table 1. Mullen Scales of\nEarly Learning (MSEL; Mullen, 1995) is an assessment of cognitive, motor, and\nlanguage development for children aged 1 to 68 months. MSEL scores were used to\ndescribe the sample. Scores on the Fine Motor and Visual Reception subtests were\nused to evaluate discriminant validity since these developmental domains are\ntheoretically unrelated to the social-communicative focus of the VIRSA. Autism\nDiagnostic Observation Schedule, 2nd edition (ADOS-2; Lord et al.,2012) is an\nobservational measure that assesses ASD symptoms through semi-standardized\ninteractions between the clinician and child. It is comprised of five modules\nappropriate for various ages and language levels; the Toddler module was\nadministered at 18 and 24 months and either module 1 or 2 was used at 36 months,\ndepending upon the child’s verbal level. Analyses utilized the overall total\nalgorithm (Social Affect + Restricted Repetitive Behavior or SARRB) score.\nScores on the ADOS at 18 months were also used to examine convergent validity\nwith VIRSA ratings at 18 months. Psychometric properties of the VIRSA were\nexamined in several ways. Split-half reliability was analyzed by comparing the\nfirst half of the ratings within a given session to the second half.\nPage number: 5\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.7705\nThe VIRSA app also asked for confirmation of the child’s age in order to present\nvideos from a matching age group. Since multiple video exemplars of each scale\npoint (1 to 10) were available, videos were sampled from the pool without\nreplacement. The UC Davis Institutional Review Board approved the study\nprocedures. Parents signed an informed consent form prior to participation. They\ncompleted VIRSA ratings when their child was 6-, 9-, 12-, and 18-months-old and\nagain two weeks later to examine test-retest reliability. An automated email\ninvited parents to the online VIRSA app, which could be accessed by computer or\nmobile device (e.g., smartphones, tablets). VIRSA ratings were always done prior\nto in-person assessments, which were conducted at 6, 12, 18, 24, and 36 months\nby examiners unaware of risk group or previous test results. The VIRSA\nvalidation sample consisted of 110 infants (73 with an older sibling with ASD,\n37 with no known family history of autism), none of whom supplied videos used in\ninstrument development. Twenty-one children in the familial high-risk (HR) group\nreceived a diagnosis of ASD, whereas none of the low-risk (LR) children did. ASD\ndiagnoses were made at any age that a child met DSM-5 criteria, based on all\ninformation available.\nPage number: 4\nPDF name: Young_Behavior.pdf\n\n\nScore: 0.7690\n2011) using a parent- report measure (Wetherby, Brosnan-Maddox, Peace & Newton,\n2008) identified 32 infants with ASD. This represents significant under-\nidentification, even after accounting for cases with later onset (Barger,\nCampbell & McDonough, 2013), since current prevalence studies estimate that 170\nof 10,000 children have ASD (Baio et al.,2018). The lower sensitivity of early\nscreening measures may be due to the subtlety of initial ASD symptoms and the\ndifficulty of accurately conveying them to parents through written descriptions.\nMajor sources of error in parent questionnaires include comprehension and\ninterpretation problems (Koriat, Goldsmith & Pansky, 2000; Krosnick & Presser,\n2010), such as limited understanding of the queried constructs, inadequate\nknowledge of developmental milestones, and bias due to post-event information\n(e.g., eventual diagnosis). The current study moves beyond verbal descriptions\nby employing video examples to reduce subjective interpretations. The use of\nvideos has been shown to dramatically increase clarity in other fields, from\nmusic instruction to motor vehicle repair (Arguel & Jamet, 2009). Recently,\nvideo was incorporated in ASD screening by Marrus and colleagues (2015), who had\nparents complete ratings after watching a video of a socially competent toddler,\nin order to “reduce discrepant interpretations of items by providing informants\nwith a common naturalistic standard for comparison” (p. 1340). Here we describe\nthe development of a new instrument, the Video-referenced Infant Rating System\nfor Autism (VIRSA). It extends previous approaches (Marrus et al.,2015) byJ\nChild Psychol Psychiatry. Author manuscript; available in PMC 2021 January 01.\nPage number: 2\nPDF name: Young_Behavior.pdf\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Great!! \n\n## Now you can choose your desired LLM and use it to generate your text.","metadata":{}},{"cell_type":"markdown","source":"## Also try the notebook on your own set of PDFs and see how it works!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}